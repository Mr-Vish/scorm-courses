<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>SageMaker Fine-Tuning - Introduction</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Introduction to Fine-Tuning on Amazon SageMaker</h1>

<div class="intro-section">
<p>Welcome to <strong>Fine-Tuning Large Language Models with Amazon SageMaker</strong>. As AI continues to transform industries, the ability to take a powerful pre-trained model and adapt it to your specific domain, data, and business requirements is becoming an essential skill for data scientists and AI engineers. Amazon SageMaker provides a robust, scalable, and secure environment to perform this complex task, from data preparation to large-scale distributed training and high-performance deployment.</p>

<h2>Why Fine-Tune?</h2>
<p>Pre-trained foundation models like Llama 3 or Mistral have incredible general capabilities, but they often lack the specialized knowledge required for specific tasksâ€”such as legal document analysis, medical diagnosis support, or adhering to a very specific corporate brand voice. Fine-tuning allows you to "specialize" these models, often resulting in higher accuracy, lower latency, and reduced costs compared to using massive general-purpose models via generic APIs.</p>

<h2>What You'll Learn</h2>
<p>This course is designed to take you through the entire end-to-end lifecycle of fine-tuning on AWS:</p>
<ul>
    <li><strong>Module 1: Getting Started with JumpStart:</strong> Learn how to use SageMaker JumpStart for "one-click" fine-tuning of popular open-weights models.</li>
    <li><strong>Module 2: Advanced Fine-Tuning Techniques:</strong> Deep dive into Parameter-Efficient Fine-Tuning (PEFT), including <strong>LoRA (Low-Rank Adaptation)</strong> and <strong>QLoRA</strong>, which allow you to train massive models on limited hardware.</li>
    <li><strong>Module 3: Data Preparation and Feature Engineering:</strong> Master the art of preparing high-quality instruction datasets using SageMaker Data Wrangler and Ground Truth.</li>
    <li><strong>Module 4: Distributed Training at Scale:</strong> Learn how to use SageMaker's distributed training libraries to train models across multiple GPUs and instances.</li>
    <li><strong>Module 5: Evaluation and Deployment:</strong> Understand how to rigorously evaluate your fine-tuned models and deploy them to scalable, low-latency endpoints.</li>
</ul>

<h2>Course Structure</h2>
<p>The curriculum combines architectural overviews with practical, hands-on Python code examples using the SageMaker SDK and the Hugging Face library. You will learn how to configure training jobs, monitor their progress in real-time, and optimize your costs using Spot Instances and specialized hardware like AWS Trainium.</p>

<h2>Prerequisites</h2>
<p>To get the most out of this course, you should have:</p>
<ul>
    <li>A solid understanding of Machine Learning fundamentals.</li>
    <li>Proficiency in Python and familiarity with the PyTorch framework.</li>
    <li>Basic knowledge of AWS services, specifically S3 and IAM.</li>
    <li>Familiarity with the Transformer architecture.</li>
</ul>

<h2>How to Navigate</h2>
<p>Use the <strong>Next</strong> and <strong>Previous</strong> buttons at the bottom right to move through the course. Your progress is saved automatically via the SCORM framework. Each module ends with practice questions to reinforce your learning, culminating in a final comprehensive assessment.</p>

<p>Let's unlock the power of specialized AI with Amazon SageMaker.</p>
</div>

<script type="text/javascript">
</script>
</body>
</html>