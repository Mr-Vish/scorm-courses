<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Working with Different LLM Providers</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Working with Different LLM Providers</h1>

<h2>LangChain's Provider-Agnostic Approach</h2>
<p>One of LangChain's key strengths is its unified interface for working with multiple LLM providers. This allows you to switch between providers with minimal code changes, enabling flexibility and cost optimization.</p>

<h2>Supported LLM Providers</h2>
<table>
<tr>
<th>Provider</th>
<th>Package</th>
<th>Best For</th>
<th>Key Models</th>
</tr>
<tr>
<td class="rowheader">OpenAI</td>
<td>langchain-openai</td>
<td>General purpose, function calling</td>
<td>GPT-4o, GPT-4, GPT-3.5-turbo</td>
</tr>
<tr>
<td class="rowheader">Anthropic</td>
<td>langchain-anthropic</td>
<td>Long context, safety</td>
<td>Claude 3 Opus, Sonnet, Haiku</td>
</tr>
<tr>
<td class="rowheader">Google</td>
<td>langchain-google-genai</td>
<td>Multimodal, free tier</td>
<td>Gemini Pro, Gemini Pro Vision</td>
</tr>
<tr>
<td class="rowheader">Cohere</td>
<td>langchain-cohere</td>
<td>Embeddings, reranking</td>
<td>Command, Command-Light</td>
</tr>
<tr>
<td class="rowheader">HuggingFace</td>
<td>langchain-huggingface</td>
<td>Open source, self-hosted</td>
<td>Llama 2, Mistral, Falcon</td>
</tr>
</table>

<h2>OpenAI Integration</h2>
<blockquote>
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# Initialize OpenAI model
model = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7,
    max_tokens=1000,
    api_key="your-api-key"  # Or set OPENAI_API_KEY env variable
)

# Create a simple chain
prompt = ChatPromptTemplate.from_template("Explain {topic} concisely.")
chain = prompt | model

result = chain.invoke({"topic": "quantum entanglement"})
</blockquote>

<h2>Anthropic Claude Integration</h2>
<blockquote>
from langchain_anthropic import ChatAnthropic

# Initialize Claude model
model = ChatAnthropic(
    model="claude-3-opus-20240229",
    temperature=0.7,
    max_tokens=1000,
    api_key="your-api-key"  # Or set ANTHROPIC_API_KEY env variable
)

# Use the same chain structure
chain = prompt | model
result = chain.invoke({"topic": "machine learning"})
</blockquote>

<h2>Google Gemini Integration</h2>
<blockquote>
from langchain_google_genai import ChatGoogleGenerativeAI

# Initialize Gemini model
model = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0.7,
    google_api_key="your-api-key"  # Or set GOOGLE_API_KEY env variable
)

# Same chain interface
chain = prompt | model
result = chain.invoke({"topic": "neural networks"})
</blockquote>

<h2>Model Configuration Parameters</h2>
<p>Common parameters across providers:</p>

<table>
<tr>
<th>Parameter</th>
<th>Purpose</th>
<th>Typical Range</th>
</tr>
<tr>
<td class="rowheader">temperature</td>
<td>Controls randomness in output</td>
<td>0.0 (deterministic) to 2.0 (creative)</td>
</tr>
<tr>
<td class="rowheader">max_tokens</td>
<td>Maximum length of response</td>
<td>1 to model's context limit</td>
</tr>
<tr>
<td class="rowheader">top_p</td>
<td>Nucleus sampling threshold</td>
<td>0.0 to 1.0</td>
</tr>
<tr>
<td class="rowheader">frequency_penalty</td>
<td>Reduces repetition</td>
<td>-2.0 to 2.0</td>
</tr>
<tr>
<td class="rowheader">presence_penalty</td>
<td>Encourages topic diversity</td>
<td>-2.0 to 2.0</td>
</tr>
</table>

<h2>Provider-Specific Features</h2>

<h3>OpenAI Function Calling</h3>
<blockquote>
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o")

# Define functions for the model to use
functions = [
    {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City name"
                }
            },
            "required": ["location"]
        }
    }
]

result = model.invoke(
    "What's the weather in San Francisco?",
    functions=functions
)
</blockquote>

<h3>Claude's Extended Context</h3>
<blockquote>
from langchain_anthropic import ChatAnthropic

# Claude 3 supports up to 200K tokens
model = ChatAnthropic(
    model="claude-3-opus-20240229",
    max_tokens=4096
)

# Process long documents
long_document = "..." # Up to 200K tokens
result = model.invoke(f"Summarize this document: {long_document}")
</blockquote>

<h2>Switching Between Providers</h2>
<p>Design your application to easily switch providers:</p>

<blockquote>
from langchain_core.language_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

def get_model(provider: str) -> BaseChatModel:
    """Factory function to get model by provider name."""
    models = {
        "openai": ChatOpenAI(model="gpt-4o"),
        "anthropic": ChatAnthropic(model="claude-3-opus-20240229"),
        "google": ChatGoogleGenerativeAI(model="gemini-pro")
    }
    return models.get(provider, models["openai"])

# Use in your chain
provider = "anthropic"  # Can be configured via environment variable
model = get_model(provider)
chain = prompt | model
</blockquote>

<h2>Cost Optimization Strategies</h2>
<ul>
<li><strong>Model Selection:</strong> Use smaller models (GPT-3.5, Claude Haiku) for simple tasks</li>
<li><strong>Caching:</strong> Cache responses for repeated queries</li>
<li><strong>Prompt Optimization:</strong> Reduce token usage with concise prompts</li>
<li><strong>Batch Processing:</strong> Process multiple requests together</li>
<li><strong>Fallback Models:</strong> Use cheaper models as fallbacks</li>
</ul>

<h2>Implementing Fallbacks</h2>
<blockquote>
from langchain_core.runnables import RunnableWithFallbacks

# Primary model
primary_model = ChatOpenAI(model="gpt-4o")

# Fallback models
fallback_model_1 = ChatOpenAI(model="gpt-3.5-turbo")
fallback_model_2 = ChatAnthropic(model="claude-3-haiku-20240307")

# Create chain with fallbacks
model_with_fallbacks = primary_model.with_fallbacks(
    [fallback_model_1, fallback_model_2]
)

chain = prompt | model_with_fallbacks
</blockquote>

<h2>Monitoring Token Usage</h2>
<blockquote>
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    result = chain.invoke({"topic": "artificial intelligence"})
    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Prompt Tokens: {cb.prompt_tokens}")
    print(f"Completion Tokens: {cb.completion_tokens}")
    print(f"Total Cost (USD): ${cb.total_cost}")
</blockquote>

<h2>Environment Configuration</h2>
<p>Best practice: Store API keys in environment variables:</p>

<blockquote>
# .env file
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AI...

# Load in your application
from dotenv import load_dotenv
load_dotenv()

# Models will automatically use environment variables
model = ChatOpenAI(model="gpt-4o")  # Uses OPENAI_API_KEY
</blockquote>

<h2>Best Practices</h2>
<ul>
<li><strong>Abstract Provider Logic:</strong> Use factory patterns for easy switching</li>
<li><strong>Monitor Costs:</strong> Track token usage and expenses</li>
<li><strong>Test Across Providers:</strong> Ensure consistent behavior</li>
<li><strong>Implement Retries:</strong> Handle rate limits and transient errors</li>
<li><strong>Secure API Keys:</strong> Never commit keys to version control</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
