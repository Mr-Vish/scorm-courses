<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Performance Optimization</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Performance Optimization</h1>

<h2>Optimization Strategies</h2>
<table>
<tr>
<th>Strategy</th>
<th>Impact</th>
<th>Complexity</th>
</tr>
<tr>
<td class="rowheader">Caching</td>
<td>High - Reduces API calls</td>
<td>Low</td>
</tr>
<tr>
<td class="rowheader">Streaming</td>
<td>High - Improves perceived latency</td>
<td>Low</td>
</tr>
<tr>
<td class="rowheader">Async Processing</td>
<td>High - Increases throughput</td>
<td>Medium</td>
</tr>
<tr>
<td class="rowheader">Prompt Optimization</td>
<td>Medium - Reduces tokens</td>
<td>Medium</td>
</tr>
<tr>
<td class="rowheader">Model Selection</td>
<td>High - Balances cost and quality</td>
<td>Low</td>
</tr>
</table>

<h2>Caching Responses</h2>
<blockquote>
from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache

# In-memory cache (fast, not persistent)
set_llm_cache(InMemoryCache())

# SQLite cache (persistent)
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# Subsequent identical requests use cached responses
result1 = chain.invoke({"topic": "AI"})
result2 = chain.invoke({"topic": "AI"})  # Uses cache
</blockquote>

<h2>Redis Cache for Production</h2>
<blockquote>
from langchain.cache import RedisCache
from redis import Redis

set_llm_cache(RedisCache(redis_=Redis(host="localhost", port=6379)))
</blockquote>

<h2>Semantic Caching</h2>
<blockquote>
from langchain.cache import RedisSemanticCache
from langchain_openai import OpenAIEmbeddings

# Cache similar queries, not just exact matches
set_llm_cache(
    RedisSemanticCache(
        redis_url="redis://localhost:6379",
        embedding=OpenAIEmbeddings(),
        score_threshold=0.9
    )
)
</blockquote>

<h2>Batch Processing</h2>
<blockquote>
# Process multiple inputs efficiently
inputs = [
    {"topic": "AI"},
    {"topic": "ML"},
    {"topic": "DL"}
]

# Batch invocation
results = chain.batch(inputs)

# Async batch with concurrency control
results = await chain.abatch(
    inputs,
    config={"max_concurrency": 5}
)
</blockquote>

<h2>Prompt Optimization</h2>
<blockquote>
# Before: Verbose prompt
verbose_prompt = """
Please provide a comprehensive and detailed explanation of the following topic.
Make sure to include all relevant information and context.
Topic: {topic}
"""

# After: Concise prompt
concise_prompt = "Explain {topic} concisely."

# Saves tokens while maintaining quality
</blockquote>

<h2>Model Selection Strategy</h2>
<blockquote>
from langchain_openai import ChatOpenAI

def get_model_for_task(task_complexity):
    """Select model based on task complexity."""
    if task_complexity == "simple":
        return ChatOpenAI(model="gpt-3.5-turbo")  # Fast, cheap
    elif task_complexity == "medium":
        return ChatOpenAI(model="gpt-4o-mini")  # Balanced
    else:
        return ChatOpenAI(model="gpt-4o")  # High quality

# Use appropriate model
simple_model = get_model_for_task("simple")
complex_model = get_model_for_task("complex")
</blockquote>

<h2>Parallel Execution</h2>
<blockquote>
from langchain_core.runnables import RunnableParallel

# Run multiple chains in parallel
parallel_chain = RunnableParallel(
    summary=summary_chain,
    keywords=keyword_chain,
    sentiment=sentiment_chain
)

# All chains execute concurrently
results = parallel_chain.invoke({"text": article})
</blockquote>

<h2>Streaming for Better UX</h2>
<blockquote>
# Stream responses for immediate feedback
for chunk in chain.stream({"topic": "AI"}):
    print(chunk.content, end="", flush=True)

# Async streaming
async for chunk in chain.astream({"topic": "AI"}):
    print(chunk.content, end="", flush=True)
</blockquote>

<h2>Connection Pooling</h2>
<blockquote>
from langchain_openai import ChatOpenAI
import httpx

# Configure connection pool
http_client = httpx.Client(
    limits=httpx.Limits(
        max_connections=100,
        max_keepalive_connections=20
    )
)

model = ChatOpenAI(
    model="gpt-4o",
    http_client=http_client
)
</blockquote>

<h2>Lazy Loading</h2>
<blockquote>
# Load resources only when needed
class LazyChain:
    def __init__(self):
        self._chain = None
    
    @property
    def chain(self):
        if self._chain is None:
            self._chain = self._build_chain()
        return self._chain
    
    def _build_chain(self):
        # Expensive initialization
        return prompt | model | parser

lazy_chain = LazyChain()
# Chain is built only on first use
result = lazy_chain.chain.invoke({"topic": "AI"})
</blockquote>

<h2>Embedding Cache</h2>
<blockquote>
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings

# Cache embeddings to avoid recomputation
store = LocalFileStore("./embedding_cache")
embeddings = CacheBackedEmbeddings.from_bytes_store(
    OpenAIEmbeddings(),
    store,
    namespace="openai_embeddings"
)
</blockquote>

<h2>Rate Limiting</h2>
<blockquote>
from ratelimit import limits, sleep_and_retry

@sleep_and_retry
@limits(calls=50, period=60)  # 50 calls per minute
def rate_limited_invoke(chain, input_data):
    return chain.invoke(input_data)
</blockquote>

<h2>Best Practices</h2>
<ul>
<li><strong>Cache Aggressively:</strong> Cache responses for repeated queries</li>
<li><strong>Use Streaming:</strong> Improve perceived performance</li>
<li><strong>Batch When Possible:</strong> Process multiple requests together</li>
<li><strong>Optimize Prompts:</strong> Reduce unnecessary tokens</li>
<li><strong>Choose Right Model:</strong> Balance cost, speed, and quality</li>
<li><strong>Monitor Performance:</strong> Track and optimize bottlenecks</li>
<li><strong>Use Async:</strong> Handle concurrent requests efficiently</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
