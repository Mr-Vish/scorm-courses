<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Error Handling and Retry Strategies</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Error Handling and Retry Strategies</h1>

<h2>Common LangChain Errors</h2>
<table>
<tr>
<th>Error Type</th>
<th>Cause</th>
<th>Solution</th>
</tr>
<tr>
<td class="rowheader">RateLimitError</td>
<td>Too many API requests</td>
<td>Implement exponential backoff</td>
</tr>
<tr>
<td class="rowheader">TimeoutError</td>
<td>Request takes too long</td>
<td>Increase timeout or optimize prompt</td>
</tr>
<tr>
<td class="rowheader">AuthenticationError</td>
<td>Invalid API key</td>
<td>Verify credentials</td>
</tr>
<tr>
<td class="rowheader">ContextLengthExceeded</td>
<td>Input too long</td>
<td>Reduce context or use summarization</td>
</tr>
<tr>
<td class="rowheader">OutputParserException</td>
<td>Invalid model output format</td>
<td>Use OutputFixingParser</td>
</tr>
</table>

<h2>Basic Error Handling</h2>
<blockquote>
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_template("Explain {topic}")
chain = prompt | model

try:
    result = chain.invoke({"topic": "quantum physics"})
except Exception as e:
    print(f"Error occurred: {type(e).__name__}: {str(e)}")
    # Implement fallback logic
</blockquote>

<h2>Retry with Exponential Backoff</h2>
<blockquote>
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from openai import RateLimitError

@retry(
    retry=retry_if_exception_type(RateLimitError),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5)
)
def invoke_with_retry(chain, input_data):
    return chain.invoke(input_data)

# Use the retry wrapper
result = invoke_with_retry(chain, {"topic": "AI"})
</blockquote>

<h2>Fallback Chains</h2>
<blockquote>
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

primary_model = ChatOpenAI(model="gpt-4o")
fallback_model = ChatAnthropic(model="claude-3-haiku-20240307")

chain_with_fallback = (prompt | primary_model).with_fallbacks(
    [prompt | fallback_model]
)

# Automatically uses fallback if primary fails
result = chain_with_fallback.invoke({"topic": "machine learning"})
</blockquote>

<h2>Timeout Handling</h2>
<blockquote>
import asyncio

async def invoke_with_timeout(chain, input_data, timeout=30):
    try:
        result = await asyncio.wait_for(
            chain.ainvoke(input_data),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        return "Request timed out. Please try again."

result = asyncio.run(invoke_with_timeout(chain, {"topic": "AI"}))
</blockquote>

<h2>Handling Context Length Errors</h2>
<blockquote>
from langchain.text_splitter import RecursiveCharacterTextSplitter

def safe_invoke(chain, text, max_length=4000):
    if len(text) > max_length:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=max_length,
            chunk_overlap=200
        )
        chunks = splitter.split_text(text)
        results = [chain.invoke({"text": chunk}) for chunk in chunks]
        return " ".join(results)
    return chain.invoke({"text": text})
</blockquote>

<h2>Output Parser Error Fixing</h2>
<blockquote>
from langchain_core.output_parsers import JsonOutputParser, OutputFixingParser

base_parser = JsonOutputParser()
fixing_parser = OutputFixingParser.from_llm(
    parser=base_parser,
    llm=model
)

chain = prompt | model | fixing_parser
# Automatically fixes malformed JSON
</blockquote>

<h2>Logging and Monitoring</h2>
<blockquote>
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def invoke_with_logging(chain, input_data):
    try:
        logger.info(f"Invoking chain with input: {input_data}")
        result = chain.invoke(input_data)
        logger.info("Chain invocation successful")
        return result
    except Exception as e:
        logger.error(f"Chain invocation failed: {str(e)}")
        raise
</blockquote>

<h2>Best Practices</h2>
<ul>
<li><strong>Always Handle Exceptions:</strong> Wrap chain invocations in try-except blocks</li>
<li><strong>Implement Retries:</strong> Use exponential backoff for transient errors</li>
<li><strong>Set Timeouts:</strong> Prevent hanging requests</li>
<li><strong>Use Fallbacks:</strong> Provide alternative models or responses</li>
<li><strong>Log Errors:</strong> Track failures for debugging and monitoring</li>
<li><strong>Validate Inputs:</strong> Check input size and format before processing</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
