<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Querying Indexes and Performance Optimization</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Querying Indexes and Performance Optimization</h1>

<h2>Basic Query Operations</h2>
<p>Once your index is deployed, you can query it to find nearest neighbors for any query vector:</p>

<div class="code-block">
<pre><code>from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel

# Initialize
aiplatform.init(project="my-project", location="us-central1")
embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")

# Get endpoint
endpoint = aiplatform.MatchingEngineIndexEndpoint(
    index_endpoint_name="projects/123/locations/us-central1/indexEndpoints/456"
)

# Generate query embedding
query_text = "How to deploy machine learning models on Google Cloud?"
query_embedding = embedding_model.get_embeddings([query_text])[0].values

# Search for nearest neighbors
results = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=[query_embedding],
    num_neighbors=10
)

# Process results
for idx, neighbor in enumerate(results[0]):
    print(f"{idx+1}. ID: {neighbor.id}, Distance: {neighbor.distance:.4f}")
</code></pre>
</div>

<h2>Query Parameters</h2>
<table>
    <tr>
        <th>Parameter</th>
        <th>Description</th>
        <th>Default</th>
        <th>Recommendation</th>
    </tr>
    <tr>
        <td class="rowheader">deployed_index_id</td>
        <td>Which deployed index to query</td>
        <td>Required</td>
        <td>Use versioned IDs (search_v1, search_v2)</td>
    </tr>
    <tr>
        <td class="rowheader">queries</td>
        <td>List of query vectors</td>
        <td>Required</td>
        <td>Batch multiple queries for efficiency</td>
    </tr>
    <tr>
        <td class="rowheader">num_neighbors</td>
        <td>Number of results to return</td>
        <td>10</td>
        <td>10-50 for most use cases</td>
    </tr>
    <tr>
        <td class="rowheader">filter</td>
        <td>Metadata filtering criteria</td>
        <td>None</td>
        <td>Use for category/date filtering</td>
    </tr>
</table>

<h2>Batch Querying for Performance</h2>
<p>Process multiple queries in a single API call to reduce overhead:</p>

<div class="code-block">
<pre><code># Single query: 10ms latency + 5ms overhead = 15ms total
# Batch of 10 queries: 10ms latency + 5ms overhead = 15ms total (1.5ms per query!)

queries = [
    "cloud computing basics",
    "machine learning deployment",
    "data pipeline architecture"
]

# Generate embeddings in batch
query_embeddings = [
    embedding_model.get_embeddings([q])[0].values 
    for q in queries
]

# Batch query
batch_results = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=query_embeddings,  # List of vectors
    num_neighbors=10
)

# Process each query's results
for i, results in enumerate(batch_results):
    print(f"\nQuery {i+1}: {queries[i]}")
    for neighbor in results[:3]:  # Top 3
        print(f"  - {neighbor.id}: {neighbor.distance:.4f}")
</code></pre>
</div>

<h2>Metadata Filtering</h2>
<p>Filter results by metadata to narrow search scope:</p>

<div class="code-block">
<pre><code>from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import (
    Namespace,
    NumericNamespace
)

# Filter by category
results = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=[query_embedding],
    num_neighbors=20,
    filter=[
        Namespace(
            name="category",
            allow_tokens=["machine-learning", "cloud-computing"]
        )
    ]
)

# Filter by numeric range (e.g., publication date)
results = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=[query_embedding],
    num_neighbors=20,
    filter=[
        NumericNamespace(
            name="publish_year",
            value_int=2023,
            op="GREATER_EQUAL"
        )
    ]
)

# Combine multiple filters (AND logic)
results = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=[query_embedding],
    num_neighbors=20,
    filter=[
        Namespace(name="category", allow_tokens=["AI"]),
        NumericNamespace(name="rating", value_float=4.0, op="GREATER_EQUAL")
    ]
)
</code></pre>
</div>

<h2>Understanding Query Results</h2>
<div class="code-block">
<pre><code># Each result contains:
for neighbor in results[0]:
    print(f"ID: {neighbor.id}")              # Document identifier
    print(f"Distance: {neighbor.distance}")  # Similarity score
    
    # For DOT_PRODUCT_DISTANCE:
    # - Higher values = more similar
    # - Range: typically 0.0 to 1.0 for normalized vectors
    
    # For SQUARED_L2_DISTANCE:
    # - Lower values = more similar
    # - Range: 0.0 (identical) to unbounded
</code></pre>
</div>

<h2>Performance Optimization Techniques</h2>

<h3>1. Connection Pooling</h3>
<div class="code-block">
<pre><code>from google.cloud import aiplatform
from concurrent.futures import ThreadPoolExecutor

# Reuse endpoint connection
endpoint = aiplatform.MatchingEngineIndexEndpoint(
    index_endpoint_name="projects/123/locations/us-central1/indexEndpoints/456"
)

# Process queries in parallel
def query_index(query_text):
    query_emb = embedding_model.get_embeddings([query_text])[0].values
    return endpoint.find_neighbors(
        deployed_index_id="search_v1",
        queries=[query_emb],
        num_neighbors=10
    )

# Parallel execution
with ThreadPoolExecutor(max_workers=10) as executor:
    results = list(executor.map(query_index, user_queries))
</code></pre>
</div>

<h3>2. Caching Frequent Queries</h3>
<div class="code-block">
<pre><code>from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_search(query_hash, num_neighbors=10):
    """Cache search results for identical queries"""
    query_emb = get_embedding_from_cache(query_hash)
    return endpoint.find_neighbors(
        deployed_index_id="search_v1",
        queries=[query_emb],
        num_neighbors=num_neighbors
    )

# Use cache
query_hash = hashlib.md5(query_text.encode()).hexdigest()
results = cached_search(query_hash)

# Cache hit rate of 20-30% typical for production systems
</code></pre>
</div>

<h3>3. Reduce num_neighbors</h3>
<div class="code-block">
<pre><code># Retrieve fewer neighbors for faster queries
# Then re-rank or filter in application layer

# Fast initial retrieval (5ms)
candidates = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=[query_embedding],
    num_neighbors=50  # Retrieve 50 candidates
)

# Application-level re-ranking (2ms)
reranked = rerank_by_business_logic(candidates[0])

# Return top 10 to user
final_results = reranked[:10]

# Total: 7ms vs 12ms for retrieving 100 neighbors directly
</code></pre>
</div>

<h2>Latency Benchmarks</h2>
<table>
    <tr>
        <th>Operation</th>
        <th>Latency (p50)</th>
        <th>Latency (p99)</th>
        <th>Notes</th>
    </tr>
    <tr>
        <td class="rowheader">Single query (10 neighbors)</td>
        <td>8ms</td>
        <td>15ms</td>
        <td>1M vector index</td>
    </tr>
    <tr>
        <td class="rowheader">Batch query (10 queries)</td>
        <td>12ms</td>
        <td>25ms</td>
        <td>1.2ms per query</td>
    </tr>
    <tr>
        <td class="rowheader">Filtered query</td>
        <td>10ms</td>
        <td>20ms</td>
        <td>+2ms overhead</td>
    </tr>
    <tr>
        <td class="rowheader">Large result set (100 neighbors)</td>
        <td>15ms</td>
        <td>30ms</td>
        <td>Linear with num_neighbors</td>
    </tr>
</table>

<h2>Error Handling and Retries</h2>
<div class="code-block">
<pre><code>from google.api_core import retry
from google.api_core import exceptions

# Configure retry policy
@retry.Retry(
    predicate=retry.if_exception_type(
        exceptions.ServiceUnavailable,
        exceptions.DeadlineExceeded
    ),
    initial=1.0,  # Initial delay
    maximum=10.0,  # Max delay
    multiplier=2.0,  # Exponential backoff
    deadline=60.0  # Total timeout
)
def robust_query(query_embedding):
    """Query with automatic retries"""
    return endpoint.find_neighbors(
        deployed_index_id="search_v1",
        queries=[query_embedding],
        num_neighbors=10
    )

# Handle errors gracefully
try:
    results = robust_query(query_embedding)
except exceptions.NotFound:
    print("Index not found or not deployed")
except exceptions.InvalidArgument:
    print("Invalid query parameters")
except Exception as e:
    print(f"Unexpected error: {e}")
</code></pre>
</div>

<h2>Monitoring Query Performance</h2>
<div class="code-block">
<pre><code>import time

def monitored_query(query_embedding):
    """Query with performance monitoring"""
    start_time = time.time()
    
    try:
        results = endpoint.find_neighbors(
            deployed_index_id="search_v1",
            queries=[query_embedding],
            num_neighbors=10
        )
        
        latency = (time.time() - start_time) * 1000  # Convert to ms
        
        # Log metrics
        log_metric("query_latency_ms", latency)
        log_metric("query_success", 1)
        
        return results
        
    except Exception as e:
        log_metric("query_error", 1)
        raise

# Track key metrics:
# - Average latency
# - p50, p95, p99 latency
# - Error rate
# - Queries per second
</code></pre>
</div>

<h2>Query Optimization Checklist</h2>
<ul>
<li>✓ Use batch queries when processing multiple requests</li>
<li>✓ Implement connection pooling to reuse endpoint connections</li>
<li>✓ Cache frequent queries using Redis or in-memory cache</li>
<li>✓ Request only the number of neighbors you actually need</li>
<li>✓ Use metadata filtering to reduce search space</li>
<li>✓ Implement retry logic with exponential backoff</li>
<li>✓ Monitor latency and error rates in production</li>
<li>✓ Consider geographic proximity: deploy endpoints near users</li>
</ul>

<h2>Advanced: Hybrid Search</h2>
<p>Combine vector search with traditional keyword search for best results:</p>

<div class="code-block">
<pre><code># Step 1: Vector search for semantic similarity
vector_results = endpoint.find_neighbors(
    deployed_index_id="search_v1",
    queries=[query_embedding],
    num_neighbors=50
)

# Step 2: Keyword search (e.g., Elasticsearch)
keyword_results = elasticsearch_search(query_text, size=50)

# Step 3: Merge and re-rank using RRF (Reciprocal Rank Fusion)
def reciprocal_rank_fusion(vector_results, keyword_results, k=60):
    scores = {}
    
    for rank, doc in enumerate(vector_results):
        scores[doc.id] = scores.get(doc.id, 0) + 1 / (k + rank + 1)
    
    for rank, doc in enumerate(keyword_results):
        scores[doc.id] = scores.get(doc.id, 0) + 1 / (k + rank + 1)
    
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)

# Hybrid results combine semantic and lexical matching
hybrid_results = reciprocal_rank_fusion(vector_results[0], keyword_results)
</code></pre>
</div>

<h2>Key Takeaways</h2>
<ul>
<li>Query performance is optimized through batch processing, connection pooling, and caching strategies</li>
<li>Metadata filtering allows narrowing search scope without sacrificing performance significantly</li>
<li>Typical query latency is 8-15ms (p50-p99) for indexes with millions of vectors</li>
<li>Implement robust error handling with retries for production reliability</li>
<li>Monitor query latency, error rates, and throughput to identify performance bottlenecks</li>
<li>Hybrid search combining vector and keyword search often provides superior results</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
