<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Production RAG Implementation</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Production RAG Implementation</h1>

<h2>Production-Ready RAG Architecture</h2>
<p>Building a production RAG system requires careful consideration of performance, reliability, cost, and user experience. This section covers advanced patterns and best practices.</p>

<h2>Advanced Retrieval Strategies</h2>

<h3>Hybrid Retrieval (Dense + Sparse)</h3>
<p>Combine vector search with keyword search for optimal results:</p>
<div class="code-block">
<pre><code>def hybrid_retrieval(query, num_results=10):
    """Combine vector search with keyword search"""
    
    # Dense retrieval (vector search)
    query_embedding = embedding_model.get_embeddings([query])[0].values
    vector_results = endpoint.find_neighbors(
        deployed_index_id="docs_v1",
        queries=[query_embedding],
        num_neighbors=20
    )
    
    # Sparse retrieval (BM25)
    keyword_results = elasticsearch.search(
        index="documents",
        body={"query": {"match": {"text": query}}, "size": 20}
    )
    
    # Reciprocal Rank Fusion
    combined = reciprocal_rank_fusion(vector_results[0], keyword_results)
    return combined[:num_results]
</code></pre>
</div>

<h3>Query Expansion</h3>
<div class="code-block">
<pre><code>def expand_query(original_query):
    """Generate multiple query variations for better recall"""
    
    expansion_prompt = f"""Generate 3 alternative phrasings:
    
Original: {original_query}

Alternatives:"""
    
    response = llm.generate_content(expansion_prompt)
    expanded_queries = [original_query] + response.text.strip().split("\n")
    
    # Search with all variations
    all_results = []
    for query in expanded_queries:
        query_emb = embedding_model.get_embeddings([query])[0].values
        results = endpoint.find_neighbors(
            deployed_index_id="docs_v1",
            queries=[query_emb],
            num_neighbors=5
        )
        all_results.extend(results[0])
    
    # Deduplicate
    unique_results = {r.id: r for r in all_results}
    return list(unique_results.values())[:10]
</code></pre>
</div>

<h2>Prompt Engineering for RAG</h2>

<h3>Basic Prompt Template</h3>
<div class="code-block">
<pre><code>prompt = f"""Answer based on the context below.

Context:
{context}

Question: {question}

Answer:"""
</code></pre>
</div>

<h3>Advanced Prompt with Instructions</h3>
<div class="code-block">
<pre><code>prompt = f"""You are a technical assistant. Guidelines:

1. Answer ONLY based on provided context
2. If answer not in context, say "I don't have enough information"
3. Cite sources using [Source N]
4. Be concise but complete

Context:
{context}

Question: {question}

Answer:"""
</code></pre>
</div>

<h2>Multi-Turn Conversations</h2>
<div class="code-block">
<pre><code>class ConversationalRAG:
    def __init__(self):
        self.history = []
    
    def query(self, user_message):
        """Handle multi-turn conversation"""
        self.history.append({"role": "user", "content": user_message})
        
        # Create contextualized query
        contextualized = self._create_context()
        
        # Retrieve chunks
        query_emb = embedding_model.get_embeddings([contextualized])[0].values
        results = endpoint.find_neighbors(
            deployed_index_id="docs_v1",
            queries=[query_emb],
            num_neighbors=5
        )
        
        # Generate response
        chunks = fetch_chunks(results[0])
        context = assemble_context(chunks)
        prompt = self._create_prompt(context)
        response = llm.generate_content(prompt)
        
        self.history.append({"role": "assistant", "content": response.text})
        return response.text
</code></pre>
</div>

<h2>Performance Optimization</h2>
<table>
    <tr>
        <th>Technique</th>
        <th>Impact</th>
        <th>Implementation Complexity</th>
    </tr>
    <tr>
        <td class="rowheader">Caching frequent queries</td>
        <td>50-70% latency reduction</td>
        <td>Low</td>
    </tr>
    <tr>
        <td class="rowheader">Async retrieval</td>
        <td>30-40% latency reduction</td>
        <td>Medium</td>
    </tr>
    <tr>
        <td class="rowheader">Streaming responses</td>
        <td>Better UX, perceived speed</td>
        <td>Medium</td>
    </tr>
    <tr>
        <td class="rowheader">Batch processing</td>
        <td>3-5x throughput increase</td>
        <td>Low</td>
    </tr>
</table>

<h2>Error Handling and Fallbacks</h2>
<div class="code-block">
<pre><code>def robust_rag_query(question):
    """RAG with error handling"""
    try:
        # Primary: Vector search
        results = vector_search(question)
        if results:
            return generate_response(results)
    except Exception as e:
        log_error(f"Vector search failed: {e}")
    
    try:
        # Fallback: Keyword search
        results = keyword_search(question)
        if results:
            return generate_response(results)
    except Exception as e:
        log_error(f"Keyword search failed: {e}")
    
    # Final fallback: LLM without context
    return llm.generate_content(
        f"Answer this question: {question}\n\n"
        "Note: Limited context available."
    )
</code></pre>
</div>

<h2>Monitoring and Observability</h2>
<ul>
<li><strong>Retrieval Metrics:</strong> Track precision, recall, and latency</li>
<li><strong>Generation Metrics:</strong> Monitor LLM latency and token usage</li>
<li><strong>User Feedback:</strong> Collect thumbs up/down on responses</li>
<li><strong>Cost Tracking:</strong> Monitor embedding and LLM API costs</li>
</ul>

<h2>Cost Optimization</h2>
<ul>
<li>Cache embeddings for frequently queried content</li>
<li>Use smaller LLMs (Gemini Flash) for simple queries</li>
<li>Implement query classification to route appropriately</li>
<li>Batch embedding generation during indexing</li>
<li>Set max_output_tokens to prevent excessive generation</li>
</ul>

<h2>Key Takeaways</h2>
<ul>
<li>Production RAG requires hybrid retrieval, re-ranking, and robust error handling</li>
<li>Prompt engineering significantly impacts response quality and accuracy</li>
<li>Multi-turn conversations need conversation history for context</li>
<li>Caching and async processing reduce latency by 50%+</li>
<li>Monitor retrieval quality, generation quality, and costs continuously</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
