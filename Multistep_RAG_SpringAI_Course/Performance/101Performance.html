<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Module 10: Performance and Optimization</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Module 10: Performance Optimization and Caching</h1>

<p>Multi-step RAG pipelines can be slow and expensive. Every additional step (query transformation, re-ranking, multi-stage retrieval) adds latency. This module covers techniques for making your Spring AI applications faster and more efficient.</p>

<h2>10.1 Semantic Caching</h2>
<p>Traditional caching uses exact string matches. Semantic caching is smarter: it uses embeddings to find previous queries that are *semantically identical* to the current one.
<ul>
    <li><strong>Workflow:</strong> When a query comes in, we embed it and search our cache (e.g., in Redis) for similar queries. If we find one with a high similarity score (e.g., > 0.98), we return the cached answer.</li>
    <li><strong>Benefits:</strong> Drastically reduces latency and token costs for common questions.</li>
    <li><strong>Risks:</strong> Potential for "stale" or slightly incorrect answers if the underlying documentation has changed.</li>
</ul></p>

<h2>10.2 Parallel Processing</h2>
<p>Many steps in a multi-step pipeline can be performed in parallel. For example, if you are generating multiple queries for retrieval, you can execute all the searches simultaneously using Java's <code>CompletableFuture</code>.
<div class="code-block">
<pre><code>List&lt;CompletableFuture&lt;List&lt;Document&gt;&gt;&gt; futures = queries.stream()
    .map(q -> CompletableFuture.supplyAsync(() -> vectorStore.similaritySearch(q)))
    .toList();

CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();</code></pre>
</div></p>

<h2>10.3 Streaming Responses</h2>
<p>To improve the "perceived" latency for the end-user, always use streaming responses (SSE). This allows the user to start reading the answer as it's being generated, rather than waiting for the entire multi-step process to complete. Spring AI provides native support for <code>Flux&lt;String&gt;</code> responses.</p>

<h2>10.4 Model Selection and Distillation</h2>
<p>Not every step requires a "frontier" model like GPT-4o.
<ul>
    <li><strong>Router Models:</strong> Use a small, fast model (like Llama 3 8B or Claude Haiku) to decide which path the query should take.</li>
    <li><strong>Summarization/Extraction:</strong> Smaller models are often excellent at these specific tasks.</li>
    <li><strong>Save the Big Model for the Final Step:</strong> Use the most capable model only for the final synthesis and generation.</li>
</ul></p>

<h2>10.5 Vector Store Tuning</h2>
<ul>
    <li><strong>Indexing:</strong> Choose the right index type (e.g., HNSW) for your vector store to balance speed and accuracy.</li>
    <li><strong>Metadata Filtering:</strong> Use metadata filters (like <code>tenant_id</code> or <code>document_type</code>) to narrow down the search space before performing the vector comparison.</li>
</ul>

<p>By applying these optimization techniques, you can transform a slow, experimental RAG pipeline into a snappy, production-ready feature that provides a great user experience.</p>

<script type="text/javascript">
</script>
</body>
</html>
