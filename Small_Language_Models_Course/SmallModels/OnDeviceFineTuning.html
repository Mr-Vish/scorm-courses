<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>On-Device Deployment and Fine-Tuning</title>
    <style type="text/css" media="screen">
        @import url( ../shared/style.css );
    </style>
    <script src="../shared/scormfunctions.js" type="text/javascript"></script>
    <script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>

<h1>On-Device Deployment and Fine-Tuning</h1>

<h2>Why On-Device AI Matters</h2>
<p>
On-device deployment refers to running machine learning models directly on user-controlled
hardware such as phones, laptops, edge devices, or embedded systems. Instead of sending data
to a cloud service, inference happens locally.
</p>

<p>
This approach fundamentally changes the trust, cost, and performance profile of AI systems.
Users gain privacy and responsiveness, while organizations reduce infrastructure dependency
and operational expense.
</p>

<h2>Key Benefits of On-Device Deployment</h2>
<ul>
    <li><strong>Privacy:</strong> Sensitive data never leaves the device</li>
    <li><strong>Latency:</strong> Inference happens in milliseconds, not seconds</li>
    <li><strong>Offline capability:</strong> Works without network connectivity</li>
    <li><strong>Cost control:</strong> No per-request inference billing</li>
    <li><strong>Reliability:</strong> No dependency on external APIs</li>
</ul>

<p>
These advantages are the primary reason small language models are gaining adoption across
consumer and enterprise applications.
</p>

<h2>On-Device Deployment Targets</h2>
<table>
    <tr><th>Platform</th><th>Suitable Models</th><th>Framework</th><th>Notes</th></tr>
    <tr><td>iPhone / iPad</td><td>1-3B quantized</td><td>Core ML, MLX</td><td>Apple Neural Engine acceleration</td></tr>
    <tr><td>Android</td><td>1-3B quantized</td><td>TensorFlow Lite, MediaPipe</td><td>GPU delegate for speed</td></tr>
    <tr><td>Laptop (Mac)</td><td>3-14B quantized</td><td>Ollama, MLX, llama.cpp</td><td>Metal GPU acceleration</td></tr>
    <tr><td>Laptop (Windows)</td><td>3-14B quantized</td><td>Ollama, llama.cpp</td><td>CUDA or CPU inference</td></tr>
    <tr><td>Raspberry Pi</td><td>0.5-1B quantized</td><td>llama.cpp</td><td>CPU only, aggressive quantization</td></tr>
    <tr><td>Web browser</td><td>0.5-2B quantized</td><td>WebLLM, Transformers.js</td><td>WebGPU acceleration</td></tr>
</table>

<h2>Understanding Quantization</h2>
<p>
Quantization reduces the precision of model weights, typically from 16-bit floating point
to 8-bit, 4-bit, or even lower representations. This dramatically reduces memory usage
and increases inference speed.
</p>

<p>
For on-device deployment, quantization is not optional—it is essential.
</p>

<ul>
    <li>FP16 → baseline accuracy, high memory usage</li>
    <li>INT8 → good balance of quality and speed</li>
    <li>Q4 / Q5 → optimal for laptops and phones</li>
    <li>Q2 / Q3 → extreme compression, reduced quality</li>
</ul>

<p>
Modern quantization methods preserve surprisingly high quality, especially for instruction-
tuned models.
</p>

<h2>Apple MLX for Local Inference</h2>
<p>
MLX is Apple’s machine learning framework designed specifically for Apple Silicon. It allows
efficient execution of transformer models using unified memory and Metal acceleration.
</p>

<div class="code-block">
<pre><code>
# Install MLX language model tools
pip install mlx-lm

from mlx_lm import load, generate

# Load a quantized Phi-3 model
model, tokenizer = load(
    "mlx-community/Phi-3-mini-4k-instruct-4bit"
)

prompt = "Explain the concept of recursion in programming."

response = generate(
    model,
    tokenizer,
    prompt=prompt,
    max_tokens=200,
    temp=0.7,
)

print(response)
</code></pre>
</div>

<p>
This setup allows high-quality LLM inference entirely on a MacBook without cloud access.
</p>

<h2>On-Device Performance Considerations</h2>
<p>
On-device performance depends on several interacting factors:
</p>

<ul>
    <li>Model size and quantization level</li>
    <li>Available RAM and memory bandwidth</li>
    <li>Hardware accelerators (GPU, NPU, ANE)</li>
    <li>Batch size (usually 1 for interactive use)</li>
</ul>

<p>
Unlike server environments, on-device systems are typically optimized for single-user,
low-latency inference rather than throughput.
</p>

<h2>Why Fine-Tune Small Models?</h2>
<p>
Out-of-the-box models are trained to be generalists. Fine-tuning adapts them to your specific
domain, terminology, and tasks.
</p>

<p>
For small models, fine-tuning can produce dramatic quality improvements, often exceeding
the performance of much larger general-purpose models.
</p>

<h2>LoRA: Low-Rank Adaptation</h2>
<p>
LoRA works by injecting small, trainable matrices into existing model layers while freezing
the original weights. This approach:
</p>

<ul>
    <li>Reduces trainable parameters by 99%+</li>
    <li>Lowers memory and compute requirements</li>
    <li>Allows fast iteration and experimentation</li>
    <li>Preserves the base model for reuse</li>
</ul>

<p>
LoRA is the dominant fine-tuning method for small and medium language models.
</p>

<h2>Fine-Tuning Small Models with LoRA</h2>
<div class="code-block">
<pre><code>
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct"
)
tokenizer = AutoTokenizer.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct"
)

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)

# Fine-tuning configuration
config = SFTConfig(
    output_dir="./phi3-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-4,
)

trainer = SFTTrainer(
    model=model,
    args=config,
    train_dataset=my_dataset,
    tokenizer=tokenizer,
)

trainer.train()
</code></pre>
</div>

<h2>Fine-Tuning Cost Comparison</h2>
<table>
    <tr><th>Model</th><th>LoRA Params</th><th>GPU Required</th><th>Training Time (1K samples)</th></tr>
    <tr><td>Phi-3 Mini (3.8B)</td><td>~2M</td><td>8 GB VRAM</td><td>~30 minutes</td></tr>
    <tr><td>Gemma 2 9B</td><td>~5M</td><td>16 GB VRAM</td><td>~1 hour</td></tr>
    <tr><td>Llama 3 8B</td><td>~4M</td><td>16 GB VRAM</td><td>~1 hour</td></tr>
    <tr><td>Llama 3 70B</td><td>~20M</td><td>80+ GB VRAM</td><td>~8 hours</td></tr>
</table>

<h2>Common Pitfalls</h2>
<ul>
    <li>Overfitting on small datasets</li>
    <li>Using too high a LoRA rank</li>
    <li>Skipping evaluation on real tasks</li>
    <li>Deploying without quantization</li>
    <li>Assuming benchmarks reflect real usage</li>
</ul>

<h2>Key Takeaways</h2>
<ul>
    <li><strong>On-device AI is practical today</strong> with small, quantized models</li>
    <li><strong>Quantization is mandatory</strong> for edge deployment</li>
    <li><strong>LoRA enables efficient fine-tuning</strong> with minimal resources</li>
    <li><strong>Smaller models scale better</strong> operationally and financially</li>
    <li><strong>Benchmark on your data</strong>, not generic leaderboards</li>
</ul>

<script type="text/javascript">
</script>

</body>
</html>
