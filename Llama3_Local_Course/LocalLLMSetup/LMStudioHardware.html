<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>LM Studio and Hardware Requirements</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>LM Studio and Hardware Requirements</h1>


<h2>What is LM Studio?</h2>
<p>LM Studio is a desktop application with a graphical interface for downloading, configuring, and running LLMs locally. It supports GGUF model files from Hugging Face and provides a chat interface, local API server, and model comparison tools.</p>

<h2>LM Studio Features</h2>
<table>
    <tr><th>Feature</th><th>Description</th></tr>
    <tr><td>Model Discovery</td><td>Search and download models directly from Hugging Face</td></tr>
    <tr><td>Chat Interface</td><td>Built-in chat UI with conversation history</td></tr>
    <tr><td>Local Server</td><td>OpenAI-compatible API server for integration</td></tr>
    <tr><td>GPU Offloading</td><td>Configure how many layers run on GPU vs CPU</td></tr>
    <tr><td>Model Comparison</td><td>Run the same prompt against multiple models side by side</td></tr>
    <tr><td>Preset Management</td><td>Save and load inference parameter presets</td></tr>
</table>

<h2>Hardware Requirements</h2>
<table>
    <tr><th>Model Size</th><th>Quantization</th><th>RAM / VRAM</th><th>Suitable Hardware</th></tr>
    <tr><td>3B parameters</td><td>Q4_K_M</td><td>~3 GB</td><td>Any modern laptop, 8 GB RAM</td></tr>
    <tr><td>7-8B parameters</td><td>Q4_K_M</td><td>~5-6 GB</td><td>16 GB RAM laptop, or GPU with 6+ GB VRAM</td></tr>
    <tr><td>13B parameters</td><td>Q4_K_M</td><td>~9 GB</td><td>GPU with 10+ GB VRAM, or 32 GB RAM for CPU</td></tr>
    <tr><td>34B parameters</td><td>Q4_K_M</td><td>~20 GB</td><td>GPU with 24 GB VRAM (RTX 4090)</td></tr>
    <tr><td>70B parameters</td><td>Q4_K_M</td><td>~40 GB</td><td>Multi-GPU setup or 64+ GB RAM for CPU</td></tr>
</table>

<h2>GPU vs CPU Inference</h2>
<div class="code-block">
<pre><code># Ollama automatically uses GPU if available
# Check GPU usage:
ollama ps  # Shows which models are loaded and GPU utilization

# For partial GPU offloading in Ollama, set layers:
# OLLAMA_NUM_GPU_LAYERS=20 ollama run llama3

# In LM Studio:
# Use the GPU Offload slider to control how many layers go to GPU
# More layers on GPU = faster inference, more VRAM needed</code></pre>
</div>

<h2>Performance Comparison</h2>
<table>
    <tr><th>Setup</th><th>Llama 3 8B Speed</th><th>Notes</th></tr>
    <tr><td>Apple M2 Pro (CPU)</td><td>~20 tokens/sec</td><td>Good for interactive use</td></tr>
    <tr><td>Apple M2 Pro (Metal)</td><td>~40 tokens/sec</td><td>Excellent for daily use</td></tr>
    <tr><td>RTX 3080 10 GB</td><td>~60 tokens/sec</td><td>Fast, fits 8B models fully</td></tr>
    <tr><td>RTX 4090 24 GB</td><td>~100 tokens/sec</td><td>Handles 13B+ models easily</td></tr>
    <tr><td>CPU only (Intel i7)</td><td>~5 tokens/sec</td><td>Usable but slow for long outputs</td></tr>
</table>

<h2>Choosing Between Ollama and LM Studio</h2>
<ul>
    <li><strong>Ollama:</strong> Best for developers, CLI-first, easy API integration, lighter resource usage, scriptable</li>
    <li><strong>LM Studio:</strong> Best for exploration, visual model management, side-by-side comparison, non-developers</li>
    <li><strong>Both:</strong> Support GGUF format, provide OpenAI-compatible APIs, and run on Mac/Windows/Linux</li>
</ul>


<script type="text/javascript">
</script>
</body>
</html>