<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Ollama Installation and Usage</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Ollama Installation and Usage</h1>


<h2>Why Run LLMs Locally?</h2>
<p>Running models on your own hardware offers data privacy (nothing leaves your machine), zero API costs, offline availability, and full control over model behavior. With modern quantized models, you can run capable LLMs on consumer hardware.</p>

<h2>What is Ollama?</h2>
<p>Ollama is an open-source tool that makes running LLMs locally as simple as running Docker containers. It handles model downloading, quantization format management, and provides an OpenAI-compatible API.</p>

<h2>Installation</h2>
<div class="code-block">
<pre><code># macOS / Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows - download installer from ollama.ai
# Or via winget:
winget install Ollama.Ollama

# Verify installation
ollama --version</code></pre>
</div>

<h2>Running Models</h2>
<div class="code-block">
<pre><code># Pull and run Llama 3 (8B parameters)
ollama run llama3

# Pull a specific size
ollama pull llama3:70b

# List downloaded models
ollama list

# Run with a system prompt
ollama run llama3 "You are a Python expert. Explain decorators."

# Run Mistral, Gemma, Phi, or other models
ollama run mistral
ollama run gemma2:9b
ollama run phi3</code></pre>
</div>

<h2>Using the Ollama API</h2>
<div class="code-block">
<pre><code># Ollama exposes an OpenAI-compatible API on localhost:11434
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama",  # Required but unused
)

response = client.chat.completions.create(
    model="llama3",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain REST APIs in simple terms."},
    ],
)
print(response.choices[0].message.content)</code></pre>
</div>

<h2>Popular Models on Ollama</h2>
<table>
    <tr><th>Model</th><th>Parameters</th><th>RAM Needed</th><th>Best For</th></tr>
    <tr><td>llama3:8b</td><td>8B</td><td>~6 GB</td><td>General purpose, good balance</td></tr>
    <tr><td>llama3:70b</td><td>70B</td><td>~40 GB</td><td>High quality, needs beefy hardware</td></tr>
    <tr><td>mistral:7b</td><td>7B</td><td>~5 GB</td><td>Fast, efficient reasoning</td></tr>
    <tr><td>gemma2:9b</td><td>9B</td><td>~7 GB</td><td>Google's efficient model</td></tr>
    <tr><td>phi3:mini</td><td>3.8B</td><td>~3 GB</td><td>Small devices, quick responses</td></tr>
    <tr><td>codellama:13b</td><td>13B</td><td>~9 GB</td><td>Code generation and completion</td></tr>
</table>


<script type="text/javascript">
</script>
</body>
</html>