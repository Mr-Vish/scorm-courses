<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>RAG Implementation in UI Applications</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>RAG Implementation in UI Applications</h1>

<h2>Understanding RAG (Retrieval-Augmented Generation)</h2>
<p>RAG combines information retrieval with LLM generation to provide accurate, context-aware responses based on your own documents. This is essential for building domain-specific AI applications.</p>

<h2>RAG Architecture Overview</h2>
<div style="margin: 2rem 0; padding: 1.5rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #4CAF50;">
    <div style="margin-bottom: 1rem;">
        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
            <div style="width: 30px; height: 30px; background: #4CAF50; color: white; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem;">1</div>
            <strong>Document Ingestion:</strong> Upload and process documents
        </div>
    </div>
    <div style="margin-bottom: 1rem;">
        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
            <div style="width: 30px; height: 30px; background: #2196F3; color: white; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem;">2</div>
            <strong>Text Chunking:</strong> Split documents into manageable pieces
        </div>
    </div>
    <div style="margin-bottom: 1rem;">
        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
            <div style="width: 30px; height: 30px; background: #FF9800; color: white; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem;">3</div>
            <strong>Embedding Generation:</strong> Convert chunks to vector embeddings
        </div>
    </div>
    <div style="margin-bottom: 1rem;">
        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
            <div style="width: 30px; height: 30px; background: #9C27B0; color: white; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem;">4</div>
            <strong>Vector Storage:</strong> Store embeddings in vector database
        </div>
    </div>
    <div style="margin-bottom: 1rem;">
        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
            <div style="width: 30px; height: 30px; background: #F44336; color: white; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem;">5</div>
            <strong>Query & Retrieval:</strong> Find relevant chunks for user questions
        </div>
    </div>
    <div>
        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
            <div style="width: 30px; height: 30px; background: #607D8B; color: white; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 1rem;">6</div>
            <strong>Generation:</strong> LLM generates answer using retrieved context
        </div>
    </div>
</div>

<h2>Basic RAG with Gradio and LangChain</h2>
<blockquote>
import gradio as gr
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import tempfile
import os

# Global variables
vector_store = None
qa_chain = None

def process_document(file):
    '''Process uploaded PDF and create vector store.'''
    global vector_store, qa_chain
    
    try:
        # Save uploaded file temporarily
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(file)
            tmp_path = tmp_file.name
        
        # Load PDF
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        
        # Split into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        
        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings()
        vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
        
        # Create QA chain
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 3})
        )
        
        # Clean up
        os.unlink(tmp_path)
        
        return f"âœ“ Document processed: {len(chunks)} chunks created"
        
    except Exception as e:
        return f"âœ— Error: {str(e)}"

def query_document(message, history):
    '''Query the processed document.'''
    global qa_chain
    
    if qa_chain is None:
        yield "Please upload and process a document first."
        return
    
    try:
        # Get answer
        result = qa_chain.invoke({"query": message})
        answer = result["result"]
        
        # Stream the response
        partial = ""
        for char in answer:
            partial += char
            yield partial
            
    except Exception as e:
        yield f"Error: {str(e)}"

with gr.Blocks() as demo:
    gr.Markdown("# Document Q&A with RAG")
    
    with gr.Row():
        with gr.Column(scale=1):
            file_upload = gr.File(
                label="Upload PDF Document",
                file_types=[".pdf"]
            )
            process_btn = gr.Button("Process Document", variant="primary")
            status = gr.Textbox(label="Status", interactive=False)
            
            process_btn.click(
                fn=process_document,
                inputs=file_upload,
                outputs=status
            )
        
        with gr.Column(scale=2):
            chatbot = gr.ChatInterface(
                fn=query_document,
                title="Ask Questions",
                examples=[
                    "What is the main topic?",
                    "Summarize the key points",
                    "What are the conclusions?"
                ]
            )

demo.launch()
</blockquote>

<h2>Advanced RAG with Streamlit</h2>
<blockquote>
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import tempfile
import os

st.title("Advanced RAG System")

# Initialize session state
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# Sidebar for document upload
with st.sidebar:
    st.header("Document Management")
    
    uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])
    
    chunk_size = st.slider("Chunk Size", 500, 2000, 1000)
    chunk_overlap = st.slider("Chunk Overlap", 0, 500, 200)
    top_k = st.slider("Chunks to Retrieve", 1, 10, 3)
    
    if st.button("Process Document") and uploaded_file:
        with st.spinner("Processing document..."):
            try:
                # Save uploaded file
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(uploaded_file.read())
                    tmp_path = tmp_file.name
                
                # Load and split
                loader = PyPDFLoader(tmp_path)
                documents = loader.load()
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                chunks = text_splitter.split_documents(documents)
                
                # Create vector store
                embeddings = OpenAIEmbeddings()
                st.session_state.vector_store = Chroma.from_documents(
                    documents=chunks,
                    embedding=embeddings
                )
                
                # Create conversational chain with memory
                memory = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True,
                    output_key="answer"
                )
                
                llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
                st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=llm,
                    retriever=st.session_state.vector_store.as_retriever(
                        search_kwargs={"k": top_k}
                    ),
                    memory=memory,
                    return_source_documents=True
                )
                
                os.unlink(tmp_path)
                
                st.success(f"âœ“ Processed {len(chunks)} chunks")
                
            except Exception as e:
                st.error(f"Error: {str(e)}")

# Main chat area
if st.session_state.qa_chain is None:
    st.info("ðŸ‘ˆ Upload a document to get started")
else:
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # Show sources if available
            if "sources" in message:
                with st.expander("View Sources"):
                    for i, source in enumerate(message["sources"]):
                        st.markdown(f"**Source {i+1}:**")
                        st.text(source.page_content[:300] + "...")
    
    # Chat input
    if prompt := st.chat_input("Ask about the document"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                result = st.session_state.qa_chain.invoke({"question": prompt})
                answer = result["answer"]
                sources = result["source_documents"]
                
                st.markdown(answer)
                
                with st.expander("View Sources"):
                    for i, source in enumerate(sources):
                        st.markdown(f"**Source {i+1}:**")
                        st.text(source.page_content[:300] + "...")
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": sources
        })
</blockquote>

<h2>Multi-Document RAG System</h2>
<blockquote>
import gradio as gr
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import tempfile
import os

class MultiDocRAG:
    '''Handle multiple documents in a single RAG system.'''
    
    def __init__(self):
        self.vector_store = None
        self.qa_chain = None
        self.documents = []
    
    def add_document(self, file, file_type):
        '''Add a document to the system.'''
        try:
            # Save temporarily
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as tmp_file:
                tmp_file.write(file)
                tmp_path = tmp_file.name
            
            # Load based on type
            if file_type == 'pdf':
                loader = PyPDFLoader(tmp_path)
            else:
                loader = TextLoader(tmp_path)
            
            docs = loader.load()
            
            # Add metadata
            for doc in docs:
                doc.metadata['source_file'] = file.name
            
            self.documents.extend(docs)
            
            os.unlink(tmp_path)
            
            return f"âœ“ Added {file.name}: {len(docs)} pages"
            
        except Exception as e:
            return f"âœ— Error adding {file.name}: {str(e)}"
    
    def build_index(self):
        '''Build vector index from all documents.'''
        if not self.documents:
            return "No documents to index"
        
        try:
            # Split all documents
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            chunks = text_splitter.split_documents(self.documents)
            
            # Create vector store
            embeddings = OpenAIEmbeddings()
            self.vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings
            )
            
            # Create QA chain
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=self.vector_store.as_retriever(search_kwargs={"k": 5}),
                return_source_documents=True
            )
            
            return f"âœ“ Index built: {len(chunks)} chunks from {len(self.documents)} documents"
            
        except Exception as e:
            return f"âœ— Error building index: {str(e)}"
    
    def query(self, question):
        '''Query across all documents.'''
        if self.qa_chain is None:
            return "Please build the index first", []
        
        try:
            result = self.qa_chain.invoke({"query": question})
            answer = result["result"]
            sources = result["source_documents"]
            
            # Format sources
            source_info = []
            for doc in sources:
                source_info.append(f"From {doc.metadata.get('source_file', 'Unknown')}: {doc.page_content[:200]}...")
            
            return answer, "\n\n".join(source_info)
            
        except Exception as e:
            return f"Error: {str(e)}", ""

# Create interface
rag_system = MultiDocRAG()

def add_doc(file):
    file_type = file.name.split('.')[-1]
    return rag_system.add_document(file, file_type)

def build_idx():
    return rag_system.build_index()

def query_docs(question):
    answer, sources = rag_system.query(question)
    return answer, sources

with gr.Blocks() as demo:
    gr.Markdown("# Multi-Document RAG System")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### 1. Upload Documents")
            file_input = gr.File(label="Upload PDF or TXT")
            add_btn = gr.Button("Add Document")
            add_status = gr.Textbox(label="Status")
            
            gr.Markdown("### 2. Build Index")
            build_btn = gr.Button("Build Index", variant="primary")
            build_status = gr.Textbox(label="Index Status")
            
            add_btn.click(fn=add_doc, inputs=file_input, outputs=add_status)
            build_btn.click(fn=build_idx, outputs=build_status)
        
        with gr.Column():
            gr.Markdown("### 3. Query Documents")
            question_input = gr.Textbox(label="Question", lines=2)
            query_btn = gr.Button("Ask")
            answer_output = gr.Textbox(label="Answer", lines=5)
            sources_output = gr.Textbox(label="Sources", lines=5)
            
            query_btn.click(
                fn=query_docs,
                inputs=question_input,
                outputs=[answer_output, sources_output]
            )

demo.launch()
</blockquote>

<h2>RAG with Hybrid Search</h2>
<blockquote>
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

def create_hybrid_retriever(documents):
    '''Combine semantic and keyword search.'''
    
    # Semantic search with embeddings
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(documents, embeddings)
    semantic_retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    
    # Keyword search with BM25
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = 3
    
    # Combine both
    hybrid_retriever = EnsembleRetriever(
        retrievers=[semantic_retriever, bm25_retriever],
        weights=[0.5, 0.5]  # Equal weight to both methods
    )
    
    return hybrid_retriever
</blockquote>

<h2>RAG Best Practices</h2>
<ul>
    <li><strong>Optimal Chunk Size:</strong> Use 500-1000 tokens with 10-20% overlap</li>
    <li><strong>Metadata Enrichment:</strong> Add source, page numbers, and timestamps to chunks</li>
    <li><strong>Hybrid Search:</strong> Combine semantic and keyword search for better retrieval</li>
    <li><strong>Reranking:</strong> Use a reranker model to improve retrieved chunk relevance</li>
    <li><strong>Citation:</strong> Always show sources to users for transparency</li>
    <li><strong>Caching:</strong> Cache embeddings and frequently accessed chunks</li>
    <li><strong>Incremental Updates:</strong> Support adding documents without rebuilding entire index</li>
    <li><strong>Query Expansion:</strong> Rephrase user questions for better retrieval</li>
    <li><strong>Context Window Management:</strong> Fit retrieved chunks within LLM context limits</li>
    <li><strong>Evaluation:</strong> Test retrieval quality with sample questions</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>