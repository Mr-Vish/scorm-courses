<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Deployment and Scaling</title>
    <style type="text/css" media="screen">
        @import url( ../shared/style.css );
    </style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>																	   
																									  
</head>
<body>
							   

<h1>Deployment and Scaling</h1>

<h2>vLLM vs TGI Comparison</h2>
<table>
<tr>
    <th>Feature</th>
    <th>vLLM</th>
    <th>TGI</th>
</tr>
<tr>
    <td>Core innovation</td>
    <td>PagedAttention</td>
    <td>Flash Attention + batching</td>
</tr>
<tr>
    <td>API</td>
    <td>OpenAI-compatible</td>
    <td>REST + OpenAI</td>
</tr>
<tr>
    <td>Quantization</td>
    <td>GPTQ, AWQ, FP8</td>
    <td>GPTQ, AWQ</td>
</tr>
<tr>
    <td>LoRA support</td>
    <td>Multiple adapters</td>
    <td>Single adapter</td>
</tr>
</table>

<h2>Tensor Parallelism for Large Models</h2>
<pre><code>
# Run a 70B model across 4 GPUs
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3-70b-instruct \
  --tensor-parallel-size 4 \
  --max-model-len 8192
</code></pre>

<p>
Each GPU holds a shard of the model weights. Fast interconnects (NVLink) are strongly
recommended to avoid communication bottlenecks.
																	   
</p>

<h2>Production Deployment (Docker)</h2>
						
<pre><code>
					
services:
  vllm:
    image: vllm/vllm-openai:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "8000:8000"
    volumes:
      - model-cache:/root/.cache
    command: >
      --model meta-llama/Llama-3-8b-instruct
      --max-model-len 8192
      --gpu-memory-utilization 0.9
				
																 
				   

		
					   
		  
			   
</code></pre>
	  

<h2>Scaling Strategies</h2>
<table>
<tr>
    <th>Strategy</th>
    <th>When to Use</th>
</tr>
<tr>
    <td>Vertical scaling</td>
    <td>Larger models, lower latency</td>
</tr>
<tr>
    <td>Horizontal scaling</td>
    <td>Higher throughput</td>
</tr>
<tr>
    <td>Autoscaling</td>
    <td>Bursty or unpredictable traffic</td>
</tr>
</table>

<h2>Performance Tuning</h2>
<ul>
																																								   
    <li>gpu-memory-utilization: 0.85–0.95 recommended</li>
    <li>Cap max-model-len aggressively</li>
    <li>Increase batch size for throughput-oriented workloads</li>
    <li>Enable prefix caching for shared prompts</li>
</ul>

<h2>Memory and Throughput Expectations</h2>
<ul>
    <li>7B FP16 model: ~14 GB VRAM</li>
    <li>13B FP16 model: ~26 GB VRAM</li>
    <li>7B model on A100: 50–150 req/s (short responses)</li>
</ul>

<h2>Limitations</h2>
<ul>
    <li>GPU-only serving</li>
    <li>Requires careful SLA and latency tuning</li>
    <li>No built-in request routing or quota management</li>
</ul>

<h2>Best Organizational Fit</h2>
<ul>
    <li>AI-first startups</li>
    <li>SaaS platforms</li>
    <li>Enterprise internal AI teams</li>
</ul>

</body>
</html>
