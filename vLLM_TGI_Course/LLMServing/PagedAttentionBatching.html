<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>PagedAttention and Continuous Batching</title>
    <style type="text/css" media="screen">
        @import url( ../shared/style.css );
    </style>
												<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>						   
																									  
</head>
<body>
																	  

<h1>PagedAttention and Continuous Batching</h1>

<h2>The LLM Serving Challenge</h2>
<p>
Serving large language models in production is challenging due to GPU memory limits,
variable prompt lengths, and unpredictable concurrency. Traditional static batching
and pre-allocated KV-cache strategies lead to wasted memory and poor GPU utilization.
</p>

<h2>What is vLLM?</h2>
<p>
vLLM is an open-source, high-throughput LLM inference engine designed specifically
for serving workloads. Its primary goal is to maximize GPU utilization while
supporting large numbers of concurrent requests.
</p>

<h2>PagedAttention Explained</h2>
<table>
<tr>
    <th>Traditional KV Cache</th>
    <th>PagedAttention</th>
</tr>
<tr>
    <td>Allocates memory for max sequence length</td>
    <td>Allocates fixed-size pages on demand</td>
</tr>
<tr>
    <td>Requires contiguous memory</td>
    <td>Non-contiguous allocation</td>
</tr>
<tr>
    <td>60–80% KV-cache waste</td>
    <td>Near-zero memory waste</td>
</tr>
<tr>
    <td>Limits concurrency</td>
    <td>2–4× higher concurrency</td>
</tr>
</table>

<h3>Why This Matters</h3>
<ul>
    <li>Efficient handling of variable-length prompts</li>
    <li>More concurrent users per GPU</li>
    <li>Copy-on-write sharing for identical prefixes (system prompts, RAG templates)</li>
</ul>

<h2>Continuous Batching</h2>
<p>
Instead of waiting for an entire batch to complete, continuous batching dynamically
adds and removes requests from the active batch.
</p>
<ul>
    <li>Completed requests are immediately replaced</li>
    <li>Short and long requests can coexist</li>
    <li>10–20× higher throughput vs static batching</li>
</ul>

<h2>Running vLLM</h2>
<pre><code>
# Install vLLM
pip install vllm

									
																																																				

									   
						 

# Start OpenAI-compatible API server
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3-8b-instruct \
  --tensor-parallel-size 1 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.9
</code></pre>
	<h2>Text Generation Inference (TGI)</h2>
<p>TGI is Hugging Face's production-grade LLM serving solution:</p>
<div class="code-block">
<pre><code># Run TGI with Docker
docker run --gpus all -p 8080:80     -v /data:/data     ghcr.io/huggingface/text-generation-inference:latest     --model-id meta-llama/Llama-3-8b-instruct     --max-input-length 4096     --max-total-tokens 8192
	 

# TGI provides both REST and gRPC APIs
curl http://localhost:8080/generate     -X POST     -H "Content-Type: application/json"     -d '{"inputs": "What is deep learning?", "parameters": {"max_new_tokens": 200}}'</code></pre>
									  
												   
							  
											 
</div>

  

<h2>Corner Cases and Cautions</h2>
<ul>
    <li>Long context (8k–32k) combined with high concurrency can exhaust GPU memory</li>
    <li>Over-aggressive gpu-memory-utilization may cause OOM crashes</li>
    <li>Mixed workloads (short + very long generations) can increase latency variance</li>
</ul>

<h2>Best-Suited Use Cases</h2>
<ul>
    <li>High-concurrency chatbots</li>
    <li>RAG systems with shared system prompts</li>
    <li>Internal copilots</li>
    <li>SaaS platforms exposing LLM APIs</li>
</ul>

<h2>Limitations</h2>
<ul>
    <li>GPU-only inference</li>
    <li>Not optimized for ultra-low-latency single-user flows</li>
</ul>

							   
		 
</body>
</html>
