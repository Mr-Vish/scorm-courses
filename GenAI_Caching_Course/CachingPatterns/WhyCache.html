<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Why Cache GenAI Responses?</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Why Cache GenAI Responses?</h1>


<h2>The Case for Caching</h2>
<p>GenAI API calls are expensive ($0.003-$0.075 per 1K tokens) and slow (200ms-5s latency). Caching can dramatically reduce both costs and response times.</p>

<h2>Cost Savings Example</h2>
<table>
    <tr><th>Metric</th><th>Without Cache</th><th>With 50% Hit Rate</th></tr>
    <tr><td>Daily API calls</td><td>100,000</td><td>50,000</td></tr>
    <tr><td>Avg tokens/call</td><td>2,000</td><td>2,000</td></tr>
    <tr><td>Daily cost (Claude Sonnet)</td><td>$600</td><td>$300</td></tr>
    <tr><td>Monthly cost</td><td>$18,000</td><td>$9,000 + $50 cache</td></tr>
    <tr><td>Avg latency</td><td>1.5s</td><td>50ms (cache hit)</td></tr>
</table>

<h2>Caching Strategies</h2>
<table>
    <tr><th>Strategy</th><th>Description</th><th>Hit Rate</th><th>Complexity</th></tr>
    <tr><td>Exact Match</td><td>Cache identical prompts</td><td>Low (10-20%)</td><td>Simple</td></tr>
    <tr><td>Semantic Cache</td><td>Cache similar meaning prompts</td><td>High (40-60%)</td><td>Moderate</td></tr>
    <tr><td>Prompt Prefix Cache</td><td>Cache common system prompts</td><td>Very High</td><td>Simple</td></tr>
    <tr><td>Result Cache</td><td>Cache computed results</td><td>Varies</td><td>Simple</td></tr>
</table>

<h2>Exact Match Caching</h2>
<div class="code-block">
<pre><code>import hashlib
import json
import redis

redis_client = redis.Redis(host="localhost", port=6379)

def cached_llm_call(messages, model="claude-sonnet-4-20250514", ttl=3600):
    # Create cache key from input
    cache_key = hashlib.sha256(
        json.dumps({"messages": messages, "model": model}).encode()
    ).hexdigest()

    # Check cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # Call LLM
    response = call_llm(messages, model)

    # Store in cache
    redis_client.setex(cache_key, ttl, json.dumps(response))
    return response</code></pre>
</div>


<script type="text/javascript">
</script>
</body>
</html>