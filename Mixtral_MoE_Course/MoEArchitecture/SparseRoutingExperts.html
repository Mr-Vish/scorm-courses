<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Sparse Routing and Expert Specialization</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Sparse Routing and Expert Specialization</h1>


<h2>What is Mixture-of-Experts?</h2>
<p>Mixture-of-Experts (MoE) is a neural network architecture where each input is processed by only a subset of the model's parameters (experts). This allows models to have a very large total parameter count while keeping the computation per token relatively small.</p>

<h2>How MoE Works</h2>
<ol>
    <li><strong>Router network:</strong> A small neural network examines each token and decides which experts should process it</li>
    <li><strong>Expert selection:</strong> The router selects the top-K experts (typically 2 out of 8) for each token</li>
    <li><strong>Expert processing:</strong> Only the selected experts run their computations</li>
    <li><strong>Weighted combination:</strong> The outputs from selected experts are combined using router-assigned weights</li>
</ol>

<h2>Mixtral 8x7B Architecture</h2>
<table>
    <tr><th>Property</th><th>Value</th><th>Comparison to Dense</th></tr>
    <tr><td>Total parameters</td><td>46.7B</td><td>Size of a 47B model on disk</td></tr>
    <tr><td>Active parameters per token</td><td>~12.9B</td><td>Runs as fast as a 13B model</td></tr>
    <tr><td>Number of experts</td><td>8 per layer</td><td>Each expert is ~7B</td></tr>
    <tr><td>Active experts per token</td><td>2</td><td>Only 2 of 8 experts fire</td></tr>
    <tr><td>Context window</td><td>32K tokens</td><td>Comparable to dense 13B models</td></tr>
    <tr><td>Performance</td><td>Matches Llama 2 70B</td><td>At ~6x lower inference cost</td></tr>
</table>

<h2>The Router Network</h2>
<div class="code-block">
<pre><code># Simplified view of the MoE routing mechanism
# For each token:
# 1. Router computes scores for all experts
# 2. Top-K experts are selected
# 3. Scores are normalized with softmax

# Conceptual pseudocode:
def moe_forward(token_hidden_state, experts, router, top_k=2):
    # Router produces logits for each expert
    router_logits = router(token_hidden_state)  # shape: [num_experts]

    # Select top-k experts
    top_k_values, top_k_indices = torch.topk(router_logits, k=top_k)
    weights = torch.softmax(top_k_values, dim=-1)

    # Run only the selected experts
    output = torch.zeros_like(token_hidden_state)
    for i, expert_idx in enumerate(top_k_indices):
        expert_output = experts[expert_idx](token_hidden_state)
        output += weights[i] * expert_output

    return output</code></pre>
</div>

<h2>Expert Specialization</h2>
<p>Research shows that experts tend to specialize in different types of knowledge:</p>
<ul>
    <li><strong>Syntax experts:</strong> Handle grammatical structure and language patterns</li>
    <li><strong>Domain experts:</strong> Specialize in topics like science, code, or mathematics</li>
    <li><strong>Language experts:</strong> Handle specific languages in multilingual models</li>
    <li><strong>Token-type experts:</strong> Some experts activate more for punctuation, numbers, or rare words</li>
</ul>

<h2>Load Balancing</h2>
<p>A key challenge in MoE is ensuring all experts are utilized evenly. Without load balancing, some experts get overloaded while others are idle:</p>
<ul>
    <li><strong>Auxiliary loss:</strong> An additional loss term during training that penalizes uneven expert utilization</li>
    <li><strong>Expert capacity:</strong> Each expert has a maximum number of tokens it can process per batch</li>
    <li><strong>Dropped tokens:</strong> Tokens routed to a full expert may be dropped or sent to overflow experts</li>
</ul>


<script type="text/javascript">
</script>
</body>
</html>