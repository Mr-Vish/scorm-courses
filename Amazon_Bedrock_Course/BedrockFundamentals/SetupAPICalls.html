<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Setting Up Bedrock and Making API Calls</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Setting Up Bedrock and Making API Calls</h1>

<h3>Learning Objectives</h3>
<ul>
    <li>Understand the prerequisites and setup process for Amazon Bedrock</li>
    <li>Learn how to enable model access and configure permissions</li>
    <li>Master the API invocation patterns for different use cases</li>
    <li>Implement error handling and best practices for production applications</li>
</ul>

<h3>Prerequisites and Initial Setup</h3>

<h4>AWS Account Requirements</h4>
<p>Before using Amazon Bedrock, ensure your AWS environment meets these requirements:</p>
<ul>
    <li><strong>Active AWS Account:</strong> Bedrock is available in select AWS regions (us-east-1, us-west-2, eu-central-1, ap-southeast-1, and others)</li>
    <li><strong>IAM Permissions:</strong> Appropriate permissions to access Bedrock services and enable models</li>
    <li><strong>Service Quotas:</strong> Awareness of default quotas for API requests and token limits</li>
    <li><strong>Billing Setup:</strong> Valid payment method configured for pay-per-use charges</li>
</ul>

<h4>Required IAM Permissions</h4>
<p>Users or roles invoking Bedrock require specific IAM permissions. A minimal policy includes:</p>

<blockquote>
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": "arn:aws:bedrock:*::foundation-model/*"
    }
  ]
}
</blockquote>

<p>For comprehensive access including model management and agent operations:</p>

<blockquote>
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:*"
      ],
      "Resource": "*"
    }
  ]
}
</blockquote>

<h3>Enabling Model Access</h3>

<h4>Model Access Request Process</h4>
<p>Amazon Bedrock implements an opt-in model access system for governance and cost control. Before using any foundation model, you must explicitly request access:</p>

<p><strong>Step-by-Step Process:</strong></p>
<ol>
    <li>Navigate to the Amazon Bedrock console in your AWS account</li>
    <li>Select <strong>Model access</strong> from the left navigation panel</li>
    <li>Click <strong>Manage model access</strong> or <strong>Modify model access</strong></li>
    <li>Review available models and their terms of use</li>
    <li>Select the checkbox next to each model you wish to enable</li>
    <li>Click <strong>Request model access</strong> or <strong>Save changes</strong></li>
    <li>Wait for approval (most models are instantly available; some require manual review)</li>
</ol>

<h4>Model Access Considerations</h4>
<ul>
    <li><strong>Instant Access:</strong> Most models (Claude, Titan, Llama, Mistral) are immediately available upon request</li>
    <li><strong>Manual Review:</strong> Some models may require AWS review, typically completed within 1-2 business days</li>
    <li><strong>Regional Availability:</strong> Model access is region-specific; enable models in each region where you'll deploy applications</li>
    <li><strong>Organizational Policies:</strong> In AWS Organizations, model access can be managed centrally through service control policies</li>
</ul>

<h3>API Invocation Patterns</h3>

<h4>Synchronous Invocation</h4>
<p>Synchronous invocation follows a request-response pattern where the client waits for the complete response before proceeding. This pattern is suitable for batch processing, backend services, and scenarios where immediate streaming is not required.</p>

<p><strong>Python Example (Boto3):</strong></p>

<blockquote>
import boto3
import json

# Initialize Bedrock Runtime client
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-east-1'
)

# Prepare request payload
request_body = {
    "anthropic_version": "bedrock-2023-05-31",
    "max_tokens": 1024,
    "temperature": 0.7,
    "messages": [
        {
            "role": "user",
            "content": "Explain the benefits of serverless architecture in 3 paragraphs."
        }
    ]
}

# Invoke model
response = bedrock_runtime.invoke_model(
    modelId='anthropic.claude-3-sonnet-20240229-v1:0',
    contentType='application/json',
    accept='application/json',
    body=json.dumps(request_body)
)

# Parse response
response_body = json.loads(response['body'].read())
generated_text = response_body['content'][0]['text']

print(generated_text)
</blockquote>

<h4>Streaming Invocation</h4>
<p>Streaming invocation delivers responses progressively as tokens are generated, providing immediate user feedback and improved perceived performance. This pattern is essential for interactive applications like chatbots and real-time assistants.</p>

<p><strong>Python Streaming Example:</strong></p>

<blockquote>
import boto3
import json

bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-east-1'
)

request_body = {
    "anthropic_version": "bedrock-2023-05-31",
    "max_tokens": 1024,
    "messages": [
        {
            "role": "user",
            "content": "Write a short story about AI."
        }
    ]
}

# Invoke with streaming
response = bedrock_runtime.invoke_model_with_response_stream(
    modelId='anthropic.claude-3-sonnet-20240229-v1:0',
    contentType='application/json',
    accept='application/json',
    body=json.dumps(request_body)
)

# Process stream
stream = response['body']
for event in stream:
    chunk = json.loads(event['chunk']['bytes'])
    
    if chunk['type'] == 'content_block_delta':
        if 'delta' in chunk and 'text' in chunk['delta']:
            print(chunk['delta']['text'], end='', flush=True)
    
    elif chunk['type'] == 'message_stop':
        print("\n[Stream completed]")
</blockquote>

<h4>Request Parameters</h4>
<p>Understanding key request parameters enables fine-tuned control over model behavior:</p>

<table>
    <tr>
        <th>Parameter</th>
        <th>Purpose</th>
        <th>Typical Values</th>
        <th>Impact</th>
    </tr>
    <tr>
        <td class="rowheader">max_tokens</td>
        <td>Maximum length of generated response</td>
        <td>256-4096</td>
        <td>Controls output length and cost; set based on expected response size</td>
    </tr>
    <tr>
        <td class="rowheader">temperature</td>
        <td>Randomness/creativity of output</td>
        <td>0.0-1.0</td>
        <td>Lower (0.0-0.3) for factual/deterministic; higher (0.7-1.0) for creative</td>
    </tr>
    <tr>
        <td class="rowheader">top_p</td>
        <td>Nucleus sampling threshold</td>
        <td>0.0-1.0</td>
        <td>Alternative to temperature; typically use one or the other</td>
    </tr>
    <tr>
        <td class="rowheader">top_k</td>
        <td>Limits vocabulary to top K tokens</td>
        <td>1-500</td>
        <td>Reduces randomness by constraining token selection</td>
    </tr>
    <tr>
        <td class="rowheader">stop_sequences</td>
        <td>Strings that halt generation</td>
        <td>Custom strings</td>
        <td>Prevents over-generation; useful for structured outputs</td>
    </tr>
</table>

<h3>Model-Specific Request Formats</h3>

<h4>Anthropic Claude Models</h4>
<p>Claude models use a messages-based format with explicit role designation:</p>

<blockquote>
{
  "anthropic_version": "bedrock-2023-05-31",
  "max_tokens": 1024,
  "temperature": 0.5,
  "messages": [
    {
      "role": "user",
      "content": "Your question here"
    }
  ],
  "system": "Optional system prompt defining behavior"
}
</blockquote>

<h4>Amazon Titan Models</h4>
<p>Titan models use a simpler text-based format:</p>

<blockquote>
{
  "inputText": "Your prompt here",
  "textGenerationConfig": {
    "maxTokenCount": 512,
    "temperature": 0.7,
    "topP": 0.9,
    "stopSequences": []
  }
}
</blockquote>

<h4>Meta Llama Models</h4>
<p>Llama models support both simple and chat formats:</p>

<blockquote>
{
  "prompt": "Your prompt here",
  "max_gen_len": 512,
  "temperature": 0.5,
  "top_p": 0.9
}
</blockquote>

<h3>Error Handling and Resilience</h3>

<h4>Common Error Scenarios</h4>
<ul>
    <li><strong>ModelNotReadyException:</strong> Model access not enabled or still pending approval</li>
    <li><strong>ThrottlingException:</strong> Request rate exceeds service quotas</li>
    <li><strong>ValidationException:</strong> Invalid request parameters or format</li>
    <li><strong>ServiceQuotaExceededException:</strong> Account limits reached</li>
    <li><strong>ModelTimeoutException:</strong> Request processing exceeded timeout limits</li>
</ul>

<h4>Robust Error Handling Implementation</h4>

<blockquote>
import boto3
from botocore.exceptions import ClientError
import time

def invoke_bedrock_with_retry(model_id, request_body, max_retries=3):
    bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')
    
    for attempt in range(max_retries):
        try:
            response = bedrock_runtime.invoke_model(
                modelId=model_id,
                contentType='application/json',
                accept='application/json',
                body=json.dumps(request_body)
            )
            return json.loads(response['body'].read())
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            
            if error_code == 'ThrottlingException':
                # Exponential backoff for throttling
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"Throttled. Retrying in {wait_time:.2f} seconds...")
                time.sleep(wait_time)
                
            elif error_code == 'ModelNotReadyException':
                raise Exception("Model access not enabled. Enable in Bedrock console.")
                
            elif error_code == 'ValidationException':
                raise Exception(f"Invalid request: {e.response['Error']['Message']}")
                
            else:
                raise Exception(f"Unexpected error: {error_code}")
    
    raise Exception(f"Failed after {max_retries} retries")
</blockquote>

<h3>Best Practices for Production</h3>

<h4>1. Connection Management</h4>
<ul>
    <li>Reuse Boto3 clients across invocations to avoid connection overhead</li>
    <li>Implement connection pooling for high-throughput applications</li>
    <li>Configure appropriate timeout values based on expected response times</li>
</ul>

<h4>2. Request Optimization</h4>
<ul>
    <li>Minimize prompt length by removing unnecessary context</li>
    <li>Set appropriate max_tokens to prevent over-generation</li>
    <li>Use stop_sequences to halt generation at logical boundaries</li>
    <li>Cache responses for identical or similar queries</li>
</ul>

<h4>3. Monitoring and Logging</h4>
<ul>
    <li>Log all requests and responses for debugging and analysis</li>
    <li>Track token consumption for cost monitoring</li>
    <li>Monitor latency metrics (TTFT, total response time)</li>
    <li>Set up CloudWatch alarms for error rates and throttling</li>
</ul>

<h4>4. Security Considerations</h4>
<ul>
    <li>Never hardcode AWS credentials; use IAM roles or environment variables</li>
    <li>Implement input validation to prevent prompt injection attacks</li>
    <li>Sanitize user inputs before including in prompts</li>
    <li>Use VPC endpoints for private connectivity without internet exposure</li>
    <li>Encrypt sensitive data in prompts and responses</li>
</ul>

<h4>5. Cost Management</h4>
<ul>
    <li>Implement request quotas per user or application</li>
    <li>Monitor token consumption and set budget alerts</li>
    <li>Use efficient models for simple tasks</li>
    <li>Implement caching to reduce redundant API calls</li>
</ul>

<h3>Testing and Validation</h3>

<h4>Unit Testing Approach</h4>
<blockquote>
import unittest
from unittest.mock import patch, MagicMock

class TestBedrockIntegration(unittest.TestCase):
    
    @patch('boto3.client')
    def test_successful_invocation(self, mock_client):
        # Mock Bedrock response
        mock_response = {
            'body': MagicMock(read=lambda: json.dumps({
                'content': [{'text': 'Test response'}]
            }).encode())
        }
        mock_client.return_value.invoke_model.return_value = mock_response
        
        # Test invocation
        result = invoke_bedrock_model("test prompt")
        self.assertEqual(result, "Test response")
    
    @patch('boto3.client')
    def test_throttling_retry(self, mock_client):
        # Mock throttling then success
        mock_client.return_value.invoke_model.side_effect = [
            ClientError({'Error': {'Code': 'ThrottlingException'}}, 'InvokeModel'),
            mock_response
        ]
        
        result = invoke_bedrock_with_retry("test prompt")
        self.assertIsNotNone(result)
</blockquote>

<h3>Key Takeaways</h3>
<ul>
    <li>Model access must be explicitly enabled in the Bedrock console before API usage</li>
    <li>IAM permissions control who can invoke models and access Bedrock features</li>
    <li>Synchronous invocation suits batch processing; streaming provides better user experience for interactive applications</li>
    <li>Request parameters like temperature and max_tokens significantly impact output characteristics and cost</li>
    <li>Robust error handling with exponential backoff is essential for production reliability</li>
    <li>Implement comprehensive monitoring, logging, and security measures for production deployments</li>
    <li>Optimize requests to minimize token consumption and reduce costs</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
