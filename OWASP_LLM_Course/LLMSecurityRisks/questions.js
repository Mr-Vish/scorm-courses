test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.owaspllm.m1_q1",
                                "How is Web poisoning: Placing malicious content on websites that get scraped for training best defined in this context?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Set max_tokens limits per request", "Incident response: Have a plan for prompt injection attacks and data leaks", "Backdoor triggers: Model behaves normally except when specific trigger phrases appear", "Web poisoning: Placing malicious content on websites that get scraped for training"),
                                "Web poisoning: Placing malicious content on websites that get scraped for training",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.owaspllm.m1_q2",
                                "What is a key characteristic of Fine-tuning attacks: Poisoned datasets uploaded to model hubs or shared repositories?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Send maximum-length inputs repeatedly", "Fine-tuning attacks: Poisoned datasets uploaded to model hubs or shared repositories", "Red team regularly: Test with adversarial prompts before and after deployment", "Unvetted plugins: Third-party tools and plugins with excessive permissions"),
                                "Fine-tuning attacks: Poisoned datasets uploaded to model hubs or shared repositories",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.owaspllm.m1_q3",
                                "Which of the following best describes Backdoor triggers: Model behaves normally except when specific trigger phrases appear?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Stay updated: Follow OWASP LLM project for emerging threats and mitigations", "XSS via LLM output", "Backdoor triggers: Model behaves normally except when specific trigger phrases appear", "LLM09: Overreliance"),
                                "Backdoor triggers: Model behaves normally except when specific trigger phrases appear",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.owaspllm.m1_q4",
                                "In the context of LLM Security Risks, what does Layer 1: Input validation and sanitization before the prompt refer to?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Layer 1: Input validation and sanitization before the prompt", "Model generates SQL that is executed directly", "Model reveals PII, secrets, or proprietary data", "Command injection"),
                                "Layer 1: Input validation and sanitization before the prompt",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.owaspllm.m1_q5",
                                "Which statement about Layer 2: System prompt hardening with clear boundaries is accurate?",
                                QUESTION_TYPE_CHOICE,
                                new Array("LLM05: Supply Chain Vulnerabilities", "Layer 2: System prompt hardening with clear boundaries", "Monitor in production: Log and analyze all LLM interactions for anomalies", "Prompts requiring extensive generation"),
                                "Layer 2: System prompt hardening with clear boundaries",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.owaspllm.m1_q6",
                                "What is the primary purpose of Layer 3: Output validation and sanitization after the response?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Layer 3: Output validation and sanitization after the response", "OWASP Top 10 for LLM Applications", "PII scanning, output filtering", "Incident response: Have a plan for prompt injection attacks and data leaks"),
                                "Layer 3: Output validation and sanitization after the response",
                                "obj_module_1")
                );