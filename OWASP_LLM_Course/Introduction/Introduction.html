<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>OWASP Top 10 for LLM Applications - Introduction</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Introduction to the OWASP Top 10 for LLM Applications</h1>

<div class="intro-section">
<p>As Large Language Models (LLMs) become integrated into core business processes and consumer applications, they introduce a new set of security risks that traditional security frameworks are not fully equipped to handle. The <strong>OWASP Top 10 for LLM Applications</strong> project was created to provide developers, architects, and security professionals with a comprehensive guide to the most critical security vulnerabilities in the LLM space.</p>

<p>This course provides a deep dive into each of the ten risks, explaining how they work, how they can be exploited, and—most importantly—how to defend against them. From the well-known threat of prompt injection to the subtle risks of model theft and excessive agency, we will cover the entire LLM security landscape.</p>

<h2>Why LLM Security is Different</h2>
<p>Traditional web security often focuses on "sanitizing inputs" to prevent code execution (like SQL injection). In LLMs, the "input" is natural language, which is inherently flexible and ambiguous. This makes it much harder to distinguish between a legitimate request and a malicious one. Furthermore, LLMs are non-deterministic, meaning the same input can lead to different outputs, complicating security testing and validation.</p>

<h2>Course Roadmap</h2>
<p>We will explore each of the OWASP LLM risks in detail:</p>
<ul>
    <li><strong>LLM01: Prompt Injection:</strong> Both direct (jailbreaking) and indirect attacks.</li>
    <li><strong>LLM02: Insecure Output Handling:</strong> Risks when LLM outputs are trusted blindly by other systems.</li>
    <li><strong>LLM03: Training Data Poisoning:</strong> Manipulating the model's "knowledge" at the source.</li>
    <li><strong>LLM04: Model Denial of Service:</strong> Overwhelming the model's resources to cause failure.</li>
    <li><strong>LLM05: Supply Chain Vulnerabilities:</strong> Risks in third-party models, datasets, and plugins.</li>
    <li><strong>LLM06: Sensitive Information Disclosure:</strong> Preventing the model from leaking private data.</li>
    <li><strong>LLM07: Insecure Plugin Design:</strong> Securing the "arms and legs" of the LLM.</li>
    <li><strong>LLM08: Excessive Agency:</strong> Limiting what an AI agent can do without human approval.</li>
    <li><strong>LLM09: Overreliance:</strong> Dealing with hallucinations and biased outputs.</li>
    <li><strong>LLM10: Model Theft:</strong> Protecting your intellectual property and proprietary models.</li>
</ul>

<h2>Course Structure</h2>
<p>Each module includes technical explanations, real-world exploit examples, and detailed remediation strategies. We also include hands-on exercises where you will attempt to "break" and then "fix" a vulnerable LLM application. Successfully completing the course requires passing all module assessments and a final comprehensive exam with a score of 70% or higher.</p>

<p>Join us as we explore the frontier of AI security and learn how to build LLM applications that are not only powerful but also safe and resilient.</p>
</div>

<script type="text/javascript">
</script>
</body>
</html>
