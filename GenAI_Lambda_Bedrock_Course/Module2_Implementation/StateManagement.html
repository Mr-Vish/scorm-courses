<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>State Management and Conversation History</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>State Management and Conversation History</h1>

<h2>Why State Management Matters</h2>
<p>Lambda functions are stateless by design—each invocation starts fresh with no memory of previous requests. For conversational AI applications, maintaining context across multiple interactions is essential for coherent, contextual responses.</p>

<h3>State Management Challenges</h3>
<ul>
    <li><strong>Stateless Functions:</strong> Lambda doesn't persist data between invocations</li>
    <li><strong>Context Windows:</strong> LLMs have token limits for conversation history</li>
    <li><strong>Performance:</strong> Retrieving history must be fast to avoid latency</li>
    <li><strong>Cost:</strong> Storing and retrieving conversation data adds expenses</li>
</ul>

<h2>DynamoDB for Conversation History</h2>
<p>DynamoDB provides fast, scalable storage for conversation history with single-digit millisecond latency. It's the ideal choice for serverless GenAI applications.</p>

<h3>Table Design</h3>
<div class="code-block">
<pre><code># DynamoDB Table Schema
Table Name: ConversationHistory
Partition Key: sessionId (String)
Sort Key: timestamp (Number)

Attributes:
- sessionId: Unique identifier for conversation
- timestamp: Unix timestamp for message ordering
- role: "user" or "assistant"
- content: Message text
- tokens: Token count for this message
- ttl: Time-to-live for automatic cleanup
</code></pre>
</div>

<h3>Storing Conversation Messages</h3>
<div class="code-block">
<pre><code>import boto3
import time
import uuid

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('ConversationHistory')

def store_message(session_id, role, content, tokens):
    """Store a message in conversation history"""
    timestamp = int(time.time() * 1000)  # Milliseconds for ordering
    
    table.put_item(Item={
        'sessionId': session_id,
        'timestamp': timestamp,
        'role': role,
        'content': content,
        'tokens': tokens,
        'ttl': int(time.time()) + (7 * 24 * 3600)  # 7 days retention
    })
    
    return timestamp

def lambda_handler(event, context):
    body = json.loads(event['body'])
    session_id = body.get('session_id', str(uuid.uuid4()))
    user_message = body['message']
    
    # Store user message
    store_message(session_id, 'user', user_message, len(user_message.split()))
    
    # Retrieve conversation history
    history = get_conversation_history(session_id, max_messages=10)
    
    # Build messages for Bedrock
    messages = []
    for msg in history:
        messages.append({
            'role': msg['role'],
            'content': msg['content']
        })
    
    # Add current message
    messages.append({'role': 'user', 'content': user_message})
    
    # Invoke Bedrock with history
    bedrock = boto3.client('bedrock-runtime')
    response = bedrock.invoke_model(
        modelId='anthropic.claude-3-sonnet-20240229-v1:0',
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 2048,
            'messages': messages
        })
    )
    
    result = json.loads(response['body'].read())
    ai_response = result['content'][0]['text']
    
    # Store assistant response
    store_message(session_id, 'assistant', ai_response, result['usage']['output_tokens'])
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'session_id': session_id,
            'response': ai_response
        })
    }
</code></pre>
</div>

<h3>Retrieving Conversation History</h3>
<div class="code-block">
<pre><code>def get_conversation_history(session_id, max_messages=10, max_tokens=4000):
    """Retrieve recent conversation history with token limit"""
    
    response = table.query(
        KeyConditionExpression='sessionId = :sid',
        ExpressionAttributeValues={':sid': session_id},
        ScanIndexForward=False,  # Most recent first
        Limit=max_messages * 2   # Get extra to account for token filtering
    )
    
    messages = []
    total_tokens = 0
    
    # Process messages in reverse chronological order
    for item in reversed(response['Items']):
        message_tokens = item.get('tokens', 0)
        
        # Stop if adding this message exceeds token limit
        if total_tokens + message_tokens > max_tokens:
            break
        
        messages.append({
            'role': item['role'],
            'content': item['content'],
            'timestamp': item['timestamp']
        })
        total_tokens += message_tokens
    
    # Return in chronological order
    return list(reversed(messages))
</code></pre>
</div>

<h2>Conversation Summarization</h2>
<p>For long conversations, summarize older messages to stay within token limits while preserving context.</p>

<div class="code-block">
<pre><code>def summarize_old_messages(messages):
    """Summarize older messages to reduce token count"""
    
    if len(messages) <= 6:
        return messages
    
    # Keep most recent 4 messages
    recent_messages = messages[-4:]
    old_messages = messages[:-4]
    
    # Create summary of old messages
    old_content = "\n".join([f"{m['role']}: {m['content']}" for m in old_messages])
    
    bedrock = boto3.client('bedrock-runtime')
    summary_response = bedrock.invoke_model(
        modelId='anthropic.claude-3-haiku-20240307-v1:0',  # Use cheaper model
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 500,
            'messages': [{
                'role': 'user',
                'content': f'Summarize this conversation concisely:\n\n{old_content}'
            }]
        })
    )
    
    result = json.loads(summary_response['body'].read())
    summary = result['content'][0]['text']
    
    # Return summary + recent messages
    return [
        {'role': 'user', 'content': f'Previous conversation summary: {summary}'}
    ] + recent_messages
</code></pre>
</div>

<h2>RAG Pattern with Vector Databases</h2>
<p>Retrieval-Augmented Generation (RAG) enhances GenAI responses by providing relevant context from your knowledge base. This enables AI to answer questions about your specific data.</p>

<h3>RAG Architecture</h3>
<div class="flow-diagram">
    <div class="flow-box">User Query</div>
    <div class="flow-arrow">→</div>
    <div class="flow-box">Embed Query</div>
    <div class="flow-arrow">→</div>
    <div class="flow-box">Vector Search</div>
    <div class="flow-arrow">→</div>
    <div class="flow-box">Retrieve Docs</div>
    <div class="flow-arrow">→</div>
    <div class="flow-box">Bedrock + Context</div>
    <div class="flow-arrow">→</div>
    <div class="flow-box">Response</div>
</div>

<h3>Using Amazon OpenSearch for Vector Storage</h3>
<div class="code-block">
<pre><code>from opensearchpy import OpenSearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

def create_opensearch_client():
    """Create OpenSearch client with AWS authentication"""
    credentials = boto3.Session().get_credentials()
    awsauth = AWS4Auth(
        credentials.access_key,
        credentials.secret_key,
        'us-east-1',
        'es',
        session_token=credentials.token
    )
    
    return OpenSearch(
        hosts=[{'host': 'your-domain.us-east-1.es.amazonaws.com', 'port': 443}],
        http_auth=awsauth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection
    )

def embed_text(text):
    """Generate embeddings using Bedrock Titan Embeddings"""
    bedrock = boto3.client('bedrock-runtime')
    
    response = bedrock.invoke_model(
        modelId='amazon.titan-embed-text-v1',
        body=json.dumps({'inputText': text})
    )
    
    result = json.loads(response['body'].read())
    return result['embedding']

def search_knowledge_base(query, top_k=3):
    """Search vector database for relevant documents"""
    opensearch = create_opensearch_client()
    
    # Generate query embedding
    query_embedding = embed_text(query)
    
    # Vector search
    search_body = {
        'size': top_k,
        'query': {
            'knn': {
                'embedding': {
                    'vector': query_embedding,
                    'k': top_k
                }
            }
        }
    }
    
    response = opensearch.search(index='knowledge-base', body=search_body)
    
    # Extract relevant documents
    documents = []
    for hit in response['hits']['hits']:
        documents.append({
            'content': hit['_source']['content'],
            'score': hit['_score'],
            'metadata': hit['_source'].get('metadata', {})
        })
    
    return documents
</code></pre>
</div>

<h3>RAG-Enhanced Lambda Handler</h3>
<div class="code-block">
<pre><code>def rag_lambda_handler(event, context):
    body = json.loads(event['body'])
    user_query = body['query']
    
    # Search knowledge base
    relevant_docs = search_knowledge_base(user_query, top_k=3)
    
    # Build context from retrieved documents
    context = "\n\n".join([
        f"Document {i+1}:\n{doc['content']}" 
        for i, doc in enumerate(relevant_docs)
    ])
    
    # Create prompt with context
    prompt = f"""Use the following context to answer the question. If the answer isn't in the context, say so.

Context:
{context}

Question: {user_query}

Answer:"""
    
    # Invoke Bedrock with context
    bedrock = boto3.client('bedrock-runtime')
    response = bedrock.invoke_model(
        modelId='anthropic.claude-3-sonnet-20240229-v1:0',
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 1024,
            'messages': [{'role': 'user', 'content': prompt}]
        })
    )
    
    result = json.loads(response['body'].read())
    ai_response = result['content'][0]['text']
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'answer': ai_response,
            'sources': [doc['metadata'] for doc in relevant_docs]
        })
    }
</code></pre>
</div>

<h2>State Management Best Practices</h2>
<table>
    <tr><th>Practice</th><th>Implementation</th><th>Benefit</th></tr>
    <tr><td class="rowheader">TTL</td><td>Set DynamoDB TTL for automatic cleanup</td><td>Reduces storage costs</td></tr>
    <tr><td class="rowheader">Token Limits</td><td>Track and limit conversation token count</td><td>Prevents context overflow</td></tr>
    <tr><td class="rowheader">Summarization</td><td>Summarize old messages periodically</td><td>Maintains context efficiently</td></tr>
    <tr><td class="rowheader">Caching</td><td>Cache embeddings for repeated queries</td><td>Reduces embedding costs</td></tr>
    <tr><td class="rowheader">Indexing</td><td>Use GSI for user-based queries</td><td>Enables multi-session retrieval</td></tr>
</table>

<h2>Cost Optimization for State Management</h2>
<ul>
    <li><strong>DynamoDB On-Demand:</strong> Use on-demand billing for unpredictable traffic</li>
    <li><strong>Compression:</strong> Compress large conversation histories before storage</li>
    <li><strong>Selective Storage:</strong> Only store essential conversation turns</li>
    <li><strong>Cheaper Embeddings:</strong> Use Titan Embeddings ($0.0001/1K tokens) vs OpenAI</li>
    <li><strong>Batch Operations:</strong> Batch DynamoDB writes when possible</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
