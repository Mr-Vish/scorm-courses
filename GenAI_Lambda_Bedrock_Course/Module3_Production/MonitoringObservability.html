<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Monitoring and Observability</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Monitoring and Observability</h1>

<h2>Why Monitoring Matters for GenAI</h2>
<p>GenAI applications have unique monitoring requirements beyond traditional serverless applications. You need visibility into model performance, token usage, latency, errors, and costs to ensure reliable, cost-effective operations.</p>

<h3>Key Metrics to Track</h3>
<ul>
    <li><strong>Latency:</strong> Time from request to response (target: &lt;5s for interactive apps)</li>
    <li><strong>Token Usage:</strong> Input/output tokens per request (directly impacts cost)</li>
    <li><strong>Error Rates:</strong> Throttling, timeouts, validation errors</li>
    <li><strong>Cost per Request:</strong> Bedrock + Lambda + data transfer costs</li>
    <li><strong>Model Performance:</strong> Response quality, hallucination rates</li>
</ul>

<h2>CloudWatch Metrics and Logging</h2>
<p>CloudWatch provides comprehensive monitoring for Lambda and Bedrock integrations with custom metrics for GenAI-specific tracking.</p>

<h3>Structured Logging Implementation</h3>
<div class="code-block">
<pre><code>import json
import time
from datetime import datetime

class GenAILogger:
    """Structured logger for GenAI applications"""
    
    def __init__(self, context):
        self.request_id = context.request_id
        self.function_name = context.function_name
    
    def log_request(self, user_message, session_id, model_id):
        """Log incoming request"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'request_id': self.request_id,
            'event_type': 'genai_request',
            'session_id': session_id,
            'model_id': model_id,
            'input_length': len(user_message),
            'function': self.function_name
        }
        print(json.dumps(log_entry))
    
    def log_bedrock_call(self, model_id, input_tokens, output_tokens, latency_ms, cost):
        """Log Bedrock API call metrics"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'request_id': self.request_id,
            'event_type': 'bedrock_call',
            'model_id': model_id,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'total_tokens': input_tokens + output_tokens,
            'latency_ms': latency_ms,
            'estimated_cost': cost
        }
        print(json.dumps(log_entry))
    
    def log_error(self, error_type, error_message, context_data=None):
        """Log errors with context"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'request_id': self.request_id,
            'event_type': 'error',
            'error_type': error_type,
            'error_message': error_message,
            'context': context_data or {}
        }
        print(json.dumps(log_entry))

def lambda_handler(event, context):
    logger = GenAILogger(context)
    
    try:
        body = json.loads(event['body'])
        user_message = body['message']
        session_id = body.get('session_id', 'anonymous')
        model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'
        
        logger.log_request(user_message, session_id, model_id)
        
        # Invoke Bedrock with timing
        start_time = time.time()
        bedrock = boto3.client('bedrock-runtime')
        
        response = bedrock.invoke_model(
            modelId=model_id,
            body=json.dumps({
                'anthropic_version': 'bedrock-2023-05-31',
                'max_tokens': 2048,
                'messages': [{'role': 'user', 'content': user_message}]
            })
        )
        
        latency_ms = int((time.time() - start_time) * 1000)
        result = json.loads(response['body'].read())
        
        # Calculate cost
        input_tokens = result['usage']['input_tokens']
        output_tokens = result['usage']['output_tokens']
        cost = (input_tokens * 0.003 / 1000) + (output_tokens * 0.015 / 1000)
        
        logger.log_bedrock_call(model_id, input_tokens, output_tokens, latency_ms, cost)
        
        return {
            'statusCode': 200,
            'body': json.dumps({'response': result['content'][0]['text']})
        }
        
    except Exception as e:
        logger.log_error(type(e).__name__, str(e), {'event': event})
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal server error'})
        }
</code></pre>
</div>

<h3>Custom CloudWatch Metrics</h3>
<div class="code-block">
<pre><code>import boto3

cloudwatch = boto3.client('cloudwatch')

def publish_genai_metrics(model_id, input_tokens, output_tokens, latency_ms, cost):
    """Publish custom metrics to CloudWatch"""
    
    metrics = [
        {
            'MetricName': 'TokensUsed',
            'Value': input_tokens + output_tokens,
            'Unit': 'Count',
            'Dimensions': [
                {'Name': 'ModelId', 'Value': model_id},
                {'Name': 'TokenType', 'Value': 'Total'}
            ]
        },
        {
            'MetricName': 'InputTokens',
            'Value': input_tokens,
            'Unit': 'Count',
            'Dimensions': [{'Name': 'ModelId', 'Value': model_id}]
        },
        {
            'MetricName': 'OutputTokens',
            'Value': output_tokens,
            'Unit': 'Count',
            'Dimensions': [{'Name': 'ModelId', 'Value': model_id}]
        },
        {
            'MetricName': 'BedrockLatency',
            'Value': latency_ms,
            'Unit': 'Milliseconds',
            'Dimensions': [{'Name': 'ModelId', 'Value': model_id}]
        },
        {
            'MetricName': 'EstimatedCost',
            'Value': cost,
            'Unit': 'None',
            'Dimensions': [{'Name': 'ModelId', 'Value': model_id}]
        }
    ]
    
    cloudwatch.put_metric_data(
        Namespace='GenAI/Application',
        MetricData=metrics
    )
</code></pre>
</div>

<h2>AWS X-Ray Distributed Tracing</h2>
<p>X-Ray provides end-to-end visibility into request flows, helping identify bottlenecks and performance issues across your GenAI application.</p>

<h3>Enabling X-Ray Tracing</h3>
<div class="code-block">
<pre><code>from aws_xray_sdk.core import xray_recorder
from aws_xray_sdk.core import patch_all

# Patch AWS SDK calls
patch_all()

@xray_recorder.capture('invoke_bedrock')
def invoke_bedrock_with_tracing(model_id, messages):
    """Invoke Bedrock with X-Ray tracing"""
    
    bedrock = boto3.client('bedrock-runtime')
    
    # Add custom metadata
    xray_recorder.put_metadata('model_id', model_id)
    xray_recorder.put_metadata('message_count', len(messages))
    
    # Add annotations for filtering
    xray_recorder.put_annotation('model_family', 'claude-3')
    xray_recorder.put_annotation('operation', 'text_generation')
    
    response = bedrock.invoke_model(
        modelId=model_id,
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 2048,
            'messages': messages
        })
    )
    
    result = json.loads(response['body'].read())
    
    # Record token usage
    xray_recorder.put_metadata('tokens_used', result['usage'])
    
    return result

@xray_recorder.capture('lambda_handler')
def lambda_handler(event, context):
    # X-Ray automatically traces this function
    body = json.loads(event['body'])
    
    result = invoke_bedrock_with_tracing(
        'anthropic.claude-3-sonnet-20240229-v1:0',
        [{'role': 'user', 'content': body['message']}]
    )
    
    return {
        'statusCode': 200,
        'body': json.dumps({'response': result['content'][0]['text']})
    }
</code></pre>
</div>

<h2>CloudWatch Alarms and Alerts</h2>
<p>Set up proactive alerts to detect issues before they impact users.</p>

<h3>Critical Alarms to Configure</h3>
<table>
    <tr><th>Alarm</th><th>Metric</th><th>Threshold</th><th>Action</th></tr>
    <tr><td class="rowheader">High Error Rate</td><td>Lambda Errors</td><td>&gt; 5% of invocations</td><td>SNS notification</td></tr>
    <tr><td class="rowheader">Throttling</td><td>Bedrock Throttles</td><td>&gt; 10 per minute</td><td>Auto-scaling trigger</td></tr>
    <tr><td class="rowheader">High Latency</td><td>Duration</td><td>&gt; 10 seconds (p99)</td><td>Investigation alert</td></tr>
    <tr><td class="rowheader">Cost Spike</td><td>Estimated Cost</td><td>&gt; $100/hour</td><td>Budget alert</td></tr>
    <tr><td class="rowheader">Token Overflow</td><td>Input Tokens</td><td>&gt; 150K</td><td>Rate limit trigger</td></tr>
</table>

<h3>CloudWatch Alarm Configuration</h3>
<div class="code-block">
<pre><code># CloudFormation/SAM Template
HighErrorRateAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: GenAI-HighErrorRate
    AlarmDescription: Alert when error rate exceeds 5%
    MetricName: Errors
    Namespace: AWS/Lambda
    Statistic: Sum
    Period: 300
    EvaluationPeriods: 2
    Threshold: 5
    ComparisonOperator: GreaterThanThreshold
    Dimensions:
      - Name: FunctionName
        Value: !Ref GenAIFunction
    AlarmActions:
      - !Ref AlertTopic
    TreatMissingData: notBreaching

BedrockCostAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: GenAI-HighCost
    MetricName: EstimatedCost
    Namespace: GenAI/Application
    Statistic: Sum
    Period: 3600
    EvaluationPeriods: 1
    Threshold: 100
    ComparisonOperator: GreaterThanThreshold
    AlarmActions:
      - !Ref AlertTopic
</code></pre>
</div>

<h2>CloudWatch Insights Queries</h2>
<p>Use CloudWatch Logs Insights to analyze GenAI application behavior.</p>

<h3>Useful Query Examples</h3>
<div class="code-block">
<pre><code># Average token usage by model
fields @timestamp, model_id, total_tokens
| filter event_type = "bedrock_call"
| stats avg(total_tokens) as avg_tokens by model_id

# P99 latency by hour
fields @timestamp, latency_ms
| filter event_type = "bedrock_call"
| stats pct(latency_ms, 99) as p99_latency by bin(1h)

# Error analysis
fields @timestamp, error_type, error_message
| filter event_type = "error"
| stats count() by error_type

# Cost analysis by session
fields @timestamp, session_id, estimated_cost
| filter event_type = "bedrock_call"
| stats sum(estimated_cost) as total_cost by session_id
| sort total_cost desc
| limit 20

# Token efficiency (output/input ratio)
fields @timestamp, input_tokens, output_tokens
| filter event_type = "bedrock_call"
| fields output_tokens / input_tokens as efficiency
| stats avg(efficiency) as avg_efficiency
</code></pre>
</div>

<h2>Dashboard Creation</h2>
<p>Create comprehensive dashboards to monitor GenAI application health at a glance.</p>

<h3>Recommended Dashboard Widgets</h3>
<ul>
    <li><strong>Request Volume:</strong> Line graph of requests per minute</li>
    <li><strong>Latency Distribution:</strong> P50, P90, P99 latency over time</li>
    <li><strong>Token Usage:</strong> Stacked area chart of input/output tokens</li>
    <li><strong>Error Rates:</strong> Percentage of failed requests</li>
    <li><strong>Cost Tracking:</strong> Cumulative cost over time</li>
    <li><strong>Model Distribution:</strong> Pie chart of requests by model</li>
</ul>

<h2>Best Practices for Production Monitoring</h2>
<ul>
    <li><strong>Structured Logging:</strong> Always use JSON format for easy parsing</li>
    <li><strong>Correlation IDs:</strong> Track requests across services with unique IDs</li>
    <li><strong>Sampling:</strong> Use X-Ray sampling to reduce costs (e.g., 10% of requests)</li>
    <li><strong>Retention Policies:</strong> Set appropriate log retention (7-30 days)</li>
    <li><strong>Cost Allocation Tags:</strong> Tag resources for cost tracking by team/project</li>
    <li><strong>Automated Responses:</strong> Use Lambda to auto-remediate common issues</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
