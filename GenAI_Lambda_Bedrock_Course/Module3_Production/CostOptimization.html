<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Cost Optimization Strategies</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Cost Optimization Strategies</h1>

<h2>Understanding GenAI Cost Structure</h2>
<p>GenAI applications have a unique cost profile where Bedrock API calls typically represent 80-95% of total costs. Understanding and optimizing these costs is critical for sustainable operations.</p>

<h3>Cost Breakdown</h3>
<table>
    <tr><th>Component</th><th>Typical % of Total</th><th>Pricing Model</th></tr>
    <tr><td class="rowheader">Bedrock API Calls</td><td>80-95%</td><td>Per 1K tokens (input + output)</td></tr>
    <tr><td class="rowheader">Lambda Execution</td><td>2-10%</td><td>Per GB-second</td></tr>
    <tr><td class="rowheader">API Gateway</td><td>1-5%</td><td>Per million requests</td></tr>
    <tr><td class="rowheader">Data Transfer</td><td>1-3%</td><td>Per GB transferred</td></tr>
    <tr><td class="rowheader">Storage (DynamoDB/S3)</td><td>1-2%</td><td>Per GB stored + requests</td></tr>
</table>

<h2>Bedrock Model Selection</h2>
<p>Choosing the right model for each task dramatically impacts costs. Claude 3 offers three models with different price-performance tradeoffs.</p>

<h3>Claude 3 Model Comparison</h3>
<table>
    <tr><th>Model</th><th>Input Cost (per 1K tokens)</th><th>Output Cost (per 1K tokens)</th><th>Best For</th></tr>
    <tr><td class="rowheader">Claude 3 Haiku</td><td>$0.00025</td><td>$0.00125</td><td>Simple tasks, classification, extraction</td></tr>
    <tr><td class="rowheader">Claude 3 Sonnet</td><td>$0.003</td><td>$0.015</td><td>Balanced performance, most use cases</td></tr>
    <tr><td class="rowheader">Claude 3 Opus</td><td>$0.015</td><td>$0.075</td><td>Complex reasoning, critical tasks</td></tr>
</table>

<h3>Smart Model Selection Implementation</h3>
<div class="code-block">
<pre><code>def select_optimal_model(task_type, complexity_score, budget_tier='standard'):
    """Select most cost-effective model for the task"""
    
    # Simple classification/extraction tasks
    if task_type in ['classification', 'extraction', 'simple_qa']:
        return 'anthropic.claude-3-haiku-20240307-v1:0'
    
    # Complex reasoning or long-form generation
    if task_type in ['complex_reasoning', 'code_generation', 'analysis']:
        if budget_tier == 'premium' or complexity_score > 0.8:
            return 'anthropic.claude-3-opus-20240229-v1:0'
        else:
            return 'anthropic.claude-3-sonnet-20240229-v1:0'
    
    # Default to Sonnet for balanced performance
    return 'anthropic.claude-3-sonnet-20240229-v1:0'

def lambda_handler(event, context):
    body = json.loads(event['body'])
    task_type = body.get('task_type', 'general')
    complexity = body.get('complexity', 0.5)
    
    # Select appropriate model
    model_id = select_optimal_model(task_type, complexity)
    
    # Invoke with selected model
    bedrock = boto3.client('bedrock-runtime')
    response = bedrock.invoke_model(
        modelId=model_id,
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 1024,
            'messages': [{'role': 'user', 'content': body['message']}]
        })
    )
    
    return {'statusCode': 200, 'body': json.dumps({'model_used': model_id})}
</code></pre>
</div>

<h2>Prompt Optimization</h2>
<p>Efficient prompts reduce token usage without sacrificing quality. Every token saved reduces costs directly.</p>

<h3>Prompt Optimization Techniques</h3>
<ul>
    <li><strong>Concise Instructions:</strong> Remove unnecessary words while maintaining clarity</li>
    <li><strong>Structured Output:</strong> Request specific formats (JSON, bullet points) to reduce verbosity</li>
    <li><strong>Token Limits:</strong> Set appropriate max_tokens based on expected response length</li>
    <li><strong>System Prompts:</strong> Use system messages for reusable instructions</li>
    <li><strong>Few-Shot Examples:</strong> Use minimal examples that demonstrate the pattern</li>
</ul>

<h3>Before and After Optimization</h3>
<div class="code-block">
<pre><code># BEFORE: Verbose prompt (150 tokens)
prompt_verbose = """
I would like you to please analyze the following customer feedback 
and provide me with a detailed summary of the main points. Please 
make sure to include the sentiment (positive, negative, or neutral), 
the key topics mentioned, and any actionable insights. Here is the 
feedback: {feedback}
"""

# AFTER: Optimized prompt (45 tokens)
prompt_optimized = """
Analyze this feedback and provide:
1. Sentiment (positive/negative/neutral)
2. Key topics
3. Actionable insights

Feedback: {feedback}
"""

# Token savings: 70% reduction (105 tokens saved per request)
</code></pre>
</div>

<h3>Dynamic max_tokens Configuration</h3>
<div class="code-block">
<pre><code>def calculate_optimal_max_tokens(task_type, input_length):
    """Calculate appropriate max_tokens based on task"""
    
    token_configs = {
        'classification': 50,
        'extraction': 200,
        'summary': min(input_length // 4, 500),
        'qa': 300,
        'generation': 1500,
        'analysis': 2000
    }
    
    return token_configs.get(task_type, 1024)

# Usage
max_tokens = calculate_optimal_max_tokens('summary', len(document))
</code></pre>
</div>

<h2>Response Caching</h2>
<p>Cache responses for identical or similar requests to avoid redundant Bedrock calls.</p>

<h3>DynamoDB-Based Response Cache</h3>
<div class="code-block">
<pre><code>import hashlib
import json
import boto3

dynamodb = boto3.resource('dynamodb')
cache_table = dynamodb.Table('BedrockResponseCache')

def get_cache_key(prompt, model_id, parameters):
    """Generate cache key from request parameters"""
    cache_input = json.dumps({
        'prompt': prompt,
        'model': model_id,
        'params': parameters
    }, sort_keys=True)
    return hashlib.sha256(cache_input.encode()).hexdigest()

def get_cached_response(cache_key):
    """Retrieve cached response if available and not expired"""
    try:
        response = cache_table.get_item(Key={'cacheKey': cache_key})
        
        if 'Item' in response:
            item = response['Item']
            # Check if cache is still valid (e.g., 1 hour TTL)
            if int(time.time()) < item.get('expiresAt', 0):
                return item['response']
        return None
    except Exception as e:
        print(f"Cache retrieval error: {e}")
        return None

def cache_response(cache_key, response, ttl_seconds=3600):
    """Store response in cache"""
    try:
        cache_table.put_item(Item={
            'cacheKey': cache_key,
            'response': response,
            'createdAt': int(time.time()),
            'expiresAt': int(time.time()) + ttl_seconds,
            'ttl': int(time.time()) + ttl_seconds
        })
    except Exception as e:
        print(f"Cache storage error: {e}")

def invoke_with_cache(prompt, model_id, parameters):
    """Invoke Bedrock with caching"""
    
    # Check cache first
    cache_key = get_cache_key(prompt, model_id, parameters)
    cached = get_cached_response(cache_key)
    
    if cached:
        print("Cache hit - saved Bedrock call")
        return cached
    
    # Cache miss - invoke Bedrock
    bedrock = boto3.client('bedrock-runtime')
    response = bedrock.invoke_model(
        modelId=model_id,
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            **parameters,
            'messages': [{'role': 'user', 'content': prompt}]
        })
    )
    
    result = json.loads(response['body'].read())
    response_text = result['content'][0]['text']
    
    # Cache the response
    cache_response(cache_key, response_text)
    
    return response_text
</code></pre>
</div>

<h2>Batch Processing for Cost Efficiency</h2>
<p>Process multiple requests together to amortize overhead costs and optimize throughput.</p>

<div class="code-block">
<pre><code>def batch_process_with_haiku(items):
    """Process multiple simple items with cost-effective Haiku model"""
    
    # Combine multiple items into single prompt
    combined_prompt = "Process each item and return results in JSON format:\n\n"
    for i, item in enumerate(items):
        combined_prompt += f"{i+1}. {item['text']}\n"
    
    bedrock = boto3.client('bedrock-runtime')
    response = bedrock.invoke_model(
        modelId='anthropic.claude-3-haiku-20240307-v1:0',
        body=json.dumps({
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 2048,
            'messages': [{'role': 'user', 'content': combined_prompt}]
        })
    )
    
    result = json.loads(response['body'].read())
    
    # Cost comparison:
    # Individual calls: 10 items × $0.00025 = $0.0025 (minimum)
    # Batch call: 1 call × $0.00025 = $0.00025
    # Savings: 90%
    
    return result['content'][0]['text']
</code></pre>
</div>

<h2>Lambda Cost Optimization</h2>
<p>While Lambda costs are typically small compared to Bedrock, optimization still matters at scale.</p>

<h3>Memory Configuration</h3>
<div class="code-block">
<pre><code># Optimal memory configurations for GenAI workloads
memory_configs = {
    'simple_api': 512,      # Basic REST API handler
    'standard_genai': 1024,  # Standard Bedrock invocation
    'streaming': 1536,       # WebSocket streaming
    'batch_processing': 2048, # Parallel batch operations
    'rag_pipeline': 3008     # RAG with vector search
}

# Cost example (us-east-1):
# 512 MB: $0.0000083 per GB-second
# 1024 MB: $0.0000167 per GB-second
# 2048 MB: $0.0000333 per GB-second

# But 2048 MB often executes 2x faster, making it cost-neutral
</code></pre>
</div>

<h3>Provisioned Concurrency vs On-Demand</h3>
<table>
    <tr><th>Scenario</th><th>Recommendation</th><th>Cost Impact</th></tr>
    <tr><td class="rowheader">Low traffic (&lt;100 req/day)</td><td>On-Demand</td><td>Lowest cost</td></tr>
    <tr><td class="rowheader">Predictable traffic</td><td>Provisioned Concurrency</td><td>Eliminates cold starts</td></tr>
    <tr><td class="rowheader">Spiky traffic</td><td>Hybrid (base provisioned + on-demand)</td><td>Balanced cost/performance</td></tr>
    <tr><td class="rowheader">High volume (&gt;10K req/day)</td><td>Provisioned Concurrency</td><td>Cost-effective at scale</td></tr>
</table>

<h2>Cost Monitoring and Budgets</h2>
<p>Implement automated cost tracking and alerts to prevent budget overruns.</p>

<div class="code-block">
<pre><code>def track_request_cost(model_id, input_tokens, output_tokens):
    """Calculate and track cost per request"""
    
    # Pricing per 1K tokens
    pricing = {
        'anthropic.claude-3-haiku-20240307-v1:0': {
            'input': 0.00025, 'output': 0.00125
        },
        'anthropic.claude-3-sonnet-20240229-v1:0': {
            'input': 0.003, 'output': 0.015
        },
        'anthropic.claude-3-opus-20240229-v1:0': {
            'input': 0.015, 'output': 0.075
        }
    }
    
    model_pricing = pricing.get(model_id, pricing['anthropic.claude-3-sonnet-20240229-v1:0'])
    
    input_cost = (input_tokens / 1000) * model_pricing['input']
    output_cost = (output_tokens / 1000) * model_pricing['output']
    total_cost = input_cost + output_cost
    
    # Publish to CloudWatch
    cloudwatch = boto3.client('cloudwatch')
    cloudwatch.put_metric_data(
        Namespace='GenAI/Costs',
        MetricData=[{
            'MetricName': 'RequestCost',
            'Value': total_cost,
            'Unit': 'None',
            'Dimensions': [{'Name': 'ModelId', 'Value': model_id}]
        }]
    )
    
    return total_cost
</code></pre>
</div>

<h2>Cost Optimization Checklist</h2>
<ul>
    <li>✓ Use Haiku for simple tasks (classification, extraction)</li>
    <li>✓ Optimize prompts to reduce token usage by 30-50%</li>
    <li>✓ Implement response caching for repeated queries</li>
    <li>✓ Set appropriate max_tokens limits per task type</li>
    <li>✓ Batch similar requests when possible</li>
    <li>✓ Configure Lambda memory based on workload</li>
    <li>✓ Use provisioned concurrency only when justified</li>
    <li>✓ Monitor costs with CloudWatch metrics and alarms</li>
    <li>✓ Implement rate limiting to prevent abuse</li>
    <li>✓ Set up AWS Budgets with automated alerts</li>
</ul>

<h2>Real-World Cost Savings Example</h2>
<div class="code-block">
<pre><code># Scenario: Customer support chatbot (10,000 requests/day)

# BEFORE optimization:
# - All requests use Claude 3 Sonnet
# - Average 500 input + 300 output tokens per request
# - No caching
# Daily cost: 10,000 × ((500/1000 × $0.003) + (300/1000 × $0.015)) = $60/day
# Monthly cost: $1,800

# AFTER optimization:
# - 60% simple queries use Haiku (6,000 requests)
# - 40% complex queries use Sonnet (4,000 requests)
# - 30% cache hit rate
# - Optimized prompts reduce tokens by 40%

# Haiku requests: 6,000 × 0.7 × ((300/1000 × $0.00025) + (180/1000 × $0.00125)) = $1.26
# Sonnet requests: 4,000 × 0.7 × ((300/1000 × $0.003) + (180/1000 × $0.015)) = $17.64
# Daily cost: $18.90
# Monthly cost: $567

# SAVINGS: 68% reduction ($1,233/month saved)
</code></pre>
</div>

<script type="text/javascript">
</script>
</body>
</html>
