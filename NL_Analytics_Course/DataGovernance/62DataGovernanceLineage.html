<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Module: Data Governance and Lineage in NLA</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Data Governance and Lineage in NL Analytics</h1>

<p>As natural language analytics spreads across an organization, maintaining control and understanding of where data comes from and how it's being used is critical. This is the domain of data governance and data lineage.</p>

<h2>6.2 Understanding Data Lineage</h2>
<p>Data lineage provides a "map" of the data's journey, from its origin in a source system to its final presentation in a user's natural language query. In NLA, this means being able to answer:
<ul>
    <li>"Which database table provided this number?"</li>
    <li>"What transformation logic (SQL/Python) was applied to the raw data?"</li>
    <li>"What was the specific natural language prompt that initiated this request?"</li>
</ul></p>

<h2>6.3 Implementing Lineage in NLA Pipelines</h2>
<ol>
    <li><strong>Metadata Capture:</strong> Every query result should be accompanied by metadata identifying the source tables, columns, and the specific version of the database schema used.</li>
    <li><strong>Prompt-to-Code Mapping:</strong> Logging the user's question alongside the exact SQL or Python code that was generated. This is essential for debugging and for identifying when the AI might be misinterpreting a request.</li>
    <li><strong>Transformation Tracking:</strong> If the data passes through multiple steps (e.g., SQL retrieval -> Python cleaning -> Plotly visualization), each step must be recorded to provide a complete audit trail.</li>
</ol>

<h2>6.4 Role-Based Access Control (RBAC) and Governance</h2>
<p>NLA systems must be tightly integrated with the organization's RBAC policies.
<ul>
    <li><strong>Just-in-Time Schema Generation:</strong> Only including metadata for the tables and columns that the current user has permission to access in the prompt provided to the LLM.</li>
    <li><strong>Approval Workflows:</strong> Requiring human review for natural language queries that access highly sensitive data or perform destructive actions (if supported).</li>
    <li><strong>Usage Auditing:</strong> Regularly reviewing logs to identify which users are accessing which datasets and for what purposes.</li>
</ul></p>

<h2>6.5 The "Catalog-First" Approach</h2>
<p>Successful NL Analytics systems rely on a high-quality **Data Catalog**. The catalog provides the "ground-truth" descriptions of every table and column, which the NLA system uses to build its prompts.
<ul>
    <li><strong>Semantic Definitions:</strong> Instead of just `column_01`, the catalog should define it as `annual_recurring_revenue`.</li>
    <li><strong>Data Quality Scoring:</strong> Flagging certain tables as "verified" or "high-confidence" sources, which the NLA system can prioritize in its searches.</li>
</ul></p>

<p>By prioritizing data governance and lineage, you ensure that your natural language analytics platform remains a trusted and reliable asset for your entire organization.</p>

<script type="text/javascript">
</script>
</body>
</html>
