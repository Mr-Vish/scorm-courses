<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Document Loaders and Index Types</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Document Loaders and Index Types</h1>


<h2>What is LlamaIndex?</h2>
<p>LlamaIndex (formerly GPT Index) is a framework for connecting LLMs to external data. While LangChain is general-purpose, LlamaIndex specializes in data ingestion, indexing, and retrieval - making it the go-to choice for RAG applications.</p>

<h2>Document Loading</h2>
<p>LlamaIndex provides readers (also called loaders) for virtually every data source:</p>
<div class="code-block">
<pre><code>from llama_index.core import SimpleDirectoryReader
from llama_index.readers.web import SimpleWebPageReader

# Load from local files (PDF, DOCX, TXT, etc.)
documents = SimpleDirectoryReader("./data").load_data()

# Load from web pages
web_docs = SimpleWebPageReader(
    html_to_text=True
).load_data(["https://example.com/article"])

# Load from databases, APIs, S3, etc.
from llama_index.readers.database import DatabaseReader
db_reader = DatabaseReader(uri="postgresql://localhost/mydb")
db_docs = db_reader.load_data(query="SELECT content FROM articles")</code></pre>
</div>

<h2>Index Types</h2>
<table>
    <tr><th>Index Type</th><th>How It Works</th><th>Best For</th></tr>
    <tr><td>VectorStoreIndex</td><td>Embeds documents, retrieves by similarity</td><td>General-purpose semantic search</td></tr>
    <tr><td>SummaryIndex</td><td>Stores summaries, scans all nodes</td><td>Comprehensive answers over all data</td></tr>
    <tr><td>TreeIndex</td><td>Hierarchical tree of summaries</td><td>Multi-level summarization</td></tr>
    <tr><td>KeywordTableIndex</td><td>Keyword-based lookup table</td><td>Exact keyword matching use cases</td></tr>
    <tr><td>KnowledgeGraphIndex</td><td>Extracts and stores entity relationships</td><td>Relationship-based queries</td></tr>
</table>

<h2>Building a Vector Index</h2>
<div class="code-block">
<pre><code>from llama_index.core import VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# Configure global settings
Settings.llm = OpenAI(model="gpt-4o-mini")
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# Create index from documents
index = VectorStoreIndex.from_documents(documents)

# Persist to disk for reuse
index.storage_context.persist(persist_dir="./storage")</code></pre>
</div>

<h2>Node Parsing and Chunking</h2>
<ul>
    <li><strong>SentenceSplitter:</strong> Splits by sentences with overlap - good default choice</li>
    <li><strong>TokenTextSplitter:</strong> Splits by token count - precise context window management</li>
    <li><strong>SemanticSplitterNodeParser:</strong> Uses embeddings to find natural breakpoints</li>
    <li><strong>HierarchicalNodeParser:</strong> Creates parent-child node relationships for multi-level retrieval</li>
</ul>


<script type="text/javascript">
</script>
</body>
</html>