<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Hands-on Exercise: Multi-model Comparison Dashboard</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Hands-on Exercise: Building a Multi-model Comparison Dashboard</h1>

<p>In this exercise, you will create a simple web-based dashboard that allows users to send the same prompt to multiple models and compare the results side-by-side. This is a common requirement for evaluating which model is best for a specific task.</p>

<h2>Step 1: Set up the Flask Backend</h2>
<p>Create a file named <code>app.py</code>. We'll use Flask to serve the dashboard and handle requests to the LiteLLM Proxy.</p>
<pre><code>from flask import Flask, request, jsonify, render_template
from openai import OpenAI

app = Flask(__name__)
client = OpenAI(api_key="proxy-key", base_url="http://localhost:4000")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/compare', methods=['POST'])
def compare():
    prompt = request.json.get('prompt')
    models = ["gpt-4o", "claude-3-5-sonnet", "llama-3-70b"]
    results = {}

    for model in models:
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            results[model] = response.choices[0].message.content
        except Exception as e:
            results[model] = f"Error: {str(e)}"

    return jsonify(results)

if __name__ == '__main__':
    app.run(port=5000)</code></pre>

<h2>Step 2: Create the Frontend Dashboard</h2>
<p>Create a <code>templates/index.html</code> file with a simple form and a results display.</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;AI Comparison Dashboard&lt;/title&gt;
    &lt;link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"&gt;
&lt;/head&gt;
&lt;body class="container mt-5"&gt;
    &lt;h1&gt;AI Model Comparison&lt;/h1&gt;
    &lt;div class="mb-3"&gt;
        &lt;textarea id="prompt" class="form-control" placeholder="Enter your prompt here..."&gt;&lt;/textarea&gt;
    &lt;/div&gt;
    &lt;button onclick="runComparison()" class="btn btn-primary"&gt;Run Comparison&lt;/button&gt;

    &lt;div class="row mt-5" id="results"&gt;
        &lt;!-- Results will be injected here --&gt;
    &lt;/div&gt;

    &lt;script&gt;
        async function runComparison() {
            const prompt = document.getElementById('prompt').value;
            const resDiv = document.getElementById('results');
            resDiv.innerHTML = 'Loading...';

            const response = await fetch('/compare', {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({prompt})
            });

            const data = await response.json();
            resDiv.innerHTML = '';

            for (const [model, result] of Object.entries(data)) {
                resDiv.innerHTML += `
                    &lt;div class="col-md-4"&gt;
                        &lt;div class="card"&gt;
                            &lt;div class="card-header"&gt;&lt;strong&gt;${model}&lt;/strong&gt;&lt;/div&gt;
                            &lt;div class="card-body"&gt;${result}&lt;/div&gt;
                        &lt;/div&gt;
                    &lt;/div&gt;`;
            }
        }
    &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>

<h2>Step 3: Run and Explore</h2>
<ol>
    <li>Ensure your LiteLLM Proxy is running with those models configured.</li>
    <li>Start the Flask app: <code>python app.py</code>.</li>
    <li>Open your browser to <code>http://localhost:5000</code>.</li>
    <li>Enter a complex prompt and see how different models respond differently.</li>
</ol>

<h2>Challenge: Add Latency Tracking</h2>
<p>Modify the backend to measure how long each model takes to respond and display the latency in the dashboard cards. This will help you evaluate the performance-to-cost trade-offs of each model.</p>

<p>By completing this exercise, you've built a practical tool for AI model evaluation and gained experience in orchestrating multi-model requests in a real application.</p>

<script type="text/javascript">
</script>
</body>
</html>
