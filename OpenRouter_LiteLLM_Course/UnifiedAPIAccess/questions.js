test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.openrouterlitellm.m1_q1",
                                "What is a key characteristic of OpenRouter: Quick access to many models, no infrastructure to manage, pay-as-you-go?",
                                QUESTION_TYPE_CHOICE,
                                new Array("When to Use Each", "OpenRouter", "OpenRouter: Quick access to many models, no infrastructure to manage, pay-as-you-go", "Route to fastest responding deployment"),
                                "OpenRouter: Quick access to many models, no infrastructure to manage, pay-as-you-go",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.openrouterlitellm.m1_q2",
                                "In the context of Unified API Access, what does LiteLLM: Full control, self-hosted proxy, enterprise environments, cost optimization refer to?",
                                QUESTION_TYPE_CHOICE,
                                new Array("LiteLLM Proxy Server", "Fallback Strategies", "LiteLLM: Full control, self-hosted proxy, enterprise environments, cost optimization", "Comparison"),
                                "LiteLLM: Full control, self-hosted proxy, enterprise environments, cost optimization",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.openrouterlitellm.m1_q3",
                                "What is the primary purpose of Both together: Use LiteLLM as your abstraction layer with OpenRouter as one of the provide?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Provider arbitrage: Compare pricing across providers for the same model quality", "Both together: Use LiteLLM as your abstraction layer with OpenRouter as one of the provide", "Route to deployment with lowest active requests", "The Multi-Provider Challenge"),
                                "Both together: Use LiteLLM as your abstraction layer with OpenRouter as one of the provide",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.openrouterlitellm.m1_q4",
                                "What role does Model tiering: Route simple tasks to cheaper models (GPT-4o-mini, Haiku) and complex tasks play in Unified API Access?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Model tiering: Route simple tasks to cheaper models (GPT-4o-mini, Haiku) and complex tasks", "Caching: LiteLLM supports response caching to avoid duplicate API calls", "The Multi-Provider Challenge", "Provider arbitrage: Compare pricing across providers for the same model quality"),
                                "Model tiering: Route simple tasks to cheaper models (GPT-4o-mini, Haiku) and complex tasks",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.openrouterlitellm.m1_q5",
                                "Which of the following is true regarding Caching: LiteLLM supports response caching to avoid duplicate API calls?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Latency-sensitive apps", "200+ models, managed access", "LiteLLM Proxy Server", "Caching: LiteLLM supports response caching to avoid duplicate API calls"),
                                "Caching: LiteLLM supports response caching to avoid duplicate API calls",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.openrouterlitellm.m1_q6",
                                "Which statement about Token limits: Set max_tokens per request to prevent runaway costs is accurate?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Token limits: Set max_tokens per request to prevent runaway costs", "Route to fastest responding deployment", "Both together: Use LiteLLM as your abstraction layer with OpenRouter as one of the provide", "Fallback Strategies"),
                                "Token limits: Set max_tokens per request to prevent runaway costs",
                                "obj_module_1")
                );