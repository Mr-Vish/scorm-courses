<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Multi-Provider Routing</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Multi-Provider Routing</h1>


<h2>The Multi-Provider Challenge</h2>
<p>Modern GenAI applications often need to use models from multiple providers - OpenAI, Anthropic, Google, open-source models, and more. Each provider has a different API format, authentication method, and SDK. This creates complexity in code, testing, and vendor management.</p>

<h2>OpenRouter</h2>
<p>OpenRouter is a hosted API gateway that provides a single OpenAI-compatible endpoint for 200+ models from multiple providers:</p>
<div class="code-block">
<pre><code>from openai import OpenAI

# Use OpenRouter as a drop-in replacement
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="sk-or-v1-your-key-here",
)

# Access any model through one API
response = client.chat.completions.create(
    model="anthropic/claude-sonnet-4-20250514",  # Anthropic model
    messages=[{"role": "user", "content": "Hello!"}],
)

# Switch to a different provider - same code
response = client.chat.completions.create(
    model="google/gemini-2.0-flash-001",  # Google model
    messages=[{"role": "user", "content": "Hello!"}],
)</code></pre>
</div>

<h2>LiteLLM</h2>
<p>LiteLLM is an open-source Python library and proxy server that standardizes LLM API calls across 100+ providers:</p>
<div class="code-block">
<pre><code>import litellm

# Same function call, different providers
response = litellm.completion(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}],
)

response = litellm.completion(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "Hello!"}],
)

response = litellm.completion(
    model="ollama/llama3",  # Local model via Ollama
    messages=[{"role": "user", "content": "Hello!"}],
)</code></pre>
</div>

<h2>Comparison</h2>
<table>
    <tr><th>Feature</th><th>OpenRouter</th><th>LiteLLM</th></tr>
    <tr><td>Type</td><td>Hosted SaaS</td><td>Open-source library + proxy</td></tr>
    <tr><td>Models</td><td>200+ models, managed access</td><td>100+ providers, bring your own keys</td></tr>
    <tr><td>Pricing</td><td>Pass-through + small markup</td><td>Free (open-source)</td></tr>
    <tr><td>Self-hosting</td><td>No</td><td>Yes - full control</td></tr>
    <tr><td>API format</td><td>OpenAI-compatible</td><td>OpenAI-compatible</td></tr>
    <tr><td>Auth</td><td>Single API key</td><td>Individual provider keys</td></tr>
</table>

<h2>When to Use Each</h2>
<ul>
    <li><strong>OpenRouter:</strong> Quick access to many models, no infrastructure to manage, pay-as-you-go</li>
    <li><strong>LiteLLM:</strong> Full control, self-hosted proxy, enterprise environments, cost optimization</li>
    <li><strong>Both together:</strong> Use LiteLLM as your abstraction layer with OpenRouter as one of the provider backends</li>
</ul>


<script type="text/javascript">
</script>
</body>
</html>