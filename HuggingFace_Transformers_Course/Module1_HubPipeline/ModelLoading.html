<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Model Loading and Configuration</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Model Loading and Configuration</h1>

<h2>AutoModel Classes</h2>
<p>The Transformers library provides Auto classes that automatically select the correct model architecture based on the model name or configuration:</p>

<table>
    <tr>
        <th>Auto Class</th>
        <th>Purpose</th>
        <th>Example Use Cases</th>
    </tr>
    <tr>
        <td class="rowheader">AutoModel</td>
        <td>Base model without task-specific head</td>
        <td>Feature extraction, custom heads</td>
    </tr>
    <tr>
        <td class="rowheader">AutoModelForCausalLM</td>
        <td>Causal language modeling (next token prediction)</td>
        <td>Text generation, chatbots, code completion</td>
    </tr>
    <tr>
        <td class="rowheader">AutoModelForSequenceClassification</td>
        <td>Sequence classification tasks</td>
        <td>Sentiment analysis, topic classification</td>
    </tr>
    <tr>
        <td class="rowheader">AutoModelForTokenClassification</td>
        <td>Token-level classification</td>
        <td>Named entity recognition, POS tagging</td>
    </tr>
    <tr>
        <td class="rowheader">AutoModelForQuestionAnswering</td>
        <td>Extractive question answering</td>
        <td>Document QA, information retrieval</td>
    </tr>
    <tr>
        <td class="rowheader">AutoModelForSeq2SeqLM</td>
        <td>Sequence-to-sequence tasks</td>
        <td>Translation, summarization</td>
    </tr>
</table>

<h2>Basic Model Loading</h2>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model and tokenizer
model_name = "meta-llama/Llama-3-8b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Check model size
param_count = sum(p.numel() for p in model.parameters())
print(f"Model parameters: {param_count:,}")

# Check model dtype
print(f"Model dtype: {model.dtype}")

# Move model to GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)\n</code></pre>

<h2>Model Configuration</h2>
<p>Every model has a configuration object that defines its architecture and behavior:</p>

<pre><code>from transformers import AutoConfig

# Load configuration
config = AutoConfig.from_pretrained("meta-llama/Llama-3-8b-instruct")

# View key configuration parameters
print(f"Hidden size: {config.hidden_size}")
print(f"Number of layers: {config.num_hidden_layers}")
print(f"Number of attention heads: {config.num_attention_heads}")
print(f"Vocabulary size: {config.vocab_size}")
print(f"Max position embeddings: {config.max_position_embeddings}")

# Load model with custom configuration
custom_config = AutoConfig.from_pretrained(
    "meta-llama/Llama-3-8b-instruct",
    max_position_embeddings=4096,  # Override default
    use_cache=True  # Enable KV caching
)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8b-instruct",
    config=custom_config
)\n</code></pre>

<h2>Device Mapping for Large Models</h2>
<p>For models that don't fit in a single GPU, use device_map to automatically distribute layers:</p>

<pre><code>from transformers import AutoModelForCausalLM

# Automatic device mapping
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-70b-instruct",
    device_map="auto",  # Automatically distribute across available GPUs
    torch_dtype=torch.float16  # Use half precision
)

# Manual device mapping
device_map = {
    "model.embed_tokens": 0,
    "model.layers.0": 0,
    "model.layers.1": 0,
    "model.layers.2": 1,
    "model.layers.3": 1,
    "model.norm": 1,
    "lm_head": 1
}

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-70b-instruct",
    device_map=device_map,
    torch_dtype=torch.float16
)

# Check device placement
print(model.hf_device_map)\n</code></pre>

<h2>Data Types and Precision</h2>
<table>
    <tr>
        <th>Data Type</th>
        <th>Precision</th>
        <th>Memory</th>
        <th>Use Case</th>
    </tr>
    <tr>
        <td class="rowheader">float32 (FP32)</td>
        <td>Full precision</td>
        <td>4 bytes/param</td>
        <td>Training, maximum accuracy</td>
    </tr>
    <tr>
        <td class="rowheader">float16 (FP16)</td>
        <td>Half precision</td>
        <td>2 bytes/param</td>
        <td>Inference, good balance</td>
    </tr>
    <tr>
        <td class="rowheader">bfloat16 (BF16)</td>
        <td>Brain float</td>
        <td>2 bytes/param</td>
        <td>Training and inference, better range than FP16</td>
    </tr>
    <tr>
        <td class="rowheader">int8</td>
        <td>8-bit integer</td>
        <td>1 byte/param</td>
        <td>Quantized inference, 4x memory savings</td>
    </tr>
    <tr>
        <td class="rowheader">int4</td>
        <td>4-bit integer</td>
        <td>0.5 bytes/param</td>
        <td>Aggressive quantization, 8x memory savings</td>
    </tr>
</table>

<pre><code># Load in different precisions
model_fp32 = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    torch_dtype=torch.float32
)

model_fp16 = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    torch_dtype=torch.float16
)

model_bf16 = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    torch_dtype=torch.bfloat16
)

# Check memory usage
def get_model_size(model):
    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())
    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())
    return (param_size + buffer_size) / 1024**2  # MB

print(f"FP32 size: {get_model_size(model_fp32):.2f} MB")
print(f"FP16 size: {get_model_size(model_fp16):.2f} MB")\n</code></pre>

<h2>Low Memory Loading</h2>
<pre><code># Load with low_cpu_mem_usage to reduce RAM requirements
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8b-instruct",
    device_map="auto",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True  # Load directly to GPU, skip CPU
)

# Use offloading for models larger than GPU memory
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-70b-instruct",
    device_map="auto",
    offload_folder="offload",  # Offload to disk if needed
    offload_state_dict=True
)\n</code></pre>

<h2>Trust Remote Code</h2>
<p>Some models include custom code that needs explicit permission to execute:</p>

<pre><code># Models with custom modeling code
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    trust_remote_code=True  # Required for models with custom code
)

# This is a security consideration - only use with trusted models\n</code></pre>

<h2>Model Sharding</h2>
<p>Large models are often saved in multiple files (shards). The library handles this automatically:</p>

<pre><code># Automatically loads all shards
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-70b-instruct"
)
# Loads: pytorch_model-00001-of-00015.bin, pytorch_model-00002-of-00015.bin, etc.

# Check if model is sharded
from transformers import AutoConfig
config = AutoConfig.from_pretrained("meta-llama/Llama-3-70b-instruct")
print(f"Model is sharded: {config.tie_word_embeddings}")\n</code></pre>

<h2>Saving and Loading Custom Models</h2>
<pre><code># Save model and tokenizer
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")

# This creates:
# - config.json (model configuration)
# - pytorch_model.bin (model weights)
# - tokenizer.json (tokenizer)
# - tokenizer_config.json (tokenizer configuration)

# Load from local directory
model = AutoModelForCausalLM.from_pretrained("./my_model")
tokenizer = AutoTokenizer.from_pretrained("./my_model")

# Push to Hugging Face Hub
model.push_to_hub("my-username/my-model")
tokenizer.push_to_hub("my-username/my-model")\n</code></pre>

<h2>Model Inspection</h2>
<pre><code># View model architecture
print(model)

# Count parameters by layer
for name, param in model.named_parameters():
    print(f"{name}: {param.numel():,} parameters")

# Check trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable: {trainable_params:,} / {total_params:,}")

# Get specific layers
print(model.model.layers[0])  # First transformer layer
print(model.lm_head)  # Output projection layer\n</code></pre>

<h2>Generation Configuration</h2>
<pre><code>from transformers import GenerationConfig

# Load default generation config
gen_config = GenerationConfig.from_pretrained("meta-llama/Llama-3-8b-instruct")
print(gen_config)

# Create custom generation config
custom_gen_config = GenerationConfig(
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.1,
    do_sample=True
)

# Save generation config
custom_gen_config.save_pretrained("./my_model")

# Use with model
outputs = model.generate(
    input_ids,
    generation_config=custom_gen_config
)\n</code></pre>

<h2>Model Parallelism Strategies</h2>
<table>
    <tr>
        <th>Strategy</th>
        <th>Description</th>
        <th>When to Use</th>
    </tr>
    <tr>
        <td class="rowheader">Single GPU</td>
        <td>Entire model on one GPU</td>
        <td>Models &lt; 16GB, fastest inference</td>
    </tr>
    <tr>
        <td class="rowheader">Pipeline Parallelism</td>
        <td>Different layers on different GPUs</td>
        <td>Models 16-80GB, sequential processing</td>
    </tr>
    <tr>
        <td class="rowheader">Tensor Parallelism</td>
        <td>Split individual layers across GPUs</td>
        <td>Very large models, requires framework support</td>
    </tr>
    <tr>
        <td class="rowheader">CPU Offloading</td>
        <td>Keep some layers in RAM</td>
        <td>Limited GPU memory, slower but works</td>
    </tr>
</table>

<h2>Best Practices</h2>
<ul>
    <li><strong>Use Appropriate Precision:</strong> FP16/BF16 for inference, FP32 only when necessary</li>
    <li><strong>Enable device_map="auto":</strong> For large models that need multi-GPU support</li>
    <li><strong>Set low_cpu_mem_usage=True:</strong> Reduces RAM requirements during loading</li>
    <li><strong>Pin Model Versions:</strong> Use specific commits in production for reproducibility</li>
    <li><strong>Monitor Memory:</strong> Use torch.cuda.memory_summary() to track GPU usage</li>
    <li><strong>Save Configurations:</strong> Store generation configs with models for consistency</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
