<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Model Compression and ONNX</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Model Compression and ONNX</h1>


<h2>Edge AI for GenAI</h2>
<p>Edge AI runs machine learning models directly on devices (phones, IoT sensors, embedded systems) rather than in the cloud. For GenAI, this means running language models on constrained hardware with limited memory, compute, and power - enabling private, offline, low-latency AI.</p>

<h2>Edge Deployment Challenges</h2>
<table>
    <tr><th>Challenge</th><th>Cloud AI</th><th>Edge AI</th></tr>
    <tr><td>Memory</td><td>Hundreds of GB available</td><td>1-8 GB typical</td></tr>
    <tr><td>Compute</td><td>GPU clusters (A100, H100)</td><td>Mobile CPU/GPU, NPU, TPU</td></tr>
    <tr><td>Power</td><td>Unlimited (data center)</td><td>Battery-constrained</td></tr>
    <tr><td>Connectivity</td><td>Always online</td><td>Intermittent or offline</td></tr>
    <tr><td>Model size</td><td>Hundreds of GB OK</td><td>Must fit in device memory</td></tr>
    <tr><td>Latency</td><td>Network round-trip (100ms+)</td><td>On-device (10-50ms)</td></tr>
</table>

<h2>Model Compression Techniques</h2>
<table>
    <tr><th>Technique</th><th>Size Reduction</th><th>Quality Impact</th><th>Best For</th></tr>
    <tr><td>Quantization (INT4)</td><td>4x</td><td>Minimal</td><td>All edge deployments</td></tr>
    <tr><td>Pruning</td><td>2-10x</td><td>Moderate</td><td>Structured tasks with pruned layers</td></tr>
    <tr><td>Knowledge distillation</td><td>10-100x</td><td>Varies</td><td>Task-specific small models</td></tr>
    <tr><td>Weight sharing</td><td>2-4x</td><td>Minimal</td><td>Memory-constrained devices</td></tr>
    <tr><td>Low-rank factorization</td><td>2-5x</td><td>Small</td><td>Matrix-heavy layers</td></tr>
</table>

<h2>ONNX for Cross-Platform Deployment</h2>
<div class="code-block">
<pre><code># Export a HuggingFace model to ONNX format
from optimum.onnxruntime import ORTModelForCausalLM
from transformers import AutoTokenizer

# Export model to ONNX
model = ORTModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    export=True,  # Converts to ONNX automatically
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

# Save ONNX model
model.save_pretrained("./phi3-onnx")
tokenizer.save_pretrained("./phi3-onnx")

# Run inference with ONNX Runtime
from optimum.onnxruntime import ORTModelForCausalLM

ort_model = ORTModelForCausalLM.from_pretrained("./phi3-onnx")
inputs = tokenizer("Hello, how are you?", return_tensors="pt")
outputs = ort_model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))</code></pre>
</div>

<h2>ONNX Runtime Benefits</h2>
<ul>
    <li><strong>Cross-platform:</strong> Run the same model on Windows, Linux, macOS, Android, iOS, and web browsers</li>
    <li><strong>Hardware acceleration:</strong> Automatically uses CUDA, DirectML, CoreML, or XNNPACK depending on the device</li>
    <li><strong>Optimization:</strong> Graph optimizations (operator fusion, constant folding) improve speed without code changes</li>
    <li><strong>Quantization support:</strong> Built-in quantization tools to reduce model size further</li>
</ul>

<h2>Edge Model Selection Guide</h2>
<table>
    <tr><th>Device Type</th><th>RAM</th><th>Recommended Models</th><th>Runtime</th></tr>
    <tr><td>Smartphone (high-end)</td><td>6-12 GB</td><td>Phi-3 Mini, Gemma 2B, Llama 3.2 3B</td><td>ONNX, Core ML, TFLite</td></tr>
    <tr><td>Smartphone (mid-range)</td><td>4-6 GB</td><td>Qwen 0.5B, TinyLlama 1.1B</td><td>ONNX, TFLite</td></tr>
    <tr><td>Raspberry Pi 5</td><td>4-8 GB</td><td>TinyLlama, Phi-3 Mini (Q2)</td><td>llama.cpp</td></tr>
    <tr><td>Web browser</td><td>2-4 GB</td><td>Phi-3 Mini (Q4), SmolLM</td><td>WebLLM, Transformers.js</td></tr>
    <tr><td>Embedded (MCU)</td><td>&lt; 1 GB</td><td>Custom distilled models</td><td>TFLite Micro, ONNX Micro</td></tr>
</table>


<script type="text/javascript">
</script>
</body>
</html>