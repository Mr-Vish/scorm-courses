<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes">
    <title>Text Watermarking Fundamentals</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Module 2: Text Watermarking Techniques</h1>
<h2>Part 1: Text Watermarking Fundamentals</h2>

<h2>Module Objectives</h2>
<p>In this module, you will:</p>
<ul>
    <li>Understand the specific challenges of watermarking text generated by large language models</li>
    <li>Learn the token-based watermarking paradigm and its mathematical foundations</li>
    <li>Explore the green/red list watermarking algorithm in detail</li>
    <li>Analyze detection mechanisms and their statistical properties</li>
    <li>Evaluate the trade-offs between different text watermarking approaches</li>
</ul>

<h2>Why Text Watermarking is Challenging</h2>
<p>Text watermarking presents unique challenges compared to other media types:</p>

<h3>Discrete Nature of Text</h3>
<p>Unlike images (continuous pixel values) or audio (continuous waveforms), text consists of discrete tokens. This creates fundamental constraints:</p>

<ul>
    <li><strong>Limited Embedding Space:</strong> Each position can only hold one token from a finite vocabulary</li>
    <li><strong>Semantic Sensitivity:</strong> Changing a single word can dramatically alter meaning</li>
    <li><strong>Grammatical Constraints:</strong> Not all token substitutions produce valid sentences</li>
    <li><strong>Context Dependency:</strong> Token appropriateness depends heavily on surrounding context</li>
</ul>

<h3>Human Language Complexity</h3>
<p>Natural language has properties that complicate watermarking:</p>

<ul>
    <li><strong>Synonymy:</strong> Multiple words can express the same concept (paraphrasing attacks)</li>
    <li><strong>Ambiguity:</strong> Words and phrases can have multiple interpretations</li>
    <li><strong>Style Variation:</strong> The same content can be expressed in countless ways</li>
    <li><strong>Cultural Context:</strong> Language use varies across cultures and communities</li>
</ul>

<h3>Quality Expectations</h3>
<p>Users have high expectations for text quality:</p>

<ul>
    <li>Grammatical correctness is non-negotiable</li>
    <li>Unnatural phrasing is immediately noticeable</li>
    <li>Factual accuracy must be maintained</li>
    <li>Stylistic consistency is expected</li>
</ul>

<h2>The Token-Based Watermarking Paradigm</h2>
<p>Modern text watermarking operates at the token level, modifying the generation process as the model selects each token.</p>

<h3>How Language Models Generate Text</h3>
<p>Understanding the generation process is essential for watermarking:</p>

<ol>
    <li><strong>Context Encoding:</strong> The model processes all previous tokens to build a context representation</li>
    <li><strong>Logit Computation:</strong> For each possible next token, the model computes a "logit" (unnormalized probability)</li>
    <li><strong>Probability Distribution:</strong> Logits are converted to probabilities using the softmax function</li>
    <li><strong>Sampling:</strong> A token is selected from this distribution (various sampling strategies exist)</li>
    <li><strong>Iteration:</strong> The selected token becomes part of the context for the next step</li>
</ol>

<h3>Watermarking Intervention Points</h3>
<p>Watermarking can intervene at different stages:</p>

<table>
    <tr>
        <th>Intervention Point</th>
        <th>Method</th>
        <th>Advantages</th>
        <th>Disadvantages</th>
    </tr>
    <tr>
        <td class="rowheader">Logit Modification</td>
        <td>Adjust logits before softmax</td>
        <td>Fine-grained control, smooth transitions</td>
        <td>May affect probability distribution significantly</td>
    </tr>
    <tr>
        <td class="rowheader">Probability Biasing</td>
        <td>Modify probabilities after softmax</td>
        <td>Intuitive, preserves relative rankings</td>
        <td>Can create unnormalized distributions</td>
    </tr>
    <tr>
        <td class="rowheader">Sampling Modification</td>
        <td>Change how tokens are selected</td>
        <td>Minimal impact on distribution, flexible</td>
        <td>May reduce randomness, affect diversity</td>
    </tr>
    <tr>
        <td class="rowheader">Vocabulary Partitioning</td>
        <td>Divide tokens into groups, bias selection</td>
        <td>Simple to implement, provable guarantees</td>
        <td>Coarse-grained, may be detectable</td>
    </tr>
</table>

<h2>Green/Red List Watermarking: Core Concept</h2>
<p>The green/red list approach, pioneered by researchers at the University of Maryland, is one of the most influential text watermarking methods.</p>

<h3>Basic Principle</h3>
<p>At each generation step:</p>

<ol>
    <li><strong>Partition Vocabulary:</strong> Divide all possible tokens into two sets—"green" (preferred) and "red" (discouraged)</li>
    <li><strong>Deterministic Assignment:</strong> Use a hash function with the previous token and a secret key to determine which tokens are green</li>
    <li><strong>Bias Generation:</strong> Increase the probability of selecting green tokens</li>
    <li><strong>Iterate:</strong> Repeat for each token in the sequence</li>
</ol>

<h3>Why This Works</h3>
<p>The key insight is that the green/red partition is <strong>deterministic but unpredictable without the secret key</strong>:</p>

<ul>
    <li>An observer without the key sees normal text (the partition appears random)</li>
    <li>A detector with the key can reconstruct the green/red partitions and check if the text has an unusually high proportion of green tokens</li>
    <li>Human-written text will have approximately 50% green tokens (random chance)</li>
    <li>Watermarked text will have significantly more than 50% green tokens (e.g., 60-70%)</li>
</ul>

<h3>Mathematical Foundation</h3>
<p>The detection is based on hypothesis testing:</p>

<blockquote>
<strong>Null Hypothesis (H₀):</strong> The text is not watermarked (green token proportion ≈ 0.5)<br/>
<strong>Alternative Hypothesis (H₁):</strong> The text is watermarked (green token proportion &gt; 0.5)<br/><br/>
Statistical tests (e.g., z-test) determine which hypothesis is more likely given the observed data.
</blockquote>

<h3>Conceptual Example</h3>
<p>Consider generating the sentence: "The cat sat on the mat."</p>

<table>
    <tr>
        <th>Position</th>
        <th>Previous Token</th>
        <th>Green Tokens (examples)</th>
        <th>Red Tokens (examples)</th>
        <th>Selected</th>
        <th>Color</th>
    </tr>
    <tr>
        <td>1</td>
        <td>[START]</td>
        <td>The, A, This, That</td>
        <td>An, Some, One, My</td>
        <td>The</td>
        <td class="rowheader">Green</td>
    </tr>
    <tr>
        <td>2</td>
        <td>The</td>
        <td>cat, dog, bird, fish</td>
        <td>car, house, tree, book</td>
        <td>cat</td>
        <td class="rowheader">Green</td>
    </tr>
    <tr>
        <td>3</td>
        <td>cat</td>
        <td>sat, jumped, ran, walked</td>
        <td>slept, ate, played, meowed</td>
        <td>sat</td>
        <td class="rowheader">Green</td>
    </tr>
    <tr>
        <td>4</td>
        <td>sat</td>
        <td>on, in, by, near</td>
        <td>under, over, beside, behind</td>
        <td>on</td>
        <td class="rowheader">Green</td>
    </tr>
    <tr>
        <td>5</td>
        <td>on</td>
        <td>the, a, my, his</td>
        <td>her, our, their, its</td>
        <td>the</td>
        <td class="rowheader">Green</td>
    </tr>
    <tr>
        <td>6</td>
        <td>the</td>
        <td>mat, floor, chair, bed</td>
        <td>table, sofa, rug, cushion</td>
        <td>mat</td>
        <td class="rowheader">Green</td>
    </tr>
</table>

<p><strong>Result:</strong> 6 out of 6 tokens are green (100%). This is highly unlikely for non-watermarked text (expected: ~50%), providing strong evidence of watermarking.</p>

<h2>Watermark Strength: The Delta Parameter</h2>
<p>The strength of the watermark is controlled by a parameter often called "delta" (δ):</p>

<h3>Low Delta (Weak Watermark)</h3>
<ul>
    <li><strong>Effect:</strong> Small bias toward green tokens</li>
    <li><strong>Quality:</strong> Minimal impact on text naturalness</li>
    <li><strong>Detectability:</strong> Requires longer text for reliable detection</li>
    <li><strong>Robustness:</strong> Vulnerable to paraphrasing and editing</li>
    <li><strong>Use Case:</strong> Creative writing, marketing content where quality is paramount</li>
</ul>

<h3>High Delta (Strong Watermark)</h3>
<ul>
    <li><strong>Effect:</strong> Strong bias toward green tokens</li>
    <li><strong>Quality:</strong> May produce slightly unnatural phrasing</li>
    <li><strong>Detectability:</strong> Can be detected in short text segments</li>
    <li><strong>Robustness:</strong> Survives moderate editing and paraphrasing</li>
    <li><strong>Use Case:</strong> Factual content, news, documentation where authenticity is critical</li>
</ul>

<h3>Adaptive Delta</h3>
<p>Advanced systems may adjust delta dynamically:</p>

<ul>
    <li>Higher delta for high-confidence tokens (where multiple good options exist)</li>
    <li>Lower delta for low-confidence tokens (where forcing green tokens would degrade quality)</li>
    <li>Context-dependent adjustment based on content type or user preferences</li>
</ul>

<h2>Vocabulary Partitioning Strategies</h2>
<p>How tokens are divided into green and red sets significantly impacts watermark properties:</p>

<h3>Random Partitioning (50/50 Split)</h3>
<ul>
    <li>Half the vocabulary is green, half is red</li>
    <li>Maximizes entropy of the partition</li>
    <li>Provides strong statistical guarantees</li>
    <li>May force suboptimal token choices</li>
</ul>

<h3>Probability-Weighted Partitioning</h3>
<ul>
    <li>More likely tokens have higher chance of being green</li>
    <li>Reduces quality impact</li>
    <li>May weaken detection in some cases</li>
    <li>Adapts to model's natural preferences</li>
</ul>

<h3>Semantic Clustering</h3>
<ul>
    <li>Semantically similar tokens are grouped together</li>
    <li>Entire synonym sets are green or red</li>
    <li>Improves robustness to paraphrasing</li>
    <li>Complex to implement, requires semantic understanding</li>
</ul>

<h2>Hash Function Selection</h2>
<p>The hash function that determines green/red partitions must have specific properties:</p>

<h3>Required Properties</h3>
<table>
    <tr>
        <th>Property</th>
        <th>Why It Matters</th>
    </tr>
    <tr>
        <td class="rowheader">Determinism</td>
        <td>Same input always produces same output (enables detection)</td>
    </tr>
    <tr>
        <td class="rowheader">Uniformity</td>
        <td>Outputs are evenly distributed (prevents bias)</td>
    </tr>
    <tr>
        <td class="rowheader">Unpredictability</td>
        <td>Cannot guess output without computing (security)</td>
    </tr>
    <tr>
        <td class="rowheader">Efficiency</td>
        <td>Fast computation (minimal generation overhead)</td>
    </tr>
    <tr>
        <td class="rowheader">Collision Resistance</td>
        <td>Different inputs produce different outputs (prevents attacks)</td>
    </tr>
</table>

<h3>Common Choices</h3>
<ul>
    <li><strong>SHA-256:</strong> Cryptographically secure, widely used, slower</li>
    <li><strong>MD5:</strong> Faster but less secure (acceptable for watermarking)</li>
    <li><strong>MurmurHash:</strong> Very fast, good distribution, non-cryptographic</li>
    <li><strong>Custom Functions:</strong> Optimized for specific watermarking needs</li>
</ul>

<h2>Key Takeaways</h2>
<ul>
    <li>Text watermarking is challenging due to the discrete nature of language and high quality expectations</li>
    <li>Token-based watermarking modifies the generation process at each step</li>
    <li>Green/red list watermarking partitions the vocabulary and biases selection toward green tokens</li>
    <li>Detection relies on statistical testing: watermarked text has more green tokens than expected by chance</li>
    <li>The delta parameter controls the trade-off between quality and detectability</li>
    <li>Hash functions must be deterministic, uniform, and unpredictable for secure watermarking</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
