// Final Comprehensive Assessment
// Covers all 1 modules


test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.final_q1",
                                "Regarding Understanding Tokenization: What is a key characteristic of CJK languages: ~0.3-0.5 words per token (1.5-2.5x cost)?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Google (Gemini), Meta (Llama)", "CJK languages: ~0.3-0.5 words per token (1.5-2.5x cost)", "English: ~0.75 words per token (baseline cost)", "Find the most frequent pair of adjacent tokens"),
                                "CJK languages: ~0.3-0.5 words per token (1.5-2.5x cost)",
                                "obj_final_assessment")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.final_q2",
                                "Regarding Understanding Tokenization: Which statement about Code: Varies widely by language and style (~0.4-0.8 words per token) is accurate?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Code: Varies widely by language and style (~0.4-0.8 words per token)", "Byte Pair Encoding (BPE)", "Count tokens before sending requests to avoid context window overflow", "OpenAI (GPT-4, GPT-3.5)"),
                                "Code: Varies widely by language and style (~0.4-0.8 words per token)",
                                "obj_final_assessment")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.final_q3",
                                "Regarding Understanding Tokenization: In the context of Understanding Tokenization, what does Count tokens before sending requests to avoid context window overflow refer to?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Consider tokenization efficiency when choosing models for multilingual tasks", "Count tokens before sending requests to avoid context window overflow", "Why Tokenization Matters", "HuggingFace Tokenizers"),
                                "Count tokens before sending requests to avoid context window overflow",
                                "obj_final_assessment")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.final_q4",
                                "Regarding Understanding Tokenization: What is the primary purpose of Use provider-specific token counters for accurate counts?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Find the most frequent pair of adjacent tokens", "Arabic equivalent", "OpenAI (GPT-4, GPT-3.5)", "Use provider-specific token counters for accurate counts"),
                                "Use provider-specific token counters for accurate counts",
                                "obj_final_assessment")
                );