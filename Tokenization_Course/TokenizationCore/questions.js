test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.m1_q1",
                                "Which of the following is true regarding Start with individual characters as tokens?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Cost Implications", "Count tokens before sending requests to avoid context window overflow", "Repeat until vocabulary size is reached (typically 32K-100K tokens)", "Start with individual characters as tokens"),
                                "Start with individual characters as tokens",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.m1_q2",
                                "What role does Find the most frequent pair of adjacent tokens play in Understanding Tokenization?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Cost Implications", "\"Hola, como estas?\"", "Count tokens before sending requests to avoid context window overflow", "Find the most frequent pair of adjacent tokens"),
                                "Find the most frequent pair of adjacent tokens",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.m1_q3",
                                "Which of the following best describes Merge that pair into a new token?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Token Budgeting for Applications", "Best Practices", "Why Tokenization Matters", "Merge that pair into a new token"),
                                "Merge that pair into a new token",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.m1_q4",
                                "What is the primary purpose of Repeat until vocabulary size is reached (typically 32K-100K tokens)?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Google (Gemini), Meta (Llama)", "Repeat until vocabulary size is reached (typically 32K-100K tokens)", "Budget for both input and output tokens when estimating costs", "HuggingFace Tokenizers"),
                                "Repeat until vocabulary size is reached (typically 32K-100K tokens)",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.m1_q5",
                                "In the context of Understanding Tokenization, what does English: ~0.75 words per token (baseline cost) refer to?",
                                QUESTION_TYPE_CHOICE,
                                new Array("Byte Pair Encoding (BPE)", "Merge that pair into a new token", "English: ~0.75 words per token (baseline cost)", "Open-source models"),
                                "English: ~0.75 words per token (baseline cost)",
                                "obj_module_1")
                );

test.AddQuestion( new Question ("com.scorm.com.scorm.genaicourse.tokenization.m1_q6",
                                "Which statement about European languages: ~0.5-0.7 words per token (1.1-1.5x cost) is accurate?",
                                QUESTION_TYPE_CHOICE,
                                new Array("\"Hola, como estas?\"", "Start with individual characters as tokens", "European languages: ~0.5-0.7 words per token (1.1-1.5x cost)", "Merge that pair into a new token"),
                                "European languages: ~0.5-0.7 words per token (1.1-1.5x cost)",
                                "obj_module_1")
                );