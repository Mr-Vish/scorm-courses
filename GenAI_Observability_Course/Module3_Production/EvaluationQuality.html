<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Evaluation and Quality Monitoring</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Evaluation and Quality Monitoring</h1>

<h2>The Quality Challenge</h2>
<p>Unlike traditional software where correctness is binary (works or doesn't work), GenAI output quality exists on a spectrum. A response might be technically correct but unhelpful, accurate but too verbose, or creative but inappropriate for the context.</p>

<h2>Evaluation Approaches</h2>

<h3>1. Human Evaluation</h3>
<p><strong>Strengths:</strong> Most accurate, captures nuance and context</p>
<p><strong>Weaknesses:</strong> Expensive, slow, doesn't scale</p>
<p><strong>Best for:</strong> Gold standard datasets, spot-checking, edge cases</p>

<table>
    <tr><th>Evaluation Type</th><th>When to Use</th><th>Cost</th></tr>
    <tr>
        <td class="rowheader">Expert Review</td>
        <td>Domain-specific accuracy (medical, legal, technical)</td>
        <td>$50-200/hour</td>
    </tr>
    <tr>
        <td class="rowheader">Crowd Annotation</td>
        <td>General quality, helpfulness, tone</td>
        <td>$0.10-0.50/evaluation</td>
    </tr>
    <tr>
        <td class="rowheader">User Feedback</td>
        <td>Real-world satisfaction, free but biased</td>
        <td>Free</td>
    </tr>
</table>

<h3>2. Automated Evaluation (LLM-as-Judge)</h3>
<p>Use a powerful LLM to evaluate outputs from your production model:</p>

<div class="code-block">
<pre><code>async def llm_as_judge(question, answer, criteria="helpfulness"):
    judge_prompt = f"""Evaluate the following AI response on a scale of 1-5 for {criteria}.
    
Question: {question}
Answer: {answer}

Provide:
1. Score (1-5)
2. Brief justification

Format: Score: X | Justification: ...
"""
    
    judge_response = await claude_opus.generate(judge_prompt)
    
    # Parse score
    score = int(judge_response.split("Score:")[1].split("|")[0].strip())
    justification = judge_response.split("Justification:")[1].strip()
    
    return {"score": score, "justification": justification}
</code></pre>
</div>

<h3>3. Rule-Based Evaluation</h3>
<p>Automated checks for specific quality criteria:</p>

<div class="code-block">
<pre><code>class QualityChecker:
    @staticmethod
    def check_response_length(response, min_words=10, max_words=500):
        word_count = len(response.split())
        return min_words <= word_count <= max_words
    
    @staticmethod
    def check_contains_answer(response, required_keywords):
        return any(keyword.lower() in response.lower() 
                  for keyword in required_keywords)
    
    @staticmethod
    def check_no_refusal(response):
        refusal_phrases = [
            "I cannot", "I'm unable to", "I don't have access",
            "I cannot provide", "I'm not able to"
        ]
        return not any(phrase in response for phrase in refusal_phrases)
    
    @staticmethod
    def check_formatting(response, expected_format="markdown"):
        if expected_format == "markdown":
            has_headers = "#" in response
            has_lists = any(marker in response for marker in ["- ", "* ", "1. "])
            return has_headers or has_lists
        return True
</code></pre>
</div>

<h3>4. Embedding-Based Similarity</h3>
<p>Compare response embeddings to reference answers:</p>

<div class="code-block">
<pre><code>import numpy as np

async def semantic_similarity(response, reference_answer):
    # Get embeddings
    response_embedding = await get_embedding(response)
    reference_embedding = await get_embedding(reference_answer)
    
    # Calculate cosine similarity
    similarity = np.dot(response_embedding, reference_embedding) / (
        np.linalg.norm(response_embedding) * np.linalg.norm(reference_embedding)
    )
    
    return similarity  # 0 to 1, higher is more similar
</code></pre>
</div>

<h2>Production Evaluation Pipeline</h2>

<div class="code-block">
<pre><code>class ProductionEvaluator:
    def __init__(self):
        self.quality_checker = QualityChecker()
        self.sample_rate = 0.05  # Evaluate 5% of requests
    
    async def evaluate_response(self, request_id, question, response):
        # Sample requests for evaluation
        if random.random() > self.sample_rate:
            return
        
        evaluation = {
            "request_id": request_id,
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {}
        }
        
        # Rule-based checks (fast, cheap)
        evaluation["checks"]["length_ok"] = self.quality_checker.check_response_length(response)
        evaluation["checks"]["no_refusal"] = self.quality_checker.check_no_refusal(response)
        
        # LLM-as-judge (slower, more expensive)
        if evaluation["checks"]["length_ok"] and evaluation["checks"]["no_refusal"]:
            judge_result = await llm_as_judge(question, response, "helpfulness")
            evaluation["helpfulness_score"] = judge_result["score"]
            evaluation["justification"] = judge_result["justification"]
        
        # Store evaluation
        await db.insert("evaluations", evaluation)
        
        # Alert if quality is poor
        if evaluation.get("helpfulness_score", 5) < 3:
            await alert_low_quality(request_id, evaluation)
</code></pre>
</div>

<h2>Evaluation Metrics</h2>

<h3>Accuracy Metrics</h3>
<ul>
    <li><strong>Factual Accuracy:</strong> Percentage of verifiable claims that are correct</li>
    <li><strong>Hallucination Rate:</strong> Percentage of responses containing fabricated information</li>
    <li><strong>Citation Accuracy:</strong> For RAG systems, whether cited sources support claims</li>
</ul>

<h3>Relevance Metrics</h3>
<ul>
    <li><strong>Answer Relevance:</strong> Does the response address the question?</li>
    <li><strong>Context Relevance:</strong> For RAG, is retrieved context relevant to the question?</li>
    <li><strong>Groundedness:</strong> Is the response supported by provided context?</li>
</ul>

<h3>Quality Metrics</h3>
<ul>
    <li><strong>Helpfulness:</strong> Does the response solve the user's problem?</li>
    <li><strong>Clarity:</strong> Is the response easy to understand?</li>
    <li><strong>Completeness:</strong> Does it address all aspects of the question?</li>
    <li><strong>Conciseness:</strong> Is it appropriately brief without unnecessary verbosity?</li>
</ul>

<h2>Continuous Evaluation</h2>

<h3>Regression Testing</h3>
<p>Maintain a test suite of example inputs with expected outputs:</p>

<div class="code-block">
<pre><code># test_suite.yaml
test_cases:
  - id: "basic_qa_1"
    input: "What is the capital of France?"
    expected_answer: "Paris"
    evaluation_criteria:
      - contains_keyword: "Paris"
      - max_length: 50
      - min_score: 4
  
  - id: "complex_reasoning_1"
    input: "If a train leaves at 2pm going 60mph..."
    expected_answer: "The train arrives at 5pm"
    evaluation_criteria:
      - contains_keyword: "5pm"
      - shows_reasoning: true
      - min_score: 4

# Run before each deployment
async def run_regression_tests():
    results = []
    for test_case in load_test_suite():
        response = await llm.generate(test_case["input"])
        
        passed = all([
            check_criteria(response, criteria)
            for criteria in test_case["evaluation_criteria"]
        ])
        
        results.append({
            "test_id": test_case["id"],
            "passed": passed,
            "response": response
        })
    
    pass_rate = sum(r["passed"] for r in results) / len(results)
    
    if pass_rate < 0.95:
        raise Exception(f"Regression tests failed: {pass_rate:.1%} pass rate")
    
    return results
</code></pre>
</div>

<h3>A/B Testing</h3>
<p>Compare different models, prompts, or configurations:</p>

<div class="code-block">
<pre><code>class ABTestManager:
    def __init__(self):
        self.experiments = {}
    
    def assign_variant(self, user_id, experiment_name):
        # Consistent assignment based on user_id
        hash_val = int(hashlib.md5(f"{user_id}{experiment_name}".encode()).hexdigest(), 16)
        return "A" if hash_val % 2 == 0 else "B"
    
    async def run_experiment(self, user_id, experiment_name, prompt):
        variant = self.assign_variant(user_id, experiment_name)
        
        if variant == "A":
            # Control: existing prompt/model
            response = await llm_v1.generate(prompt)
        else:
            # Treatment: new prompt/model
            response = await llm_v2.generate(prompt)
        
        # Log for analysis
        await db.insert("ab_test_results", {
            "experiment": experiment_name,
            "variant": variant,
            "user_id": user_id,
            "timestamp": datetime.utcnow()
        })
        
        return response

# Analyze results
SELECT 
    variant,
    COUNT(*) as requests,
    AVG(user_rating) as avg_rating,
    AVG(latency_ms) as avg_latency,
    AVG(cost) as avg_cost
FROM ab_test_results
WHERE experiment = 'prompt_optimization_v2'
GROUP BY variant;
</code></pre>
</div>

<h2>Quality Monitoring Dashboard</h2>
<p>Track quality metrics over time:</p>
<ul>
    <li><strong>Average evaluation scores:</strong> Trend of helpfulness, accuracy, relevance</li>
    <li><strong>User feedback distribution:</strong> Histogram of ratings (1-5 stars)</li>
    <li><strong>Refusal rate:</strong> Percentage of requests the model refuses</li>
    <li><strong>Hallucination detection:</strong> Flagged responses per day</li>
    <li><strong>Quality by feature:</strong> Compare scores across different use cases</li>
    <li><strong>Quality vs. cost:</strong> Scatter plot showing trade-offs</li>
</ul>

<h2>Handling Quality Issues</h2>

<h3>Automated Responses</h3>
<ul>
    <li><strong>Low quality detected:</strong> Retry with different model or prompt</li>
    <li><strong>Hallucination detected:</strong> Add disclaimer or request human review</li>
    <li><strong>Refusal:</strong> Try alternative phrasing or escalate to human</li>
    <li><strong>Off-topic:</strong> Clarify user intent before responding</li>
</ul>

<h3>Continuous Improvement Loop</h3>
<ol>
    <li>Collect evaluation data from production</li>
    <li>Identify patterns in low-quality responses</li>
    <li>Update prompts, add examples, or fine-tune models</li>
    <li>Run regression tests to verify improvements</li>
    <li>Deploy changes and monitor impact</li>
    <li>Repeat</li>
</ol>

<script type="text/javascript">
</script>
</body>
</html>