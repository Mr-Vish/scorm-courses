<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Implementing Distributed Tracing</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Implementing Distributed Tracing</h1>

<h2>Understanding Distributed Tracing</h2>
<p>Distributed tracing tracks a single user request as it flows through multiple services, functions, and external API calls. For GenAI applications, a single user query might trigger:</p>

<div class="trace-flow-diagram" style="margin: 2rem 0; padding: 2rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-radius: 8px;">
    <div style="text-align: center; margin-bottom: 1.5rem;">
        <div style="display: inline-block; padding: 0.75rem 1.5rem; background: #F16F00; color: white; border-radius: 8px; font-weight: bold; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
            User Request
        </div>
    </div>
    <div style="text-align: center; margin: 1rem 0;">
        <div style="display: inline-block; width: 2px; height: 30px; background: #dee2e6;"></div>
    </div>
    <div style="display: flex; justify-content: space-around; flex-wrap: wrap; gap: 1rem;">
        <div style="flex: 1; min-width: 150px; padding: 1rem; background: white; border-radius: 8px; border: 2px solid #e91e63; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
            <div style="font-weight: bold; color: #e91e63; margin-bottom: 0.5rem;">① LLM API Calls</div>
            <div style="font-size: 0.85rem; color: #6c757d;">Initial response, refinement, evaluation</div>
        </div>
        <div style="flex: 1; min-width: 150px; padding: 1rem; background: white; border-radius: 8px; border: 2px solid #9c27b0; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
            <div style="font-weight: bold; color: #9c27b0; margin-bottom: 0.5rem;">② Vector DB Queries</div>
            <div style="font-size: 0.85rem; color: #6c757d;">Retrieval for context</div>
        </div>
        <div style="flex: 1; min-width: 150px; padding: 1rem; background: white; border-radius: 8px; border: 2px solid #673ab7; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
            <div style="font-weight: bold; color: #673ab7; margin-bottom: 0.5rem;">③ Tool Executions</div>
            <div style="font-size: 0.85rem; color: #6c757d;">Function and API calls</div>
        </div>
        <div style="flex: 1; min-width: 150px; padding: 1rem; background: white; border-radius: 8px; border: 2px solid #3f51b5; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
            <div style="font-weight: bold; color: #3f51b5; margin-bottom: 0.5rem;">④ External APIs</div>
            <div style="font-size: 0.85rem; color: #6c757d;">Real-time data fetch</div>
        </div>
        <div style="flex: 1; min-width: 150px; padding: 1rem; background: white; border-radius: 8px; border: 2px solid #2196f3; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
            <div style="font-weight: bold; color: #2196f3; margin-bottom: 0.5rem;">⑤ Cache Interactions</div>
            <div style="font-size: 0.85rem; color: #6c757d;">Lookup and storage</div>
        </div>
    </div>
    <div style="text-align: center; margin: 1.5rem 0 0.5rem;">
        <div style="display: inline-block; width: 2px; height: 30px; background: #dee2e6;"></div>
    </div>
    <div style="text-align: center;">
        <div style="display: inline-block; padding: 0.75rem 1.5rem; background: #28a745; color: white; border-radius: 8px; font-weight: bold; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
            Final Response
        </div>
    </div>
</div>

<p style="margin-top: 1rem;"><strong>Without tracing, debugging multi-step AI workflows is nearly impossible.</strong></p>

<h2>OpenTelemetry for LLM Tracing</h2>
<p>OpenTelemetry (OTel) is the industry standard for distributed tracing. It provides vendor-neutral instrumentation that works with any observability backend.</p>

<h3>Basic OpenTelemetry Setup</h3>
<div class="code-block">
<pre><code>from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Initialize tracer provider
provider = TracerProvider()
processor = BatchSpanProcessor(OTLPSpanExporter(
    endpoint="http://localhost:4317"
))
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

# Get tracer for your application
tracer = trace.get_tracer("genai-app", "1.0.0")
</code></pre>
</div>

<h3>Tracing an LLM Call</h3>
<div class="code-block">
<pre><code>import anthropic
from opentelemetry import trace

tracer = trace.get_tracer("genai-app")

async def chat_completion(user_message, conversation_history):
    with tracer.start_as_current_span("llm_call") as span:
        # Set input attributes
        span.set_attribute("llm.vendor", "anthropic")
        span.set_attribute("llm.model", "claude-sonnet-4-20250514")
        span.set_attribute("llm.temperature", 0.7)
        span.set_attribute("llm.max_tokens", 1024)
        span.set_attribute("user.message.length", len(user_message))
        
        # Make the API call
        client = anthropic.AsyncAnthropic()
        response = await client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            temperature=0.7,
            messages=[
                *conversation_history,
                {"role": "user", "content": user_message}
            ]
        )
        
        # Set output attributes
        span.set_attribute("llm.input_tokens", response.usage.input_tokens)
        span.set_attribute("llm.output_tokens", response.usage.output_tokens)
        span.set_attribute("llm.total_tokens", 
            response.usage.input_tokens + response.usage.output_tokens)
        
        # Calculate cost (example pricing)
        input_cost = response.usage.input_tokens * 0.000003
        output_cost = response.usage.output_tokens * 0.000015
        total_cost = input_cost + output_cost
        span.set_attribute("llm.cost_usd", total_cost)
        
        return response.content[0].text
</code></pre>
</div>

<h2>Tracing Multi-Step AI Workflows</h2>
<p>Real-world GenAI applications involve multiple operations. Tracing reveals the complete flow:</p>

<h3>RAG (Retrieval-Augmented Generation) Example</h3>
<div class="code-block">
<pre><code>async def rag_query(user_question):
    with tracer.start_as_current_span("rag_query") as parent_span:
        parent_span.set_attribute("query", user_question)
        
        # Step 1: Generate embedding for the question
        with tracer.start_as_current_span("generate_embedding") as embed_span:
            embedding = await get_embedding(user_question)
            embed_span.set_attribute("embedding.dimensions", len(embedding))
        
        # Step 2: Search vector database
        with tracer.start_as_current_span("vector_search") as search_span:
            results = await vector_db.search(embedding, top_k=5)
            search_span.set_attribute("search.results_count", len(results))
            search_span.set_attribute("search.top_score", results[0].score)
        
        # Step 3: Construct context from results
        with tracer.start_as_current_span("build_context") as context_span:
            context = "\n\n".join([r.text for r in results])
            context_span.set_attribute("context.length", len(context))
            context_span.set_attribute("context.chunks", len(results))
        
        # Step 4: Generate response with context
        with tracer.start_as_current_span("llm_generation") as llm_span:
            prompt = f"Context:\n{context}\n\nQuestion: {user_question}"
            response = await chat_completion(prompt, [])
            llm_span.set_attribute("response.length", len(response))
        
        parent_span.set_attribute("rag.success", True)
        return response
</code></pre>
</div>

<h2>Tracing AI Agents</h2>
<p>AI agents make autonomous decisions about which tools to use. Tracing captures the agent's reasoning process:</p>

<div class="code-block">
<pre><code>async def agent_execute(user_request):
    with tracer.start_as_current_span("agent_execution") as agent_span:
        agent_span.set_attribute("request", user_request)
        iteration = 0
        max_iterations = 10
        
        while iteration < max_iterations:
            iteration += 1
            
            with tracer.start_as_current_span(f"agent_iteration_{iteration}") as iter_span:
                # Agent decides next action
                with tracer.start_as_current_span("agent_reasoning") as reason_span:
                    action = await agent.decide_action(user_request)
                    reason_span.set_attribute("action.type", action.type)
                    reason_span.set_attribute("action.tool", action.tool_name)
                
                # Execute the chosen tool
                if action.type == "tool_use":
                    with tracer.start_as_current_span("tool_execution") as tool_span:
                        tool_span.set_attribute("tool.name", action.tool_name)
                        result = await execute_tool(action.tool_name, action.parameters)
                        tool_span.set_attribute("tool.success", result.success)
                
                # Check if agent is done
                elif action.type == "final_answer":
                    agent_span.set_attribute("agent.iterations", iteration)
                    agent_span.set_attribute("agent.success", True)
                    return action.answer
        
        # Max iterations reached
        agent_span.set_attribute("agent.iterations", iteration)
        agent_span.set_attribute("agent.timeout", True)
        raise Exception("Agent exceeded maximum iterations")
</code></pre>
</div>

<h2>Semantic Conventions for LLM Tracing</h2>
<p>OpenTelemetry defines standard attribute names for LLM operations. Using these conventions ensures consistency:</p>

<table>
    <tr><th>Attribute</th><th>Description</th><th>Example Value</th></tr>
    <tr>
        <td class="rowheader">gen_ai.system</td>
        <td>LLM provider name</td>
        <td>"anthropic", "openai", "cohere"</td>
    </tr>
    <tr>
        <td class="rowheader">gen_ai.request.model</td>
        <td>Model identifier</td>
        <td>"claude-sonnet-4-20250514"</td>
    </tr>
    <tr>
        <td class="rowheader">gen_ai.request.temperature</td>
        <td>Sampling temperature</td>
        <td>0.7</td>
    </tr>
    <tr>
        <td class="rowheader">gen_ai.request.max_tokens</td>
        <td>Maximum output tokens</td>
        <td>1024</td>
    </tr>
    <tr>
        <td class="rowheader">gen_ai.usage.input_tokens</td>
        <td>Input token count</td>
        <td>1250</td>
    </tr>
    <tr>
        <td class="rowheader">gen_ai.usage.output_tokens</td>
        <td>Output token count</td>
        <td>380</td>
    </tr>
    <tr>
        <td class="rowheader">gen_ai.response.finish_reason</td>
        <td>Why generation stopped</td>
        <td>"end_turn", "max_tokens", "stop_sequence"</td>
    </tr>
</table>

<h2>Automatic Instrumentation</h2>
<p>Many frameworks provide automatic tracing without manual span creation:</p>

<h3>LangChain with LangSmith</h3>
<div class="code-block">
<pre><code>import os
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate

# Enable LangSmith tracing
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"

# All LangChain operations are automatically traced
llm = ChatAnthropic(model="claude-sonnet-4-20250514")
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "{input}")
])
chain = prompt | llm

# This single call generates a complete trace
response = chain.invoke({"input": "What is observability?"})
</code></pre>
</div>

<h2>Trace Sampling Strategies</h2>
<p>For high-volume applications, trace every request is impractical:</p>

<h3>Head-Based Sampling</h3>
<p>Decision made at the start of the trace:</p>
<div class="code-block">
<pre><code>from opentelemetry.sdk.trace.sampling import TraceIdRatioBased

# Sample 10% of traces
sampler = TraceIdRatioBased(0.1)
provider = TracerProvider(sampler=sampler)
</code></pre>
</div>

<h3>Tail-Based Sampling</h3>
<p>Decision made after the trace completes (keeps all errors, samples successes):</p>
<ul>
    <li>Keep 100% of traces with errors</li>
    <li>Keep 100% of traces exceeding latency threshold</li>
    <li>Keep 10% of successful traces</li>
</ul>

<h2>Trace Visualization</h2>
<p>Traces are visualized as waterfall diagrams showing:</p>
<ul>
    <li><strong>Span Duration:</strong> How long each operation took</li>
    <li><strong>Parent-Child Relationships:</strong> Which operations triggered others</li>
    <li><strong>Attributes:</strong> Metadata about each operation</li>
    <li><strong>Critical Path:</strong> The longest sequence of dependent operations</li>
</ul>

<h2>Common Tracing Pitfalls</h2>
<ul>
    <li><strong>Over-instrumentation:</strong> Creating too many spans adds overhead and noise</li>
    <li><strong>Missing Context:</strong> Not propagating trace context across async operations</li>
    <li><strong>Sensitive Data:</strong> Accidentally logging user inputs or API keys in span attributes</li>
    <li><strong>High Cardinality:</strong> Using unique IDs as span names (use attributes instead)</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>