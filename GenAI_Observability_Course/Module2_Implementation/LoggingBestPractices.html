<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Logging Best Practices and Security</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Logging Best Practices and Security</h1>

<h2>The Logging Dilemma</h2>
<p>GenAI applications face a unique logging challenge: you need detailed logs to debug issues and improve quality, but user inputs and AI outputs often contain sensitive information. Logging everything creates security and privacy risks; logging nothing makes debugging impossible.</p>

<h2>What to Log</h2>

<h3>Always Log (Safe Metadata)</h3>
<table>
    <tr><th>Category</th><th>Data Points</th><th>Purpose</th></tr>
    <tr>
        <td class="rowheader">Request Metadata</td>
        <td>Timestamp, request ID, user ID (hashed), session ID</td>
        <td>Track requests and correlate events</td>
    </tr>
    <tr>
        <td class="rowheader">Model Configuration</td>
        <td>Model name, temperature, max_tokens, top_p</td>
        <td>Reproduce issues and compare configurations</td>
    </tr>
    <tr>
        <td class="rowheader">Token Counts</td>
        <td>Input tokens, output tokens, total tokens</td>
        <td>Cost tracking and optimization</td>
    </tr>
    <tr>
        <td class="rowheader">Performance Metrics</td>
        <td>Latency, time to first token, tokens per second</td>
        <td>Performance monitoring and optimization</td>
    </tr>
    <tr>
        <td class="rowheader">Error Information</td>
        <td>Error codes, error messages, stack traces</td>
        <td>Debugging and incident response</td>
    </tr>
    <tr>
        <td class="rowheader">Guardrail Events</td>
        <td>Trigger type, severity, action taken</td>
        <td>Safety monitoring and compliance</td>
    </tr>
</table>

<h3>Conditionally Log (With Redaction)</h3>
<ul>
    <li><strong>User inputs:</strong> Log length and type, redact actual content unless needed for debugging</li>
    <li><strong>AI outputs:</strong> Log first 100 characters or hash for deduplication</li>
    <li><strong>Tool results:</strong> Log success/failure, redact sensitive data</li>
    <li><strong>Retrieval context:</strong> Log document IDs, not full content</li>
</ul>

<h3>Never Log</h3>
<ul>
    <li>API keys, access tokens, or credentials</li>
    <li>Full user messages containing PII (names, emails, phone numbers, addresses)</li>
    <li>Sensitive business data (financial information, health records)</li>
    <li>Complete conversation histories (use references instead)</li>
    <li>Unredacted tool outputs containing secrets</li>
</ul>

<h2>Structured Logging Implementation</h2>

<h3>Basic Structured Logger</h3>
<div class="code-block">
<pre><code>import json
import logging
from datetime import datetime
from typing import Any, Dict

class GenAILogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # JSON formatter for structured logs
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)
    
    def log_llm_request(self, 
                       request_id: str,
                       user_id: str,
                       model: str,
                       input_tokens: int,
                       output_tokens: int,
                       latency_ms: float,
                       success: bool,
                       error: str = None):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": "llm_request",
            "request_id": request_id,
            "user_id": self._hash_user_id(user_id),
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "latency_ms": latency_ms,
            "success": success
        }
        
        if error:
            log_entry["error"] = error
        
        self.logger.info(json.dumps(log_entry))
    
    def _hash_user_id(self, user_id: str) -> str:
        import hashlib
        return hashlib.sha256(user_id.encode()).hexdigest()[:16]
</code></pre>
</div>

<h3>Advanced Logging with Context</h3>
<div class="code-block">
<pre><code>from contextvars import ContextVar
import uuid

# Context variables for request tracking
request_id_var: ContextVar[str] = ContextVar('request_id', default=None)
user_id_var: ContextVar[str] = ContextVar('user_id', default=None)

class ContextualLogger:
    def __init__(self):
        self.logger = GenAILogger("genai-app")
    
    def log(self, event_type: str, data: Dict[str, Any]):
        # Automatically include context
        log_data = {
            "request_id": request_id_var.get(),
            "user_id": user_id_var.get(),
            "event_type": event_type,
            **data
        }
        self.logger.logger.info(json.dumps(log_data))

# Usage in request handler
async def handle_request(user_id: str, prompt: str):
    # Set context for this request
    request_id = str(uuid.uuid4())
    request_id_var.set(request_id)
    user_id_var.set(user_id)
    
    logger = ContextualLogger()
    
    logger.log("request_started", {
        "prompt_length": len(prompt)
    })
    
    try:
        response = await llm.generate(prompt)
        
        logger.log("llm_response", {
            "model": response.model,
            "input_tokens": response.usage.input_tokens,
            "output_tokens": response.usage.output_tokens,
            "latency_ms": response.latency
        })
        
        return response
        
    except Exception as e:
        logger.log("request_failed", {
            "error": str(e),
            "error_type": type(e).__name__
        })
        raise
</code></pre>
</div>

<h2>PII Redaction</h2>

<h3>Automatic PII Detection and Redaction</h3>
<div class="code-block">
<pre><code>import re
from typing import List, Tuple

class PIIRedactor:
    # Regex patterns for common PII
    PATTERNS = {
        "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        "phone": r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        "ssn": r'\b\d{3}-\d{2}-\d{4}\b',
        "credit_card": r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
        "ip_address": r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b'
    }
    
    @staticmethod
    def redact(text: str) -> Tuple[str, List[str]]:
        """
        Redact PII from text.
        Returns: (redacted_text, list_of_detected_pii_types)
        """
        detected_types = []
        redacted = text
        
        for pii_type, pattern in PIIRedactor.PATTERNS.items():
            matches = re.findall(pattern, text)
            if matches:
                detected_types.append(pii_type)
                redacted = re.sub(pattern, f"[REDACTED_{pii_type.upper()}]", redacted)
        
        return redacted, detected_types
    
    @staticmethod
    def should_log_content(text: str) -> bool:
        """
        Determine if text is safe to log.
        """
        _, detected_types = PIIRedactor.redact(text)
        return len(detected_types) == 0

# Usage
def safe_log_user_input(user_input: str):
    if PIIRedactor.should_log_content(user_input):
        logger.log("user_input", {"content": user_input})
    else:
        redacted, pii_types = PIIRedactor.redact(user_input)
        logger.log("user_input", {
            "content": redacted,
            "pii_detected": pii_types,
            "original_length": len(user_input)
        })
</code></pre>
</div>

<h2>Guardrail Event Logging</h2>

<h3>Safety Monitoring</h3>
<div class="code-block">
<pre><code>from enum import Enum

class GuardrailType(Enum):
    CONTENT_MODERATION = "content_moderation"
    PII_DETECTION = "pii_detection"
    PROMPT_INJECTION = "prompt_injection"
    JAILBREAK_ATTEMPT = "jailbreak_attempt"
    TOXIC_CONTENT = "toxic_content"

class GuardrailLogger:
    def __init__(self):
        self.logger = GenAILogger("guardrails")
    
    def log_trigger(self,
                   guardrail_type: GuardrailType,
                   severity: str,
                   action_taken: str,
                   details: Dict[str, Any] = None):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": "guardrail_trigger",
            "guardrail_type": guardrail_type.value,
            "severity": severity,
            "action_taken": action_taken,
            "request_id": request_id_var.get(),
            "user_id": user_id_var.get()
        }
        
        if details:
            log_entry["details"] = details
        
        self.logger.logger.warning(json.dumps(log_entry))

# Usage
async def check_content_safety(text: str):
    result = await content_moderator.check(text)
    
    if result.flagged:
        guardrail_logger.log_trigger(
            guardrail_type=GuardrailType.CONTENT_MODERATION,
            severity="high",
            action_taken="request_blocked",
            details={
                "categories": result.categories,
                "scores": result.scores
            }
        )
        raise ContentPolicyViolation("Content violates safety policies")
</code></pre>
</div>

<h2>Log Retention and Storage</h2>

<h3>Retention Policy</h3>
<table>
    <tr><th>Log Type</th><th>Retention Period</th><th>Storage</th></tr>
    <tr>
        <td class="rowheader">Request Metadata</td>
        <td>90 days</td>
        <td>Hot storage (fast queries)</td>
    </tr>
    <tr>
        <td class="rowheader">Cost Data</td>
        <td>2 years</td>
        <td>Warm storage (analytics)</td>
    </tr>
    <tr>
        <td class="rowheader">Error Logs</td>
        <td>180 days</td>
        <td>Hot storage</td>
    </tr>
    <tr>
        <td class="rowheader">Guardrail Events</td>
        <td>1 year</td>
        <td>Compliance storage (immutable)</td>
    </tr>
    <tr>
        <td class="rowheader">Full Request/Response</td>
        <td>7 days</td>
        <td>Cold storage (debugging only)</td>
    </tr>
</table>

<h3>Log Aggregation</h3>
<p>For high-volume applications, aggregate logs before storage:</p>
<div class="code-block">
<pre><code># Instead of storing every request
# Store aggregated metrics per minute/hour

{
    "timestamp": "2025-01-15T10:00:00Z",
    "aggregation_period": "1_minute",
    "model": "claude-sonnet-4",
    "request_count": 1250,
    "total_input_tokens": 1500000,
    "total_output_tokens": 625000,
    "total_cost": 7.875,
    "avg_latency_ms": 1850,
    "p95_latency_ms": 3200,
    "error_count": 12,
    "success_rate": 0.990
}
</code></pre>
</div>

<h2>Compliance and Audit Logging</h2>

<h3>Immutable Audit Trail</h3>
<p>For regulated industries, maintain an immutable audit log:</p>
<div class="code-block">
<pre><code>class AuditLogger:
    def __init__(self, blockchain_or_append_only_db):
        self.storage = blockchain_or_append_only_db
    
    async def log_audit_event(self,
                             event_type: str,
                             user_id: str,
                             action: str,
                             resource: str,
                             result: str):
        audit_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": event_type,
            "user_id": self._hash_user_id(user_id),
            "action": action,
            "resource": resource,
            "result": result,
            "ip_address": self._get_client_ip(),
            "signature": self._sign_entry(...)
        }
        
        # Write to append-only storage
        await self.storage.append(audit_entry)
    
    def _sign_entry(self, entry: Dict) -> str:
        # Cryptographically sign entry for tamper detection
        import hmac
        import hashlib
        
        entry_str = json.dumps(entry, sort_keys=True)
        signature = hmac.new(
            self.secret_key.encode(),
            entry_str.encode(),
            hashlib.sha256
        ).hexdigest()
        
        return signature
</code></pre>
</div>

<h2>Log Analysis and Alerting</h2>

<h3>Anomaly Detection</h3>
<div class="code-block">
<pre><code># Query for unusual patterns
SELECT 
    user_id,
    COUNT(*) as request_count,
    SUM(total_cost) as total_cost,
    AVG(latency_ms) as avg_latency
FROM llm_logs
WHERE timestamp >= NOW() - INTERVAL '1 hour'
GROUP BY user_id
HAVING COUNT(*) > 1000  -- Unusual request volume
    OR SUM(total_cost) > 50  -- High cost
ORDER BY total_cost DESC;
</code></pre>
</div>

<h2>Best Practices Summary</h2>
<ul>
    <li><strong>Use structured logging:</strong> JSON format for easy parsing and analysis</li>
    <li><strong>Redact PII automatically:</strong> Never log sensitive data in plain text</li>
    <li><strong>Include context:</strong> Request IDs, user IDs (hashed), timestamps</li>
    <li><strong>Log metadata, not content:</strong> Token counts and latency, not full messages</li>
    <li><strong>Implement retention policies:</strong> Delete old logs to reduce storage costs</li>
    <li><strong>Monitor guardrail triggers:</strong> Track safety and compliance events</li>
    <li><strong>Aggregate high-volume logs:</strong> Store summaries instead of individual events</li>
    <li><strong>Maintain audit trails:</strong> Immutable logs for compliance requirements</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>