<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Gateway Architecture and Authentication</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Gateway Architecture and Authentication</h1>


<h2>Why Build an LLM Gateway?</h2>
<p>An LLM gateway sits between your applications and LLM providers, providing centralized control over authentication, rate limiting, cost tracking, logging, and policy enforcement. It is especially important in enterprise environments where multiple teams access LLMs.</p>

<h2>Gateway Architecture</h2>
<table>
    <tr><th>Component</th><th>Responsibility</th><th>Technology Options</th></tr>
    <tr><td>API Layer</td><td>Receives requests, validates auth</td><td>FastAPI, Express, Kong, Envoy</td></tr>
    <tr><td>Auth Service</td><td>API key validation, team identity</td><td>OAuth2, JWT, API key store</td></tr>
    <tr><td>Rate Limiter</td><td>Enforce per-team/per-user limits</td><td>Redis, in-memory token bucket</td></tr>
    <tr><td>Router</td><td>Route to correct provider/model</td><td>Custom logic, LiteLLM</td></tr>
    <tr><td>Logger</td><td>Log all requests and responses</td><td>Kafka, PostgreSQL, S3</td></tr>
    <tr><td>Policy Engine</td><td>Content filtering, PII redaction</td><td>Custom rules, regex, NLP models</td></tr>
</table>

<h2>Basic Gateway with FastAPI</h2>
<div class="code-block">
<pre><code>from fastapi import FastAPI, HTTPException, Depends, Header
from pydantic import BaseModel
import litellm
import time

app = FastAPI(title="LLM Gateway")

# API Key validation
VALID_KEYS = {
    "key-team-alpha": {"team": "alpha", "rpm_limit": 60},
    "key-team-beta": {"team": "beta", "rpm_limit": 30},
}

async def verify_api_key(x_api_key: str = Header(...)):
    if x_api_key not in VALID_KEYS:
        raise HTTPException(status_code=401, detail="Invalid API key")
    return VALID_KEYS[x_api_key]

class ChatRequest(BaseModel):
    model: str
    messages: list[dict]
    max_tokens: int = 1000
    temperature: float = 0.7

@app.post("/v1/chat/completions")
async def chat(request: ChatRequest, team=Depends(verify_api_key)):
    # Route to the LLM provider
    response = await litellm.acompletion(
        model=request.model,
        messages=request.messages,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
    )
    return response</code></pre>
</div>

<h2>Authentication Patterns</h2>
<ul>
    <li><strong>API Keys:</strong> Simple, stateless - good for internal services and CI/CD pipelines</li>
    <li><strong>JWT Tokens:</strong> Carry team/user identity claims, support expiration and rotation</li>
    <li><strong>OAuth2:</strong> Full SSO integration with corporate identity providers</li>
    <li><strong>mTLS:</strong> Mutual TLS for service-to-service authentication in zero-trust environments</li>
</ul>

<h2>Model Routing</h2>
<div class="code-block">
<pre><code># Route based on model name to different providers
MODEL_ROUTES = {
    "gpt-4o": {"provider": "azure", "deployment": "gpt4o-east"},
    "claude-sonnet": {"provider": "anthropic", "model": "claude-sonnet-4-20250514"},
    "llama3": {"provider": "ollama", "model": "llama3:8b"},
}

def route_model(requested_model: str) -&gt; dict:
    if requested_model not in MODEL_ROUTES:
        raise HTTPException(404, f"Model {requested_model} not available")
    return MODEL_ROUTES[requested_model]</code></pre>
</div>


<script type="text/javascript">
</script>
</body>
</html>