<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Human Evaluation and Evaluation Frameworks</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Human Evaluation and Evaluation Frameworks</h1>


<h2>When Human Evaluation is Essential</h2>
<ul>
    <li>Evaluating tone, empathy, and cultural appropriateness</li>
    <li>Assessing creativity and writing quality</li>
    <li>Validating domain-specific accuracy (medical, legal)</li>
    <li>Building the initial evaluation dataset</li>
    <li>Calibrating LLM-as-judge systems</li>
</ul>

<h2>Human Evaluation Framework</h2>
<table>
    <tr><th>Component</th><th>Description</th></tr>
    <tr><td>Rubric</td><td>Clear scoring criteria with examples for each level (1-5)</td></tr>
    <tr><td>Annotators</td><td>2-3 evaluators per sample for inter-annotator agreement</td></tr>
    <tr><td>Sample size</td><td>100-500 samples for statistical significance</td></tr>
    <tr><td>Blind evaluation</td><td>Evaluators do not know which model generated which response</td></tr>
    <tr><td>Agreement metric</td><td>Cohen's Kappa or Krippendorff's Alpha to measure consistency</td></tr>
</table>

<h2>Building an Evaluation Dataset</h2>
<ol>
    <li><strong>Collect real queries:</strong> Sample from production traffic or user research</li>
    <li><strong>Create reference answers:</strong> Expert-written gold standard responses</li>
    <li><strong>Define edge cases:</strong> Include adversarial, ambiguous, and out-of-scope queries</li>
    <li><strong>Categorize:</strong> Group by task type, difficulty, and domain</li>
    <li><strong>Version control:</strong> Track changes to the eval dataset over time</li>
</ol>

<h2>Evaluation Frameworks</h2>
<table>
    <tr><th>Framework</th><th>Features</th></tr>
    <tr><td>RAGAS</td><td>RAG-specific eval: faithfulness, relevance, context recall</td></tr>
    <tr><td>DeepEval</td><td>Unit testing for LLMs, pytest integration</td></tr>
    <tr><td>Promptfoo</td><td>CLI tool for prompt testing and comparison</td></tr>
    <tr><td>LangSmith</td><td>Tracing, evaluation, and dataset management</td></tr>
    <tr><td>Braintrust</td><td>Experiment tracking and scoring</td></tr>
</table>

<h2>Continuous Evaluation</h2>
<ul>
    <li>Run evaluations on every prompt or model change</li>
    <li>Track metrics over time to catch regressions</li>
    <li>Sample production traffic for ongoing quality monitoring</li>
    <li>Set up alerts for metric drops beyond thresholds</li>
</ul>


<script type="text/javascript">
</script>
</body>
</html>