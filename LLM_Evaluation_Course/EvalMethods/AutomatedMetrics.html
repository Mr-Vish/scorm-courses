<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Automated Metrics and LLM-as-Judge</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Automated Metrics and LLM-as-Judge</h1>


<h2>Why Evaluate LLM Output?</h2>
<p>Without evaluation, you cannot improve. Evaluation answers: Is the model performing well? Is version B better than version A? Did this prompt change help or hurt?</p>

<h2>Automated Metrics</h2>
<table>
    <tr><th>Metric</th><th>Measures</th><th>Best For</th><th>Limitation</th></tr>
    <tr><td>BLEU</td><td>N-gram overlap with reference</td><td>Translation</td><td>Does not capture meaning</td></tr>
    <tr><td>ROUGE</td><td>Recall of reference n-grams</td><td>Summarization</td><td>Penalizes valid paraphrases</td></tr>
    <tr><td>BERTScore</td><td>Semantic similarity via embeddings</td><td>General text quality</td><td>Requires reference text</td></tr>
    <tr><td>Exact Match</td><td>Exact string match</td><td>Factual QA, extraction</td><td>Too strict for open-ended tasks</td></tr>
    <tr><td>Pass@k</td><td>Code passes tests in k attempts</td><td>Code generation</td><td>Requires test cases</td></tr>
</table>

<h2>LLM-as-Judge</h2>
<p>Use a strong LLM (like Claude) to evaluate outputs from another model:</p>
<div class="code-block">
<pre><code>judge_prompt = (
    f"Rate the following response on a scale of 1-5 for each criterion:
"
    f"- Accuracy, Completeness, Clarity, Relevance

"
    f"Question: {question}
Response: {response}

"
    f"Provide scores as JSON."
)

scores = call_llm(judge_prompt, model="claude-sonnet-4-20250514")</code></pre>
</div>

<h2>Pairwise Comparison</h2>
<p>Compare two responses head-to-head instead of scoring individually:</p>
<div class="code-block">
<pre><code>compare_prompt = (
    f"Which response better answers the question?
"
    f"Question: {question}
"
    f"Response A: {response_a}
Response B: {response_b}
"
    f"Choose: A, B, or Tie. Explain your reasoning."
)</code></pre>
</div>


<script type="text/javascript">
</script>
</body>
</html>